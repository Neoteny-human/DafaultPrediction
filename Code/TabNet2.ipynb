{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HgdTNIByj4J","outputId":"eaa6da4f-3343-4e97-e819-5be1d1e49c7c","executionInfo":{"status":"ok","timestamp":1691456498108,"user_tz":-540,"elapsed":7441,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch_tabnet in /usr/local/lib/python3.10/dist-packages (4.1.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.22.4)\n","Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.2.2)\n","Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.10.1)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (2.0.1+cu118)\n","Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (4.65.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3->pytorch_tabnet) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3->pytorch_tabnet) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch_tabnet) (1.3.0)\n","Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (0.2.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.10.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.16.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt) (3.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.18.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt) (4.65.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt) (2.2.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.10.9.7)\n"]}],"source":["!pip install pytorch_tabnet\n","!pip install hyperopt"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","train_df = pd.read_csv('/content/drive/MyDrive/CTGAN_oversampled_train_1_1.csv',encoding='utf-8')  ###############################\n","test_df = pd.read_csv('/content/drive/MyDrive/최종_동태_test.csv',encoding='utf-8')\n","\n","\n","# Assume that the last column is the target and the rest are features\n","X = train_df.iloc[:,:-1]\n","y = train_df.iloc[:, -1]\n","\n","scaler.fit(X)\n","X = scaler.transform(X)\n","\n","# Split the data into training and validation sets (80:20)\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Convert to numpy arrays for TabNet\n","X_train = X_train\n","y_train = y_train\n","X_val = X_val\n","y_val = y_val\n"],"metadata":{"id":"pKoPLOBty0Gf","executionInfo":{"status":"ok","timestamp":1691482817390,"user_tz":-540,"elapsed":5771,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["train_df.iloc[:,:-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":540},"id":"knrAAfT1kHIB","executionInfo":{"status":"ok","timestamp":1691482817391,"user_tz":-540,"elapsed":12,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}},"outputId":"9920c3e1-fab7-47a0-ffe9-49c8f10aa069"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         총자산경상이익률    총자산순이익률  금융비용/매출액  금융비용/총부채  순금융비용/매출액   매출액경상이익률  \\\n","0        1.400000   0.520000  0.016500  0.020759   0.016518   3.880000   \n","1        0.780000   0.280000 -0.006500 -0.018509  -0.006461   1.940000   \n","2       -4.490000  -6.370000 -0.001100 -0.003166  -0.001109  -9.510000   \n","3        2.070000   2.060000 -0.004700 -0.015814  -0.004671   5.250000   \n","4        1.410000   2.570000 -0.027200 -0.105115  -0.027181   3.750000   \n","...           ...        ...       ...       ...        ...        ...   \n","262611   4.220464  28.661857 -0.022087  0.021840  -0.003883  44.077079   \n","262612  -3.491165   3.824467  0.099023  0.048209   0.041389  -6.667071   \n","262613  -0.731232   4.189051 -0.017512  0.019466  -0.056311  18.898232   \n","262614  14.629055  12.287365 -0.015072  0.045999  -0.028462  35.946646   \n","262615   7.459030   3.845231 -0.020856  0.025383  -0.015028  26.145559   \n","\n","           매출액순이익률   자기자본경상이익률   자기자본순이익률   EBITDA/이자비용  ...  이익잉여금/총자산_변화량  \\\n","0         1.450000    1.960000   0.740000  6.977523e+05  ...       0.317284   \n","1         0.690000    0.900000   0.320000  3.596125e+06  ...       0.004411   \n","2       -13.500000   -5.370000  -7.620000 -3.215209e+07  ...      -0.065183   \n","3         5.240000    2.340000   2.340000  1.839779e+11  ...       0.333272   \n","4         6.830000    1.570000   2.850000  1.839779e+11  ...       0.452513   \n","...            ...         ...        ...           ...  ...            ...   \n","262611    4.376668 -150.497079   3.379074  1.603160e+09  ...      -0.225593   \n","262612  -18.730185   31.637270  -9.688987 -2.038276e+09  ...      -0.063538   \n","262613  -21.518412  -19.685213  32.644088  7.960788e+08  ...      -0.157116   \n","262614 -356.297678   -8.099356  28.439918 -9.428074e+08  ...       0.062206   \n","262615   32.453305 -104.618638  29.333869 -1.008508e+09  ...      -0.086941   \n","\n","        이익잉여금/총부채_변화량  이익잉여금/유동자산_변화량    현금비율_변화량     당좌비율_변화량     유동비율_변화량  \\\n","0           -0.207783       -0.254297  325.820000   -39.240000     1.220000   \n","1           -3.337870       -0.485126 -293.390000  -342.510000  -370.130000   \n","2            0.552992       -0.295992  -72.240000   -33.020000   -27.730000   \n","3            0.437456    -7428.352695   -5.480000   556.960000   565.300000   \n","4            5.250437        0.000000   11.380000  -118.510000  -115.870000   \n","...               ...             ...         ...          ...          ...   \n","262611      -0.115148      -22.848917   47.891736  3624.997826   -51.082228   \n","262612      -0.198053      -12.457046  -10.459174   -47.964826   -80.908113   \n","262613      -0.035649      -61.603011  -58.826530    28.281616   -51.954252   \n","262614       0.077493      -21.808188   32.759173    24.250765  -200.364791   \n","262615      -1.326846        7.850217   11.440545    27.828073  1023.432156   \n","\n","        유동부채회전율_변화량       총자산_변화량       매출액_변화량      고정자산_변화량  \n","0          1.316550  1.430400e+10  4.147000e+09  7.704000e+09  \n","1         -1.099866  4.800000e+08  2.351000e+09  1.206400e+10  \n","2          0.446005  3.100000e+08  4.289000e+09  4.940000e+08  \n","3          0.552812  2.645600e+10  5.928000e+09  1.947000e+09  \n","4          0.207839  2.651750e+10  8.566000e+09 -3.867000e+09  \n","...             ...           ...           ...           ...  \n","262611    -2.174777  1.929359e+09 -2.493049e+10 -1.334017e+08  \n","262612    -0.181034  2.747192e+09 -2.794841e+09 -6.479318e+08  \n","262613     0.322735  6.456113e+09 -5.246069e+09 -3.904229e+09  \n","262614    -1.966277  3.374012e+09 -4.886925e+09 -8.027666e+08  \n","262615    -1.294728 -3.048078e+08  1.100806e+10  6.641444e+09  \n","\n","[262616 rows x 56 columns]"],"text/html":["\n","\n","  <div id=\"df-9b6251b4-cd00-4413-b542-ef466de72228\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>총자산경상이익률</th>\n","      <th>총자산순이익률</th>\n","      <th>금융비용/매출액</th>\n","      <th>금융비용/총부채</th>\n","      <th>순금융비용/매출액</th>\n","      <th>매출액경상이익률</th>\n","      <th>매출액순이익률</th>\n","      <th>자기자본경상이익률</th>\n","      <th>자기자본순이익률</th>\n","      <th>EBITDA/이자비용</th>\n","      <th>...</th>\n","      <th>이익잉여금/총자산_변화량</th>\n","      <th>이익잉여금/총부채_변화량</th>\n","      <th>이익잉여금/유동자산_변화량</th>\n","      <th>현금비율_변화량</th>\n","      <th>당좌비율_변화량</th>\n","      <th>유동비율_변화량</th>\n","      <th>유동부채회전율_변화량</th>\n","      <th>총자산_변화량</th>\n","      <th>매출액_변화량</th>\n","      <th>고정자산_변화량</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.400000</td>\n","      <td>0.520000</td>\n","      <td>0.016500</td>\n","      <td>0.020759</td>\n","      <td>0.016518</td>\n","      <td>3.880000</td>\n","      <td>1.450000</td>\n","      <td>1.960000</td>\n","      <td>0.740000</td>\n","      <td>6.977523e+05</td>\n","      <td>...</td>\n","      <td>0.317284</td>\n","      <td>-0.207783</td>\n","      <td>-0.254297</td>\n","      <td>325.820000</td>\n","      <td>-39.240000</td>\n","      <td>1.220000</td>\n","      <td>1.316550</td>\n","      <td>1.430400e+10</td>\n","      <td>4.147000e+09</td>\n","      <td>7.704000e+09</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.780000</td>\n","      <td>0.280000</td>\n","      <td>-0.006500</td>\n","      <td>-0.018509</td>\n","      <td>-0.006461</td>\n","      <td>1.940000</td>\n","      <td>0.690000</td>\n","      <td>0.900000</td>\n","      <td>0.320000</td>\n","      <td>3.596125e+06</td>\n","      <td>...</td>\n","      <td>0.004411</td>\n","      <td>-3.337870</td>\n","      <td>-0.485126</td>\n","      <td>-293.390000</td>\n","      <td>-342.510000</td>\n","      <td>-370.130000</td>\n","      <td>-1.099866</td>\n","      <td>4.800000e+08</td>\n","      <td>2.351000e+09</td>\n","      <td>1.206400e+10</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-4.490000</td>\n","      <td>-6.370000</td>\n","      <td>-0.001100</td>\n","      <td>-0.003166</td>\n","      <td>-0.001109</td>\n","      <td>-9.510000</td>\n","      <td>-13.500000</td>\n","      <td>-5.370000</td>\n","      <td>-7.620000</td>\n","      <td>-3.215209e+07</td>\n","      <td>...</td>\n","      <td>-0.065183</td>\n","      <td>0.552992</td>\n","      <td>-0.295992</td>\n","      <td>-72.240000</td>\n","      <td>-33.020000</td>\n","      <td>-27.730000</td>\n","      <td>0.446005</td>\n","      <td>3.100000e+08</td>\n","      <td>4.289000e+09</td>\n","      <td>4.940000e+08</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.070000</td>\n","      <td>2.060000</td>\n","      <td>-0.004700</td>\n","      <td>-0.015814</td>\n","      <td>-0.004671</td>\n","      <td>5.250000</td>\n","      <td>5.240000</td>\n","      <td>2.340000</td>\n","      <td>2.340000</td>\n","      <td>1.839779e+11</td>\n","      <td>...</td>\n","      <td>0.333272</td>\n","      <td>0.437456</td>\n","      <td>-7428.352695</td>\n","      <td>-5.480000</td>\n","      <td>556.960000</td>\n","      <td>565.300000</td>\n","      <td>0.552812</td>\n","      <td>2.645600e+10</td>\n","      <td>5.928000e+09</td>\n","      <td>1.947000e+09</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.410000</td>\n","      <td>2.570000</td>\n","      <td>-0.027200</td>\n","      <td>-0.105115</td>\n","      <td>-0.027181</td>\n","      <td>3.750000</td>\n","      <td>6.830000</td>\n","      <td>1.570000</td>\n","      <td>2.850000</td>\n","      <td>1.839779e+11</td>\n","      <td>...</td>\n","      <td>0.452513</td>\n","      <td>5.250437</td>\n","      <td>0.000000</td>\n","      <td>11.380000</td>\n","      <td>-118.510000</td>\n","      <td>-115.870000</td>\n","      <td>0.207839</td>\n","      <td>2.651750e+10</td>\n","      <td>8.566000e+09</td>\n","      <td>-3.867000e+09</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>262611</th>\n","      <td>4.220464</td>\n","      <td>28.661857</td>\n","      <td>-0.022087</td>\n","      <td>0.021840</td>\n","      <td>-0.003883</td>\n","      <td>44.077079</td>\n","      <td>4.376668</td>\n","      <td>-150.497079</td>\n","      <td>3.379074</td>\n","      <td>1.603160e+09</td>\n","      <td>...</td>\n","      <td>-0.225593</td>\n","      <td>-0.115148</td>\n","      <td>-22.848917</td>\n","      <td>47.891736</td>\n","      <td>3624.997826</td>\n","      <td>-51.082228</td>\n","      <td>-2.174777</td>\n","      <td>1.929359e+09</td>\n","      <td>-2.493049e+10</td>\n","      <td>-1.334017e+08</td>\n","    </tr>\n","    <tr>\n","      <th>262612</th>\n","      <td>-3.491165</td>\n","      <td>3.824467</td>\n","      <td>0.099023</td>\n","      <td>0.048209</td>\n","      <td>0.041389</td>\n","      <td>-6.667071</td>\n","      <td>-18.730185</td>\n","      <td>31.637270</td>\n","      <td>-9.688987</td>\n","      <td>-2.038276e+09</td>\n","      <td>...</td>\n","      <td>-0.063538</td>\n","      <td>-0.198053</td>\n","      <td>-12.457046</td>\n","      <td>-10.459174</td>\n","      <td>-47.964826</td>\n","      <td>-80.908113</td>\n","      <td>-0.181034</td>\n","      <td>2.747192e+09</td>\n","      <td>-2.794841e+09</td>\n","      <td>-6.479318e+08</td>\n","    </tr>\n","    <tr>\n","      <th>262613</th>\n","      <td>-0.731232</td>\n","      <td>4.189051</td>\n","      <td>-0.017512</td>\n","      <td>0.019466</td>\n","      <td>-0.056311</td>\n","      <td>18.898232</td>\n","      <td>-21.518412</td>\n","      <td>-19.685213</td>\n","      <td>32.644088</td>\n","      <td>7.960788e+08</td>\n","      <td>...</td>\n","      <td>-0.157116</td>\n","      <td>-0.035649</td>\n","      <td>-61.603011</td>\n","      <td>-58.826530</td>\n","      <td>28.281616</td>\n","      <td>-51.954252</td>\n","      <td>0.322735</td>\n","      <td>6.456113e+09</td>\n","      <td>-5.246069e+09</td>\n","      <td>-3.904229e+09</td>\n","    </tr>\n","    <tr>\n","      <th>262614</th>\n","      <td>14.629055</td>\n","      <td>12.287365</td>\n","      <td>-0.015072</td>\n","      <td>0.045999</td>\n","      <td>-0.028462</td>\n","      <td>35.946646</td>\n","      <td>-356.297678</td>\n","      <td>-8.099356</td>\n","      <td>28.439918</td>\n","      <td>-9.428074e+08</td>\n","      <td>...</td>\n","      <td>0.062206</td>\n","      <td>0.077493</td>\n","      <td>-21.808188</td>\n","      <td>32.759173</td>\n","      <td>24.250765</td>\n","      <td>-200.364791</td>\n","      <td>-1.966277</td>\n","      <td>3.374012e+09</td>\n","      <td>-4.886925e+09</td>\n","      <td>-8.027666e+08</td>\n","    </tr>\n","    <tr>\n","      <th>262615</th>\n","      <td>7.459030</td>\n","      <td>3.845231</td>\n","      <td>-0.020856</td>\n","      <td>0.025383</td>\n","      <td>-0.015028</td>\n","      <td>26.145559</td>\n","      <td>32.453305</td>\n","      <td>-104.618638</td>\n","      <td>29.333869</td>\n","      <td>-1.008508e+09</td>\n","      <td>...</td>\n","      <td>-0.086941</td>\n","      <td>-1.326846</td>\n","      <td>7.850217</td>\n","      <td>11.440545</td>\n","      <td>27.828073</td>\n","      <td>1023.432156</td>\n","      <td>-1.294728</td>\n","      <td>-3.048078e+08</td>\n","      <td>1.100806e+10</td>\n","      <td>6.641444e+09</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>262616 rows × 56 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b6251b4-cd00-4413-b542-ef466de72228')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-0540a740-659a-4694-9adc-84af03f608f3\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0540a740-659a-4694-9adc-84af03f608f3')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-0540a740-659a-4694-9adc-84af03f608f3 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9b6251b4-cd00-4413-b542-ef466de72228 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9b6251b4-cd00-4413-b542-ef466de72228');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["import torch\n","\n","# GPU 사용 가능한지 확인하고 설정합니다.\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"Using {device} device\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHXDxMH51-nc","executionInfo":{"status":"ok","timestamp":1691482841054,"user_tz":-540,"elapsed":2,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}},"outputId":"d59ad220-00a2-442a-bc55-42c86e66b554"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}]},{"cell_type":"code","source":["from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n","from pytorch_tabnet.tab_model import TabNetClassifier\n","from sklearn.metrics import roc_auc_score,f1_score\n","import numpy as np\n","from sklearn.metrics import precision_recall_curve\n","\n","# Define the search space\n","space = {\n","    'n_d': hp.choice('n_d', [8, 16, 24, 32]),\n","    'n_a': hp.choice('n_a', [8, 16, 24, 32]),\n","}\n","\n","def objective(params):\n","    # Ensure parameters that should be integers are integers\n","    for parameter_name in ['n_d', 'n_a']:\n","        params[parameter_name] = int(params[parameter_name])\n","\n","    clf = TabNetClassifier(\n","        n_d=params['n_d'],\n","        n_a=params['n_a'],\n","        device_name='cuda'\n","    )\n","\n","    clf.fit(\n","        X_train, y_train,\n","        eval_set=[(X_val, y_val)],\n","        patience=30,\n","        max_epochs=200,\n","    )\n","\n","    preds_proba = clf.predict_proba(X_val)[:, 1]\n","\n","    # Compute Precision-Recall curve and find optimal threshold\n","    precision, recall, thresholds = precision_recall_curve(y_val, preds_proba)\n","    f1_scores = 2*(precision*recall)/(precision+recall)\n","    f1_scores = f1_scores[~np.isnan(f1_scores)]  # Remove any NaN F1 scores\n","\n","    optimal_idx = np.argmax(f1_scores)\n","    optimal_threshold = thresholds[optimal_idx]\n","\n","    preds = (preds_proba > optimal_threshold).astype(int)\n","    f1 = f1_score(y_val, preds)\n","\n","    return {'loss': -f1, 'status': STATUS_OK}\n","\n","# Run the algorithm\n","trials = Trials()\n","best = fmin(fn=objective,\n","            space=space,\n","            algo=tpe.suggest,\n","            max_evals=15,\n","            trials=trials)\n","\n","print(\"Best: \", best)"],"metadata":{"id":"vkpbQw0O23Xu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9282f064-9e5a-4f6b-91cb-3f59b612c840","executionInfo":{"status":"ok","timestamp":1691496754499,"user_tz":-540,"elapsed":13910715,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\r  0%|          | 0/15 [00:00<?, ?trial/s, best loss=?]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.34826 | val_0_auc: 0.96682 |  0:00:08s\n","epoch 1  | loss: 0.14676 | val_0_auc: 0.98655 |  0:00:17s\n","epoch 2  | loss: 0.1196  | val_0_auc: 0.98878 |  0:00:26s\n","epoch 3  | loss: 0.10604 | val_0_auc: 0.99074 |  0:00:35s\n","epoch 4  | loss: 0.09839 | val_0_auc: 0.99158 |  0:00:44s\n","epoch 5  | loss: 0.09334 | val_0_auc: 0.99143 |  0:00:53s\n","epoch 6  | loss: 0.09226 | val_0_auc: 0.99182 |  0:01:02s\n","epoch 7  | loss: 0.08653 | val_0_auc: 0.99204 |  0:01:11s\n","epoch 8  | loss: 0.08696 | val_0_auc: 0.99234 |  0:01:20s\n","epoch 9  | loss: 0.08341 | val_0_auc: 0.99279 |  0:01:29s\n","epoch 10 | loss: 0.08153 | val_0_auc: 0.99314 |  0:01:38s\n","epoch 11 | loss: 0.08663 | val_0_auc: 0.99291 |  0:01:47s\n","epoch 12 | loss: 0.08105 | val_0_auc: 0.99305 |  0:01:56s\n","epoch 13 | loss: 0.08028 | val_0_auc: 0.99267 |  0:02:05s\n","epoch 14 | loss: 0.08302 | val_0_auc: 0.99298 |  0:02:13s\n","epoch 15 | loss: 0.07852 | val_0_auc: 0.9936  |  0:02:23s\n","epoch 16 | loss: 0.08229 | val_0_auc: 0.99358 |  0:02:31s\n","epoch 17 | loss: 0.0776  | val_0_auc: 0.99344 |  0:02:40s\n","epoch 18 | loss: 0.07692 | val_0_auc: 0.99373 |  0:02:49s\n","epoch 19 | loss: 0.07551 | val_0_auc: 0.99372 |  0:02:58s\n","epoch 20 | loss: 0.07481 | val_0_auc: 0.99398 |  0:03:07s\n","epoch 21 | loss: 0.07615 | val_0_auc: 0.99418 |  0:03:16s\n","epoch 22 | loss: 0.07417 | val_0_auc: 0.99418 |  0:03:25s\n","epoch 23 | loss: 0.07705 | val_0_auc: 0.99395 |  0:03:34s\n","epoch 24 | loss: 0.07292 | val_0_auc: 0.99427 |  0:03:43s\n","epoch 25 | loss: 0.0741  | val_0_auc: 0.99402 |  0:03:52s\n","epoch 26 | loss: 0.07294 | val_0_auc: 0.99416 |  0:04:01s\n","epoch 27 | loss: 0.07227 | val_0_auc: 0.99443 |  0:04:10s\n","epoch 28 | loss: 0.0774  | val_0_auc: 0.99448 |  0:04:18s\n","epoch 29 | loss: 0.07307 | val_0_auc: 0.99364 |  0:04:27s\n","epoch 30 | loss: 0.07089 | val_0_auc: 0.99472 |  0:04:36s\n","epoch 31 | loss: 0.07177 | val_0_auc: 0.99455 |  0:04:45s\n","epoch 32 | loss: 0.07033 | val_0_auc: 0.99466 |  0:04:54s\n","epoch 33 | loss: 0.0693  | val_0_auc: 0.99461 |  0:05:03s\n","epoch 34 | loss: 0.06924 | val_0_auc: 0.99417 |  0:05:12s\n","epoch 35 | loss: 0.07244 | val_0_auc: 0.99408 |  0:05:21s\n","epoch 36 | loss: 0.07117 | val_0_auc: 0.99451 |  0:05:30s\n","epoch 37 | loss: 0.0718  | val_0_auc: 0.99458 |  0:05:39s\n","epoch 38 | loss: 0.06938 | val_0_auc: 0.99381 |  0:05:48s\n","epoch 39 | loss: 0.07062 | val_0_auc: 0.9943  |  0:05:57s\n","epoch 40 | loss: 0.06792 | val_0_auc: 0.99452 |  0:06:06s\n","epoch 41 | loss: 0.06931 | val_0_auc: 0.99409 |  0:06:15s\n","epoch 42 | loss: 0.07    | val_0_auc: 0.99444 |  0:06:24s\n","epoch 43 | loss: 0.06842 | val_0_auc: 0.99407 |  0:06:33s\n","epoch 44 | loss: 0.0683  | val_0_auc: 0.99423 |  0:06:42s\n","epoch 45 | loss: 0.06784 | val_0_auc: 0.9943  |  0:06:51s\n","epoch 46 | loss: 0.06728 | val_0_auc: 0.99447 |  0:06:59s\n","epoch 47 | loss: 0.06726 | val_0_auc: 0.99422 |  0:07:08s\n","epoch 48 | loss: 0.06782 | val_0_auc: 0.99399 |  0:07:17s\n","epoch 49 | loss: 0.06734 | val_0_auc: 0.99436 |  0:07:26s\n","epoch 50 | loss: 0.0676  | val_0_auc: 0.99395 |  0:07:35s\n","epoch 51 | loss: 0.06874 | val_0_auc: 0.9947  |  0:07:44s\n","epoch 52 | loss: 0.0665  | val_0_auc: 0.9947  |  0:07:53s\n","epoch 53 | loss: 0.06627 | val_0_auc: 0.99379 |  0:08:02s\n","epoch 54 | loss: 0.06676 | val_0_auc: 0.99442 |  0:08:11s\n","epoch 55 | loss: 0.06688 | val_0_auc: 0.99428 |  0:08:20s\n","epoch 56 | loss: 0.06668 | val_0_auc: 0.99376 |  0:08:29s\n","epoch 57 | loss: 0.06633 | val_0_auc: 0.99442 |  0:08:39s\n","epoch 58 | loss: 0.06629 | val_0_auc: 0.99425 |  0:08:48s\n","epoch 59 | loss: 0.06666 | val_0_auc: 0.99494 |  0:08:58s\n","epoch 60 | loss: 0.06585 | val_0_auc: 0.99467 |  0:09:07s\n","epoch 61 | loss: 0.06597 | val_0_auc: 0.99439 |  0:09:16s\n","epoch 62 | loss: 0.06633 | val_0_auc: 0.99436 |  0:09:25s\n","epoch 63 | loss: 0.06557 | val_0_auc: 0.99407 |  0:09:34s\n","epoch 64 | loss: 0.06577 | val_0_auc: 0.99438 |  0:09:43s\n","epoch 65 | loss: 0.06594 | val_0_auc: 0.99426 |  0:09:52s\n","epoch 66 | loss: 0.06552 | val_0_auc: 0.99419 |  0:10:02s\n","epoch 67 | loss: 0.06494 | val_0_auc: 0.99438 |  0:10:11s\n","epoch 68 | loss: 0.06573 | val_0_auc: 0.99374 |  0:10:20s\n","epoch 69 | loss: 0.06493 | val_0_auc: 0.99456 |  0:10:30s\n","epoch 70 | loss: 0.06518 | val_0_auc: 0.99464 |  0:10:39s\n","epoch 71 | loss: 0.06518 | val_0_auc: 0.99485 |  0:10:48s\n","epoch 72 | loss: 0.06455 | val_0_auc: 0.99467 |  0:10:57s\n","epoch 73 | loss: 0.06471 | val_0_auc: 0.99449 |  0:11:06s\n","epoch 74 | loss: 0.06415 | val_0_auc: 0.99426 |  0:11:15s\n","epoch 75 | loss: 0.06464 | val_0_auc: 0.99472 |  0:11:24s\n","epoch 76 | loss: 0.0646  | val_0_auc: 0.99452 |  0:11:34s\n","epoch 77 | loss: 0.06424 | val_0_auc: 0.99408 |  0:11:43s\n","epoch 78 | loss: 0.06453 | val_0_auc: 0.99404 |  0:11:52s\n","epoch 79 | loss: 0.0641  | val_0_auc: 0.99426 |  0:12:01s\n","epoch 80 | loss: 0.06464 | val_0_auc: 0.99432 |  0:12:11s\n","epoch 81 | loss: 0.06411 | val_0_auc: 0.99391 |  0:12:20s\n","epoch 82 | loss: 0.06423 | val_0_auc: 0.9944  |  0:12:29s\n","epoch 83 | loss: 0.06401 | val_0_auc: 0.99439 |  0:12:38s\n","epoch 84 | loss: 0.06382 | val_0_auc: 0.99425 |  0:12:47s\n","epoch 85 | loss: 0.06362 | val_0_auc: 0.99437 |  0:12:56s\n","epoch 86 | loss: 0.06439 | val_0_auc: 0.99447 |  0:13:05s\n","epoch 87 | loss: 0.06329 | val_0_auc: 0.99427 |  0:13:14s\n","epoch 88 | loss: 0.06378 | val_0_auc: 0.99458 |  0:13:24s\n","epoch 89 | loss: 0.0637  | val_0_auc: 0.99389 |  0:13:33s\n","\n","Early stopping occurred at epoch 89 with best_epoch = 59 and best_val_0_auc = 0.99494\n","  0%|          | 0/15 [13:33<?, ?trial/s, best loss=?]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r  7%|▋         | 1/15 [13:47<3:13:11, 827.95s/trial, best loss: -0.9782119872838646]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.31726 | val_0_auc: 0.95832 |  0:00:09s\n","epoch 1  | loss: 0.15258 | val_0_auc: 0.98656 |  0:00:19s\n","epoch 2  | loss: 0.11723 | val_0_auc: 0.9902  |  0:00:29s\n","epoch 3  | loss: 0.10356 | val_0_auc: 0.99154 |  0:00:38s\n","epoch 4  | loss: 0.09662 | val_0_auc: 0.99198 |  0:00:48s\n","epoch 5  | loss: 0.0914  | val_0_auc: 0.99219 |  0:00:57s\n","epoch 6  | loss: 0.08909 | val_0_auc: 0.99259 |  0:01:06s\n","epoch 7  | loss: 0.08888 | val_0_auc: 0.99209 |  0:01:16s\n","epoch 8  | loss: 0.08635 | val_0_auc: 0.99252 |  0:01:25s\n","epoch 9  | loss: 0.08413 | val_0_auc: 0.99312 |  0:01:35s\n","epoch 10 | loss: 0.08264 | val_0_auc: 0.99323 |  0:01:44s\n","epoch 11 | loss: 0.0818  | val_0_auc: 0.9912  |  0:01:53s\n","epoch 12 | loss: 0.08256 | val_0_auc: 0.99341 |  0:02:03s\n","epoch 13 | loss: 0.08037 | val_0_auc: 0.99197 |  0:02:12s\n","epoch 14 | loss: 0.07939 | val_0_auc: 0.9937  |  0:02:22s\n","epoch 15 | loss: 0.07792 | val_0_auc: 0.99347 |  0:02:31s\n","epoch 16 | loss: 0.0776  | val_0_auc: 0.99344 |  0:02:41s\n","epoch 17 | loss: 0.07612 | val_0_auc: 0.9937  |  0:02:51s\n","epoch 18 | loss: 0.07694 | val_0_auc: 0.99277 |  0:03:01s\n","epoch 19 | loss: 0.07699 | val_0_auc: 0.99389 |  0:03:11s\n","epoch 20 | loss: 0.07612 | val_0_auc: 0.99386 |  0:03:20s\n","epoch 21 | loss: 0.07533 | val_0_auc: 0.99316 |  0:03:30s\n","epoch 22 | loss: 0.07472 | val_0_auc: 0.99393 |  0:03:40s\n","epoch 23 | loss: 0.07639 | val_0_auc: 0.99343 |  0:03:49s\n","epoch 24 | loss: 0.07812 | val_0_auc: 0.99379 |  0:03:59s\n","epoch 25 | loss: 0.07513 | val_0_auc: 0.99384 |  0:04:09s\n","epoch 26 | loss: 0.07421 | val_0_auc: 0.99341 |  0:04:18s\n","epoch 27 | loss: 0.0776  | val_0_auc: 0.99354 |  0:04:28s\n","epoch 28 | loss: 0.07527 | val_0_auc: 0.994   |  0:04:38s\n","epoch 29 | loss: 0.07468 | val_0_auc: 0.99417 |  0:04:47s\n","epoch 30 | loss: 0.07315 | val_0_auc: 0.99435 |  0:04:57s\n","epoch 31 | loss: 0.07251 | val_0_auc: 0.99406 |  0:05:06s\n","epoch 32 | loss: 0.07249 | val_0_auc: 0.99355 |  0:05:16s\n","epoch 33 | loss: 0.07149 | val_0_auc: 0.9938  |  0:05:26s\n","epoch 34 | loss: 0.07285 | val_0_auc: 0.99416 |  0:05:36s\n","epoch 35 | loss: 0.07233 | val_0_auc: 0.99343 |  0:05:45s\n","epoch 36 | loss: 0.07111 | val_0_auc: 0.99439 |  0:05:55s\n","epoch 37 | loss: 0.07106 | val_0_auc: 0.9928  |  0:06:04s\n","epoch 38 | loss: 0.07114 | val_0_auc: 0.99456 |  0:06:13s\n","epoch 39 | loss: 0.071   | val_0_auc: 0.9939  |  0:06:23s\n","epoch 40 | loss: 0.07044 | val_0_auc: 0.99403 |  0:06:32s\n","epoch 41 | loss: 0.07027 | val_0_auc: 0.99452 |  0:06:42s\n","epoch 42 | loss: 0.0701  | val_0_auc: 0.99443 |  0:06:51s\n","epoch 43 | loss: 0.06977 | val_0_auc: 0.99388 |  0:07:00s\n","epoch 44 | loss: 0.07119 | val_0_auc: 0.99394 |  0:07:09s\n","epoch 45 | loss: 0.07294 | val_0_auc: 0.99352 |  0:07:19s\n","epoch 46 | loss: 0.07035 | val_0_auc: 0.99433 |  0:07:28s\n","epoch 47 | loss: 0.06998 | val_0_auc: 0.99438 |  0:07:37s\n","epoch 48 | loss: 0.06974 | val_0_auc: 0.99413 |  0:07:47s\n","epoch 49 | loss: 0.07005 | val_0_auc: 0.99426 |  0:07:56s\n","epoch 50 | loss: 0.06938 | val_0_auc: 0.9944  |  0:08:05s\n","epoch 51 | loss: 0.06919 | val_0_auc: 0.99128 |  0:08:14s\n","epoch 52 | loss: 0.06864 | val_0_auc: 0.99365 |  0:08:24s\n","epoch 53 | loss: 0.0688  | val_0_auc: 0.99391 |  0:08:33s\n","epoch 54 | loss: 0.06968 | val_0_auc: 0.99462 |  0:08:42s\n","epoch 55 | loss: 0.06867 | val_0_auc: 0.9933  |  0:08:51s\n","epoch 56 | loss: 0.06824 | val_0_auc: 0.9938  |  0:09:01s\n","epoch 57 | loss: 0.06775 | val_0_auc: 0.99262 |  0:09:10s\n","epoch 58 | loss: 0.06822 | val_0_auc: 0.99415 |  0:09:19s\n","epoch 59 | loss: 0.06815 | val_0_auc: 0.99178 |  0:09:28s\n","epoch 60 | loss: 0.06824 | val_0_auc: 0.99422 |  0:09:37s\n","epoch 61 | loss: 0.0693  | val_0_auc: 0.99413 |  0:09:46s\n","epoch 62 | loss: 0.06771 | val_0_auc: 0.99474 |  0:09:55s\n","epoch 63 | loss: 0.06789 | val_0_auc: 0.99369 |  0:10:04s\n","epoch 64 | loss: 0.06761 | val_0_auc: 0.99236 |  0:10:13s\n","epoch 65 | loss: 0.06753 | val_0_auc: 0.99379 |  0:10:22s\n","epoch 66 | loss: 0.06768 | val_0_auc: 0.99359 |  0:10:31s\n","epoch 67 | loss: 0.06645 | val_0_auc: 0.99318 |  0:10:41s\n","epoch 68 | loss: 0.06677 | val_0_auc: 0.99396 |  0:10:50s\n","epoch 69 | loss: 0.06704 | val_0_auc: 0.99383 |  0:10:59s\n","epoch 70 | loss: 0.06747 | val_0_auc: 0.99377 |  0:11:08s\n","epoch 71 | loss: 0.06776 | val_0_auc: 0.99383 |  0:11:17s\n","epoch 72 | loss: 0.06666 | val_0_auc: 0.99322 |  0:11:26s\n","epoch 73 | loss: 0.0678  | val_0_auc: 0.99273 |  0:11:35s\n","epoch 74 | loss: 0.06744 | val_0_auc: 0.99402 |  0:11:44s\n","epoch 75 | loss: 0.06691 | val_0_auc: 0.99455 |  0:11:53s\n","epoch 76 | loss: 0.06648 | val_0_auc: 0.99361 |  0:12:03s\n","epoch 77 | loss: 0.06619 | val_0_auc: 0.99439 |  0:12:12s\n","epoch 78 | loss: 0.06634 | val_0_auc: 0.99456 |  0:12:21s\n","epoch 79 | loss: 0.06584 | val_0_auc: 0.99478 |  0:12:30s\n","epoch 80 | loss: 0.06577 | val_0_auc: 0.99441 |  0:12:39s\n","epoch 81 | loss: 0.06599 | val_0_auc: 0.99456 |  0:12:48s\n","epoch 82 | loss: 0.06592 | val_0_auc: 0.99309 |  0:12:57s\n","epoch 83 | loss: 0.06611 | val_0_auc: 0.99432 |  0:13:06s\n","epoch 84 | loss: 0.0656  | val_0_auc: 0.99344 |  0:13:15s\n","epoch 85 | loss: 0.06571 | val_0_auc: 0.99413 |  0:13:24s\n","epoch 86 | loss: 0.06585 | val_0_auc: 0.9936  |  0:13:33s\n","epoch 87 | loss: 0.06593 | val_0_auc: 0.99408 |  0:13:42s\n","epoch 88 | loss: 0.06558 | val_0_auc: 0.99486 |  0:13:51s\n","epoch 89 | loss: 0.06527 | val_0_auc: 0.9943  |  0:14:01s\n","epoch 90 | loss: 0.06546 | val_0_auc: 0.99464 |  0:14:10s\n","epoch 91 | loss: 0.06564 | val_0_auc: 0.99369 |  0:14:19s\n","epoch 92 | loss: 0.06524 | val_0_auc: 0.99121 |  0:14:28s\n","epoch 93 | loss: 0.06596 | val_0_auc: 0.9946  |  0:14:38s\n","epoch 94 | loss: 0.06533 | val_0_auc: 0.99421 |  0:14:47s\n","epoch 95 | loss: 0.06497 | val_0_auc: 0.99469 |  0:14:56s\n","epoch 96 | loss: 0.06479 | val_0_auc: 0.99449 |  0:15:05s\n","epoch 97 | loss: 0.06548 | val_0_auc: 0.99451 |  0:15:14s\n","epoch 98 | loss: 0.0663  | val_0_auc: 0.99409 |  0:15:24s\n","epoch 99 | loss: 0.06498 | val_0_auc: 0.994   |  0:15:33s\n","epoch 100| loss: 0.06445 | val_0_auc: 0.99246 |  0:15:42s\n","epoch 101| loss: 0.06486 | val_0_auc: 0.9912  |  0:15:51s\n","epoch 102| loss: 0.0647  | val_0_auc: 0.99377 |  0:16:00s\n","epoch 103| loss: 0.06483 | val_0_auc: 0.99089 |  0:16:09s\n","epoch 104| loss: 0.06461 | val_0_auc: 0.9945  |  0:16:19s\n","epoch 105| loss: 0.06454 | val_0_auc: 0.99429 |  0:16:28s\n","epoch 106| loss: 0.06466 | val_0_auc: 0.99334 |  0:16:37s\n","epoch 107| loss: 0.06471 | val_0_auc: 0.99424 |  0:16:46s\n","epoch 108| loss: 0.06435 | val_0_auc: 0.99    |  0:16:55s\n","epoch 109| loss: 0.06414 | val_0_auc: 0.99399 |  0:17:04s\n","epoch 110| loss: 0.06417 | val_0_auc: 0.98989 |  0:17:13s\n","epoch 111| loss: 0.0641  | val_0_auc: 0.99172 |  0:17:22s\n","epoch 112| loss: 0.06352 | val_0_auc: 0.99437 |  0:17:31s\n","epoch 113| loss: 0.06402 | val_0_auc: 0.99427 |  0:17:40s\n","epoch 114| loss: 0.06437 | val_0_auc: 0.99428 |  0:17:49s\n","epoch 115| loss: 0.06377 | val_0_auc: 0.99365 |  0:17:59s\n","epoch 116| loss: 0.06475 | val_0_auc: 0.9944  |  0:18:08s\n","epoch 117| loss: 0.06374 | val_0_auc: 0.99424 |  0:18:17s\n","epoch 118| loss: 0.06357 | val_0_auc: 0.99252 |  0:18:26s\n","\n","Early stopping occurred at epoch 118 with best_epoch = 88 and best_val_0_auc = 0.99486\n","  7%|▋         | 1/15 [32:15<3:13:11, 827.95s/trial, best loss: -0.9782119872838646]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 13%|█▎        | 2/15 [32:29<3:36:47, 1000.54s/trial, best loss: -0.9783311709688037]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.31089 | val_0_auc: 0.96027 |  0:00:09s\n","epoch 1  | loss: 0.13865 | val_0_auc: 0.98917 |  0:00:18s\n","epoch 2  | loss: 0.11253 | val_0_auc: 0.99089 |  0:00:27s\n","epoch 3  | loss: 0.10187 | val_0_auc: 0.99174 |  0:00:36s\n","epoch 4  | loss: 0.09626 | val_0_auc: 0.99167 |  0:00:45s\n","epoch 5  | loss: 0.09229 | val_0_auc: 0.9917  |  0:00:54s\n","epoch 6  | loss: 0.08848 | val_0_auc: 0.99248 |  0:01:03s\n","epoch 7  | loss: 0.0875  | val_0_auc: 0.99259 |  0:01:12s\n","epoch 8  | loss: 0.08761 | val_0_auc: 0.99221 |  0:01:21s\n","epoch 9  | loss: 0.0847  | val_0_auc: 0.99248 |  0:01:30s\n","epoch 10 | loss: 0.08237 | val_0_auc: 0.99254 |  0:01:39s\n","epoch 11 | loss: 0.08141 | val_0_auc: 0.99288 |  0:01:48s\n","epoch 12 | loss: 0.08108 | val_0_auc: 0.99307 |  0:01:57s\n","epoch 13 | loss: 0.07962 | val_0_auc: 0.99307 |  0:02:06s\n","epoch 14 | loss: 0.07867 | val_0_auc: 0.99292 |  0:02:15s\n","epoch 15 | loss: 0.07765 | val_0_auc: 0.99347 |  0:02:24s\n","epoch 16 | loss: 0.07929 | val_0_auc: 0.99322 |  0:02:33s\n","epoch 17 | loss: 0.07809 | val_0_auc: 0.99322 |  0:02:42s\n","epoch 18 | loss: 0.07695 | val_0_auc: 0.99369 |  0:02:51s\n","epoch 19 | loss: 0.0758  | val_0_auc: 0.99382 |  0:02:59s\n","epoch 20 | loss: 0.07523 | val_0_auc: 0.99378 |  0:03:08s\n","epoch 21 | loss: 0.07423 | val_0_auc: 0.99344 |  0:03:17s\n","epoch 22 | loss: 0.07472 | val_0_auc: 0.99363 |  0:03:26s\n","epoch 23 | loss: 0.07456 | val_0_auc: 0.9935  |  0:03:35s\n","epoch 24 | loss: 0.07677 | val_0_auc: 0.99241 |  0:03:44s\n","epoch 25 | loss: 0.07646 | val_0_auc: 0.99269 |  0:03:53s\n","epoch 26 | loss: 0.07555 | val_0_auc: 0.9942  |  0:04:03s\n","epoch 27 | loss: 0.07411 | val_0_auc: 0.9943  |  0:04:12s\n","epoch 28 | loss: 0.0731  | val_0_auc: 0.99413 |  0:04:21s\n","epoch 29 | loss: 0.07337 | val_0_auc: 0.99433 |  0:04:30s\n","epoch 30 | loss: 0.07161 | val_0_auc: 0.99445 |  0:04:39s\n","epoch 31 | loss: 0.07135 | val_0_auc: 0.99388 |  0:04:48s\n","epoch 32 | loss: 0.07093 | val_0_auc: 0.99415 |  0:04:57s\n","epoch 33 | loss: 0.07593 | val_0_auc: 0.99351 |  0:05:06s\n","epoch 34 | loss: 0.07301 | val_0_auc: 0.99414 |  0:05:15s\n","epoch 35 | loss: 0.0734  | val_0_auc: 0.99415 |  0:05:24s\n","epoch 36 | loss: 0.0721  | val_0_auc: 0.9939  |  0:05:33s\n","epoch 37 | loss: 0.07164 | val_0_auc: 0.99423 |  0:05:42s\n","epoch 38 | loss: 0.07038 | val_0_auc: 0.99414 |  0:05:51s\n","epoch 39 | loss: 0.07129 | val_0_auc: 0.99451 |  0:06:00s\n","epoch 40 | loss: 0.07107 | val_0_auc: 0.99413 |  0:06:09s\n","epoch 41 | loss: 0.0708  | val_0_auc: 0.99458 |  0:06:18s\n","epoch 42 | loss: 0.06955 | val_0_auc: 0.99449 |  0:06:27s\n","epoch 43 | loss: 0.06924 | val_0_auc: 0.99472 |  0:06:36s\n","epoch 44 | loss: 0.06877 | val_0_auc: 0.99479 |  0:06:45s\n","epoch 45 | loss: 0.06971 | val_0_auc: 0.99467 |  0:06:54s\n","epoch 46 | loss: 0.06861 | val_0_auc: 0.99452 |  0:07:03s\n","epoch 47 | loss: 0.06894 | val_0_auc: 0.99458 |  0:07:12s\n","epoch 48 | loss: 0.06797 | val_0_auc: 0.99477 |  0:07:21s\n","epoch 49 | loss: 0.06805 | val_0_auc: 0.99475 |  0:07:30s\n","epoch 50 | loss: 0.06832 | val_0_auc: 0.99447 |  0:07:39s\n","epoch 51 | loss: 0.06824 | val_0_auc: 0.99478 |  0:07:48s\n","epoch 52 | loss: 0.06789 | val_0_auc: 0.99476 |  0:07:57s\n","epoch 53 | loss: 0.06823 | val_0_auc: 0.99491 |  0:08:06s\n","epoch 54 | loss: 0.0675  | val_0_auc: 0.99493 |  0:08:15s\n","epoch 55 | loss: 0.06702 | val_0_auc: 0.99498 |  0:08:24s\n","epoch 56 | loss: 0.06673 | val_0_auc: 0.99499 |  0:08:33s\n","epoch 57 | loss: 0.06629 | val_0_auc: 0.995   |  0:08:42s\n","epoch 58 | loss: 0.0664  | val_0_auc: 0.99478 |  0:08:51s\n","epoch 59 | loss: 0.06628 | val_0_auc: 0.99491 |  0:09:00s\n","epoch 60 | loss: 0.0665  | val_0_auc: 0.99487 |  0:09:10s\n","epoch 61 | loss: 0.06522 | val_0_auc: 0.99482 |  0:09:19s\n","epoch 62 | loss: 0.06527 | val_0_auc: 0.9949  |  0:09:28s\n","epoch 63 | loss: 0.0653  | val_0_auc: 0.99485 |  0:09:37s\n","epoch 64 | loss: 0.06607 | val_0_auc: 0.99483 |  0:09:46s\n","epoch 65 | loss: 0.06608 | val_0_auc: 0.99482 |  0:09:55s\n","epoch 66 | loss: 0.06509 | val_0_auc: 0.99507 |  0:10:04s\n","epoch 67 | loss: 0.06519 | val_0_auc: 0.99487 |  0:10:13s\n","epoch 68 | loss: 0.06561 | val_0_auc: 0.99496 |  0:10:22s\n","epoch 69 | loss: 0.0648  | val_0_auc: 0.99497 |  0:10:31s\n","epoch 70 | loss: 0.06426 | val_0_auc: 0.99482 |  0:10:40s\n","epoch 71 | loss: 0.06486 | val_0_auc: 0.99492 |  0:10:49s\n","epoch 72 | loss: 0.0641  | val_0_auc: 0.99456 |  0:10:58s\n","epoch 73 | loss: 0.06377 | val_0_auc: 0.9947  |  0:11:07s\n","epoch 74 | loss: 0.06386 | val_0_auc: 0.99463 |  0:11:17s\n","epoch 75 | loss: 0.06314 | val_0_auc: 0.99486 |  0:11:26s\n","epoch 76 | loss: 0.06445 | val_0_auc: 0.99464 |  0:11:36s\n","epoch 77 | loss: 0.06422 | val_0_auc: 0.99443 |  0:11:45s\n","epoch 78 | loss: 0.06388 | val_0_auc: 0.99478 |  0:11:55s\n","epoch 79 | loss: 0.06481 | val_0_auc: 0.99469 |  0:12:04s\n","epoch 80 | loss: 0.06385 | val_0_auc: 0.99457 |  0:12:13s\n","epoch 81 | loss: 0.06292 | val_0_auc: 0.99476 |  0:12:22s\n","epoch 82 | loss: 0.06293 | val_0_auc: 0.9947  |  0:12:31s\n","epoch 83 | loss: 0.06319 | val_0_auc: 0.99493 |  0:12:40s\n","epoch 84 | loss: 0.06281 | val_0_auc: 0.99473 |  0:12:49s\n","epoch 85 | loss: 0.06344 | val_0_auc: 0.99404 |  0:12:58s\n","epoch 86 | loss: 0.06419 | val_0_auc: 0.99454 |  0:13:07s\n","epoch 87 | loss: 0.06497 | val_0_auc: 0.9947  |  0:13:16s\n","epoch 88 | loss: 0.06278 | val_0_auc: 0.99468 |  0:13:25s\n","epoch 89 | loss: 0.06317 | val_0_auc: 0.99488 |  0:13:34s\n","epoch 90 | loss: 0.06264 | val_0_auc: 0.9946  |  0:13:43s\n","epoch 91 | loss: 0.06263 | val_0_auc: 0.99481 |  0:13:52s\n","epoch 92 | loss: 0.06245 | val_0_auc: 0.9949  |  0:14:01s\n","epoch 93 | loss: 0.06233 | val_0_auc: 0.99498 |  0:14:10s\n","epoch 94 | loss: 0.06215 | val_0_auc: 0.99472 |  0:14:19s\n","epoch 95 | loss: 0.06296 | val_0_auc: 0.99499 |  0:14:28s\n","epoch 96 | loss: 0.0618  | val_0_auc: 0.99481 |  0:14:37s\n","\n","Early stopping occurred at epoch 96 with best_epoch = 66 and best_val_0_auc = 0.99507\n"," 13%|█▎        | 2/15 [47:06<3:36:47, 1000.54s/trial, best loss: -0.9783311709688037]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 20%|██        | 3/15 [47:21<3:10:11, 950.94s/trial, best loss: -0.9783712546997946] "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.31726 | val_0_auc: 0.95832 |  0:00:08s\n","epoch 1  | loss: 0.15258 | val_0_auc: 0.98656 |  0:00:17s\n","epoch 2  | loss: 0.11723 | val_0_auc: 0.9902  |  0:00:26s\n","epoch 3  | loss: 0.10356 | val_0_auc: 0.99154 |  0:00:35s\n","epoch 4  | loss: 0.09662 | val_0_auc: 0.99198 |  0:00:44s\n","epoch 5  | loss: 0.0914  | val_0_auc: 0.99219 |  0:00:53s\n","epoch 6  | loss: 0.08909 | val_0_auc: 0.99259 |  0:01:02s\n","epoch 7  | loss: 0.08888 | val_0_auc: 0.99209 |  0:01:11s\n","epoch 8  | loss: 0.08635 | val_0_auc: 0.99252 |  0:01:20s\n","epoch 9  | loss: 0.08413 | val_0_auc: 0.99312 |  0:01:29s\n","epoch 10 | loss: 0.08264 | val_0_auc: 0.99323 |  0:01:39s\n","epoch 11 | loss: 0.0818  | val_0_auc: 0.9912  |  0:01:48s\n","epoch 12 | loss: 0.08256 | val_0_auc: 0.99341 |  0:01:57s\n","epoch 13 | loss: 0.08037 | val_0_auc: 0.99197 |  0:02:06s\n","epoch 14 | loss: 0.07939 | val_0_auc: 0.9937  |  0:02:15s\n","epoch 15 | loss: 0.07792 | val_0_auc: 0.99347 |  0:02:24s\n","epoch 16 | loss: 0.0776  | val_0_auc: 0.99344 |  0:02:33s\n","epoch 17 | loss: 0.07612 | val_0_auc: 0.9937  |  0:02:42s\n","epoch 18 | loss: 0.07694 | val_0_auc: 0.99277 |  0:02:51s\n","epoch 19 | loss: 0.07699 | val_0_auc: 0.99389 |  0:03:00s\n","epoch 20 | loss: 0.07612 | val_0_auc: 0.99386 |  0:03:10s\n","epoch 21 | loss: 0.07533 | val_0_auc: 0.99316 |  0:03:19s\n","epoch 22 | loss: 0.07472 | val_0_auc: 0.99393 |  0:03:29s\n","epoch 23 | loss: 0.07639 | val_0_auc: 0.99343 |  0:03:38s\n","epoch 24 | loss: 0.07812 | val_0_auc: 0.99379 |  0:03:47s\n","epoch 25 | loss: 0.07513 | val_0_auc: 0.99384 |  0:03:57s\n","epoch 26 | loss: 0.07421 | val_0_auc: 0.99341 |  0:04:06s\n","epoch 27 | loss: 0.0776  | val_0_auc: 0.99354 |  0:04:15s\n","epoch 28 | loss: 0.07527 | val_0_auc: 0.994   |  0:04:24s\n","epoch 29 | loss: 0.07468 | val_0_auc: 0.99417 |  0:04:33s\n","epoch 30 | loss: 0.07315 | val_0_auc: 0.99435 |  0:04:42s\n","epoch 31 | loss: 0.07251 | val_0_auc: 0.99406 |  0:04:51s\n","epoch 32 | loss: 0.07249 | val_0_auc: 0.99355 |  0:05:00s\n","epoch 33 | loss: 0.07149 | val_0_auc: 0.9938  |  0:05:09s\n","epoch 34 | loss: 0.07285 | val_0_auc: 0.99416 |  0:05:18s\n","epoch 35 | loss: 0.07233 | val_0_auc: 0.99343 |  0:05:28s\n","epoch 36 | loss: 0.07111 | val_0_auc: 0.99439 |  0:05:37s\n","epoch 37 | loss: 0.07106 | val_0_auc: 0.9928  |  0:05:46s\n","epoch 38 | loss: 0.07114 | val_0_auc: 0.99456 |  0:05:55s\n","epoch 39 | loss: 0.071   | val_0_auc: 0.9939  |  0:06:04s\n","epoch 40 | loss: 0.07044 | val_0_auc: 0.99403 |  0:06:13s\n","epoch 41 | loss: 0.07027 | val_0_auc: 0.99452 |  0:06:22s\n","epoch 42 | loss: 0.0701  | val_0_auc: 0.99443 |  0:06:31s\n","epoch 43 | loss: 0.06977 | val_0_auc: 0.99388 |  0:06:40s\n","epoch 44 | loss: 0.07119 | val_0_auc: 0.99394 |  0:06:49s\n","epoch 45 | loss: 0.07294 | val_0_auc: 0.99352 |  0:06:58s\n","epoch 46 | loss: 0.07035 | val_0_auc: 0.99433 |  0:07:08s\n","epoch 47 | loss: 0.06998 | val_0_auc: 0.99438 |  0:07:17s\n","epoch 48 | loss: 0.06974 | val_0_auc: 0.99413 |  0:07:26s\n","epoch 49 | loss: 0.07005 | val_0_auc: 0.99426 |  0:07:35s\n","epoch 50 | loss: 0.06938 | val_0_auc: 0.9944  |  0:07:44s\n","epoch 51 | loss: 0.06919 | val_0_auc: 0.99128 |  0:07:53s\n","epoch 52 | loss: 0.06864 | val_0_auc: 0.99365 |  0:08:02s\n","epoch 53 | loss: 0.0688  | val_0_auc: 0.99391 |  0:08:11s\n","epoch 54 | loss: 0.06968 | val_0_auc: 0.99462 |  0:08:20s\n","epoch 55 | loss: 0.06867 | val_0_auc: 0.9933  |  0:08:29s\n","epoch 56 | loss: 0.06824 | val_0_auc: 0.9938  |  0:08:39s\n","epoch 57 | loss: 0.06775 | val_0_auc: 0.99262 |  0:08:48s\n","epoch 58 | loss: 0.06822 | val_0_auc: 0.99415 |  0:08:57s\n","epoch 59 | loss: 0.06815 | val_0_auc: 0.99178 |  0:09:06s\n","epoch 60 | loss: 0.06824 | val_0_auc: 0.99422 |  0:09:15s\n","epoch 61 | loss: 0.0693  | val_0_auc: 0.99413 |  0:09:24s\n","epoch 62 | loss: 0.06771 | val_0_auc: 0.99474 |  0:09:33s\n","epoch 63 | loss: 0.06789 | val_0_auc: 0.99369 |  0:09:42s\n","epoch 64 | loss: 0.06761 | val_0_auc: 0.99236 |  0:09:51s\n","epoch 65 | loss: 0.06753 | val_0_auc: 0.99379 |  0:10:00s\n","epoch 66 | loss: 0.06768 | val_0_auc: 0.99359 |  0:10:09s\n","epoch 67 | loss: 0.06645 | val_0_auc: 0.99318 |  0:10:19s\n","epoch 68 | loss: 0.06677 | val_0_auc: 0.99396 |  0:10:28s\n","epoch 69 | loss: 0.06704 | val_0_auc: 0.99383 |  0:10:36s\n","epoch 70 | loss: 0.06747 | val_0_auc: 0.99377 |  0:10:46s\n","epoch 71 | loss: 0.06776 | val_0_auc: 0.99383 |  0:10:55s\n","epoch 72 | loss: 0.06666 | val_0_auc: 0.99322 |  0:11:04s\n","epoch 73 | loss: 0.0678  | val_0_auc: 0.99273 |  0:11:13s\n","epoch 74 | loss: 0.06744 | val_0_auc: 0.99402 |  0:11:22s\n","epoch 75 | loss: 0.06691 | val_0_auc: 0.99455 |  0:11:31s\n","epoch 76 | loss: 0.06648 | val_0_auc: 0.99361 |  0:11:40s\n","epoch 77 | loss: 0.06619 | val_0_auc: 0.99439 |  0:11:49s\n","epoch 78 | loss: 0.06634 | val_0_auc: 0.99456 |  0:11:58s\n","epoch 79 | loss: 0.06584 | val_0_auc: 0.99478 |  0:12:07s\n","epoch 80 | loss: 0.06577 | val_0_auc: 0.99441 |  0:12:17s\n","epoch 81 | loss: 0.06599 | val_0_auc: 0.99456 |  0:12:26s\n","epoch 82 | loss: 0.06592 | val_0_auc: 0.99309 |  0:12:35s\n","epoch 83 | loss: 0.06611 | val_0_auc: 0.99432 |  0:12:44s\n","epoch 84 | loss: 0.0656  | val_0_auc: 0.99344 |  0:12:53s\n","epoch 85 | loss: 0.06571 | val_0_auc: 0.99413 |  0:13:02s\n","epoch 86 | loss: 0.06585 | val_0_auc: 0.9936  |  0:13:11s\n","epoch 87 | loss: 0.06593 | val_0_auc: 0.99408 |  0:13:20s\n","epoch 88 | loss: 0.06558 | val_0_auc: 0.99486 |  0:13:29s\n","epoch 89 | loss: 0.06527 | val_0_auc: 0.9943  |  0:13:38s\n","epoch 90 | loss: 0.06546 | val_0_auc: 0.99464 |  0:13:47s\n","epoch 91 | loss: 0.06564 | val_0_auc: 0.99369 |  0:13:56s\n","epoch 92 | loss: 0.06524 | val_0_auc: 0.99121 |  0:14:05s\n","epoch 93 | loss: 0.06596 | val_0_auc: 0.9946  |  0:14:15s\n","epoch 94 | loss: 0.06533 | val_0_auc: 0.99421 |  0:14:24s\n","epoch 95 | loss: 0.06497 | val_0_auc: 0.99469 |  0:14:33s\n","epoch 96 | loss: 0.06479 | val_0_auc: 0.99449 |  0:14:42s\n","epoch 97 | loss: 0.06548 | val_0_auc: 0.99451 |  0:14:51s\n","epoch 98 | loss: 0.0663  | val_0_auc: 0.99409 |  0:15:00s\n","epoch 99 | loss: 0.06498 | val_0_auc: 0.994   |  0:15:09s\n","epoch 100| loss: 0.06445 | val_0_auc: 0.99246 |  0:15:18s\n","epoch 101| loss: 0.06486 | val_0_auc: 0.9912  |  0:15:27s\n","epoch 102| loss: 0.0647  | val_0_auc: 0.99377 |  0:15:36s\n","epoch 103| loss: 0.06483 | val_0_auc: 0.99089 |  0:15:45s\n","epoch 104| loss: 0.06461 | val_0_auc: 0.9945  |  0:15:54s\n","epoch 105| loss: 0.06454 | val_0_auc: 0.99429 |  0:16:03s\n","epoch 106| loss: 0.06466 | val_0_auc: 0.99334 |  0:16:14s\n","epoch 107| loss: 0.06471 | val_0_auc: 0.99424 |  0:16:24s\n","epoch 108| loss: 0.06435 | val_0_auc: 0.99    |  0:16:33s\n","epoch 109| loss: 0.06414 | val_0_auc: 0.99399 |  0:16:43s\n","epoch 110| loss: 0.06417 | val_0_auc: 0.98989 |  0:16:52s\n","epoch 111| loss: 0.0641  | val_0_auc: 0.99172 |  0:17:02s\n","epoch 112| loss: 0.06352 | val_0_auc: 0.99437 |  0:17:11s\n","epoch 113| loss: 0.06402 | val_0_auc: 0.99427 |  0:17:20s\n","epoch 114| loss: 0.06437 | val_0_auc: 0.99428 |  0:17:30s\n","epoch 115| loss: 0.06377 | val_0_auc: 0.99365 |  0:17:39s\n","epoch 116| loss: 0.06475 | val_0_auc: 0.9944  |  0:17:49s\n","epoch 117| loss: 0.06374 | val_0_auc: 0.99424 |  0:17:58s\n","epoch 118| loss: 0.06357 | val_0_auc: 0.99252 |  0:18:08s\n","\n","Early stopping occurred at epoch 118 with best_epoch = 88 and best_val_0_auc = 0.99486\n"," 20%|██        | 3/15 [1:05:29<3:10:11, 950.94s/trial, best loss: -0.9783712546997946]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 27%|██▋       | 4/15 [1:05:43<3:05:18, 1010.81s/trial, best loss: -0.9783712546997946]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.31089 | val_0_auc: 0.96027 |  0:00:09s\n","epoch 1  | loss: 0.13865 | val_0_auc: 0.98917 |  0:00:18s\n","epoch 2  | loss: 0.11253 | val_0_auc: 0.99089 |  0:00:27s\n","epoch 3  | loss: 0.10187 | val_0_auc: 0.99174 |  0:00:36s\n","epoch 4  | loss: 0.09626 | val_0_auc: 0.99167 |  0:00:44s\n","epoch 5  | loss: 0.09229 | val_0_auc: 0.9917  |  0:00:53s\n","epoch 6  | loss: 0.08848 | val_0_auc: 0.99248 |  0:01:02s\n","epoch 7  | loss: 0.0875  | val_0_auc: 0.99259 |  0:01:12s\n","epoch 8  | loss: 0.08761 | val_0_auc: 0.99221 |  0:01:21s\n","epoch 9  | loss: 0.0847  | val_0_auc: 0.99248 |  0:01:30s\n","epoch 10 | loss: 0.08237 | val_0_auc: 0.99254 |  0:01:40s\n","epoch 11 | loss: 0.08141 | val_0_auc: 0.99288 |  0:01:49s\n","epoch 12 | loss: 0.08108 | val_0_auc: 0.99307 |  0:01:58s\n","epoch 13 | loss: 0.07962 | val_0_auc: 0.99307 |  0:02:07s\n","epoch 14 | loss: 0.07867 | val_0_auc: 0.99292 |  0:02:17s\n","epoch 15 | loss: 0.07765 | val_0_auc: 0.99347 |  0:02:26s\n","epoch 16 | loss: 0.07929 | val_0_auc: 0.99322 |  0:02:35s\n","epoch 17 | loss: 0.07809 | val_0_auc: 0.99322 |  0:02:44s\n","epoch 18 | loss: 0.07695 | val_0_auc: 0.99369 |  0:02:53s\n","epoch 19 | loss: 0.0758  | val_0_auc: 0.99382 |  0:03:03s\n","epoch 20 | loss: 0.07523 | val_0_auc: 0.99378 |  0:03:12s\n","epoch 21 | loss: 0.07423 | val_0_auc: 0.99344 |  0:03:21s\n","epoch 22 | loss: 0.07472 | val_0_auc: 0.99363 |  0:03:30s\n","epoch 23 | loss: 0.07456 | val_0_auc: 0.9935  |  0:03:40s\n","epoch 24 | loss: 0.07677 | val_0_auc: 0.99241 |  0:03:49s\n","epoch 25 | loss: 0.07646 | val_0_auc: 0.99269 |  0:03:58s\n","epoch 26 | loss: 0.07555 | val_0_auc: 0.9942  |  0:04:07s\n","epoch 27 | loss: 0.07411 | val_0_auc: 0.9943  |  0:04:16s\n","epoch 28 | loss: 0.0731  | val_0_auc: 0.99413 |  0:04:26s\n","epoch 29 | loss: 0.07337 | val_0_auc: 0.99433 |  0:04:35s\n","epoch 30 | loss: 0.07161 | val_0_auc: 0.99445 |  0:04:44s\n","epoch 31 | loss: 0.07135 | val_0_auc: 0.99388 |  0:04:53s\n","epoch 32 | loss: 0.07093 | val_0_auc: 0.99415 |  0:05:02s\n","epoch 33 | loss: 0.07593 | val_0_auc: 0.99351 |  0:05:12s\n","epoch 34 | loss: 0.07301 | val_0_auc: 0.99414 |  0:05:21s\n","epoch 35 | loss: 0.0734  | val_0_auc: 0.99415 |  0:05:30s\n","epoch 36 | loss: 0.0721  | val_0_auc: 0.9939  |  0:05:39s\n","epoch 37 | loss: 0.07164 | val_0_auc: 0.99423 |  0:05:48s\n","epoch 38 | loss: 0.07038 | val_0_auc: 0.99414 |  0:05:57s\n","epoch 39 | loss: 0.07129 | val_0_auc: 0.99451 |  0:06:06s\n","epoch 40 | loss: 0.07107 | val_0_auc: 0.99413 |  0:06:16s\n","epoch 41 | loss: 0.0708  | val_0_auc: 0.99458 |  0:06:24s\n","epoch 42 | loss: 0.06955 | val_0_auc: 0.99449 |  0:06:34s\n","epoch 43 | loss: 0.06924 | val_0_auc: 0.99472 |  0:06:43s\n","epoch 44 | loss: 0.06877 | val_0_auc: 0.99479 |  0:06:52s\n","epoch 45 | loss: 0.06971 | val_0_auc: 0.99467 |  0:07:01s\n","epoch 46 | loss: 0.06861 | val_0_auc: 0.99452 |  0:07:10s\n","epoch 47 | loss: 0.06894 | val_0_auc: 0.99458 |  0:07:18s\n","epoch 48 | loss: 0.06797 | val_0_auc: 0.99477 |  0:07:27s\n","epoch 49 | loss: 0.06805 | val_0_auc: 0.99475 |  0:07:36s\n","epoch 50 | loss: 0.06832 | val_0_auc: 0.99447 |  0:07:45s\n","epoch 51 | loss: 0.06824 | val_0_auc: 0.99478 |  0:07:54s\n","epoch 52 | loss: 0.06789 | val_0_auc: 0.99476 |  0:08:03s\n","epoch 53 | loss: 0.06823 | val_0_auc: 0.99491 |  0:08:12s\n","epoch 54 | loss: 0.0675  | val_0_auc: 0.99493 |  0:08:21s\n","epoch 55 | loss: 0.06702 | val_0_auc: 0.99498 |  0:08:30s\n","epoch 56 | loss: 0.06673 | val_0_auc: 0.99499 |  0:08:39s\n","epoch 57 | loss: 0.06629 | val_0_auc: 0.995   |  0:08:48s\n","epoch 58 | loss: 0.0664  | val_0_auc: 0.99478 |  0:08:57s\n","epoch 59 | loss: 0.06628 | val_0_auc: 0.99491 |  0:09:06s\n","epoch 60 | loss: 0.0665  | val_0_auc: 0.99487 |  0:09:15s\n","epoch 61 | loss: 0.06522 | val_0_auc: 0.99482 |  0:09:24s\n","epoch 62 | loss: 0.06527 | val_0_auc: 0.9949  |  0:09:33s\n","epoch 63 | loss: 0.0653  | val_0_auc: 0.99485 |  0:09:42s\n","epoch 64 | loss: 0.06607 | val_0_auc: 0.99483 |  0:09:51s\n","epoch 65 | loss: 0.06608 | val_0_auc: 0.99482 |  0:10:00s\n","epoch 66 | loss: 0.06509 | val_0_auc: 0.99507 |  0:10:09s\n","epoch 67 | loss: 0.06519 | val_0_auc: 0.99487 |  0:10:18s\n","epoch 68 | loss: 0.06561 | val_0_auc: 0.99496 |  0:10:27s\n","epoch 69 | loss: 0.0648  | val_0_auc: 0.99497 |  0:10:36s\n","epoch 70 | loss: 0.06426 | val_0_auc: 0.99482 |  0:10:45s\n","epoch 71 | loss: 0.06486 | val_0_auc: 0.99492 |  0:10:54s\n","epoch 72 | loss: 0.0641  | val_0_auc: 0.99456 |  0:11:03s\n","epoch 73 | loss: 0.06377 | val_0_auc: 0.9947  |  0:11:12s\n","epoch 74 | loss: 0.06386 | val_0_auc: 0.99463 |  0:11:21s\n","epoch 75 | loss: 0.06314 | val_0_auc: 0.99486 |  0:11:30s\n","epoch 76 | loss: 0.06445 | val_0_auc: 0.99464 |  0:11:39s\n","epoch 77 | loss: 0.06422 | val_0_auc: 0.99443 |  0:11:48s\n","epoch 78 | loss: 0.06388 | val_0_auc: 0.99478 |  0:11:57s\n","epoch 79 | loss: 0.06481 | val_0_auc: 0.99469 |  0:12:06s\n","epoch 80 | loss: 0.06385 | val_0_auc: 0.99457 |  0:12:15s\n","epoch 81 | loss: 0.06292 | val_0_auc: 0.99476 |  0:12:24s\n","epoch 82 | loss: 0.06293 | val_0_auc: 0.9947  |  0:12:33s\n","epoch 83 | loss: 0.06319 | val_0_auc: 0.99493 |  0:12:42s\n","epoch 84 | loss: 0.06281 | val_0_auc: 0.99473 |  0:12:51s\n","epoch 85 | loss: 0.06344 | val_0_auc: 0.99404 |  0:13:00s\n","epoch 86 | loss: 0.06419 | val_0_auc: 0.99454 |  0:13:09s\n","epoch 87 | loss: 0.06497 | val_0_auc: 0.9947  |  0:13:18s\n","epoch 88 | loss: 0.06278 | val_0_auc: 0.99468 |  0:13:27s\n","epoch 89 | loss: 0.06317 | val_0_auc: 0.99488 |  0:13:36s\n","epoch 90 | loss: 0.06264 | val_0_auc: 0.9946  |  0:13:45s\n","epoch 91 | loss: 0.06263 | val_0_auc: 0.99481 |  0:13:54s\n","epoch 92 | loss: 0.06245 | val_0_auc: 0.9949  |  0:14:03s\n","epoch 93 | loss: 0.06233 | val_0_auc: 0.99498 |  0:14:12s\n","epoch 94 | loss: 0.06215 | val_0_auc: 0.99472 |  0:14:21s\n","epoch 95 | loss: 0.06296 | val_0_auc: 0.99499 |  0:14:31s\n","epoch 96 | loss: 0.0618  | val_0_auc: 0.99481 |  0:14:40s\n","\n","Early stopping occurred at epoch 96 with best_epoch = 66 and best_val_0_auc = 0.99507\n"," 27%|██▋       | 4/15 [1:20:24<3:05:18, 1010.81s/trial, best loss: -0.9783712546997946]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 33%|███▎      | 5/15 [1:20:38<2:41:29, 968.93s/trial, best loss: -0.9783712546997946] "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.31276 | val_0_auc: 0.95096 |  0:00:08s\n","epoch 1  | loss: 0.13008 | val_0_auc: 0.98882 |  0:00:17s\n","epoch 2  | loss: 0.10735 | val_0_auc: 0.99048 |  0:00:26s\n","epoch 3  | loss: 0.09925 | val_0_auc: 0.9917  |  0:00:35s\n","epoch 4  | loss: 0.09521 | val_0_auc: 0.99171 |  0:00:44s\n","epoch 5  | loss: 0.09468 | val_0_auc: 0.99188 |  0:00:54s\n","epoch 6  | loss: 0.0884  | val_0_auc: 0.99249 |  0:01:02s\n","epoch 7  | loss: 0.08937 | val_0_auc: 0.9924  |  0:01:11s\n","epoch 8  | loss: 0.08548 | val_0_auc: 0.99286 |  0:01:20s\n","epoch 9  | loss: 0.08269 | val_0_auc: 0.99294 |  0:01:29s\n","epoch 10 | loss: 0.08141 | val_0_auc: 0.99316 |  0:01:38s\n","epoch 11 | loss: 0.07938 | val_0_auc: 0.9933  |  0:01:47s\n","epoch 12 | loss: 0.084   | val_0_auc: 0.99309 |  0:01:56s\n","epoch 13 | loss: 0.08559 | val_0_auc: 0.99201 |  0:02:05s\n","epoch 14 | loss: 0.08966 | val_0_auc: 0.99236 |  0:02:14s\n","epoch 15 | loss: 0.08235 | val_0_auc: 0.99333 |  0:02:23s\n","epoch 16 | loss: 0.07936 | val_0_auc: 0.99339 |  0:02:32s\n","epoch 17 | loss: 0.07811 | val_0_auc: 0.99361 |  0:02:41s\n","epoch 18 | loss: 0.07785 | val_0_auc: 0.99358 |  0:02:50s\n","epoch 19 | loss: 0.07819 | val_0_auc: 0.99354 |  0:02:59s\n","epoch 20 | loss: 0.07932 | val_0_auc: 0.99181 |  0:03:08s\n","epoch 21 | loss: 0.07998 | val_0_auc: 0.99128 |  0:03:17s\n","epoch 22 | loss: 0.08032 | val_0_auc: 0.99337 |  0:03:26s\n","epoch 23 | loss: 0.07786 | val_0_auc: 0.99373 |  0:03:34s\n","epoch 24 | loss: 0.07823 | val_0_auc: 0.99363 |  0:03:44s\n","epoch 25 | loss: 0.07539 | val_0_auc: 0.99369 |  0:03:53s\n","epoch 26 | loss: 0.07517 | val_0_auc: 0.99361 |  0:04:02s\n","epoch 27 | loss: 0.0739  | val_0_auc: 0.99392 |  0:04:11s\n","epoch 28 | loss: 0.07231 | val_0_auc: 0.99421 |  0:04:20s\n","epoch 29 | loss: 0.07557 | val_0_auc: 0.9941  |  0:04:29s\n","epoch 30 | loss: 0.07351 | val_0_auc: 0.99384 |  0:04:38s\n","epoch 31 | loss: 0.07369 | val_0_auc: 0.99356 |  0:04:47s\n","epoch 32 | loss: 0.07533 | val_0_auc: 0.99352 |  0:04:56s\n","epoch 33 | loss: 0.07476 | val_0_auc: 0.99395 |  0:05:06s\n","epoch 34 | loss: 0.07282 | val_0_auc: 0.99397 |  0:05:14s\n","epoch 35 | loss: 0.07201 | val_0_auc: 0.99417 |  0:05:23s\n","epoch 36 | loss: 0.07206 | val_0_auc: 0.99362 |  0:05:32s\n","epoch 37 | loss: 0.07151 | val_0_auc: 0.99392 |  0:05:41s\n","epoch 38 | loss: 0.07112 | val_0_auc: 0.99396 |  0:05:50s\n","epoch 39 | loss: 0.07192 | val_0_auc: 0.99392 |  0:05:59s\n","epoch 40 | loss: 0.07496 | val_0_auc: 0.99373 |  0:06:08s\n","epoch 41 | loss: 0.07357 | val_0_auc: 0.9941  |  0:06:17s\n","epoch 42 | loss: 0.07362 | val_0_auc: 0.99431 |  0:06:26s\n","epoch 43 | loss: 0.07127 | val_0_auc: 0.99444 |  0:06:36s\n","epoch 44 | loss: 0.06969 | val_0_auc: 0.99451 |  0:06:45s\n","epoch 45 | loss: 0.06925 | val_0_auc: 0.99452 |  0:06:54s\n","epoch 46 | loss: 0.0695  | val_0_auc: 0.99423 |  0:07:03s\n","epoch 47 | loss: 0.06931 | val_0_auc: 0.99448 |  0:07:12s\n","epoch 48 | loss: 0.07004 | val_0_auc: 0.99426 |  0:07:21s\n","epoch 49 | loss: 0.06945 | val_0_auc: 0.9943  |  0:07:30s\n","epoch 50 | loss: 0.06929 | val_0_auc: 0.99379 |  0:07:39s\n","epoch 51 | loss: 0.06952 | val_0_auc: 0.99415 |  0:07:48s\n","epoch 52 | loss: 0.06872 | val_0_auc: 0.99393 |  0:07:57s\n","epoch 53 | loss: 0.06846 | val_0_auc: 0.9939  |  0:08:06s\n","epoch 54 | loss: 0.06855 | val_0_auc: 0.9941  |  0:08:15s\n","epoch 55 | loss: 0.06813 | val_0_auc: 0.99469 |  0:08:24s\n","epoch 56 | loss: 0.06831 | val_0_auc: 0.9945  |  0:08:33s\n","epoch 57 | loss: 0.07102 | val_0_auc: 0.99436 |  0:08:42s\n","epoch 58 | loss: 0.06956 | val_0_auc: 0.9938  |  0:08:51s\n","epoch 59 | loss: 0.06913 | val_0_auc: 0.99399 |  0:09:00s\n","epoch 60 | loss: 0.06815 | val_0_auc: 0.99365 |  0:09:09s\n","epoch 61 | loss: 0.06841 | val_0_auc: 0.9944  |  0:09:18s\n","epoch 62 | loss: 0.06868 | val_0_auc: 0.99433 |  0:09:27s\n","epoch 63 | loss: 0.06735 | val_0_auc: 0.99449 |  0:09:36s\n","epoch 64 | loss: 0.06748 | val_0_auc: 0.99408 |  0:09:45s\n","epoch 65 | loss: 0.06755 | val_0_auc: 0.99458 |  0:09:54s\n","epoch 66 | loss: 0.06638 | val_0_auc: 0.99452 |  0:10:03s\n","epoch 67 | loss: 0.06746 | val_0_auc: 0.99433 |  0:10:12s\n","epoch 68 | loss: 0.06682 | val_0_auc: 0.99457 |  0:10:21s\n","epoch 69 | loss: 0.0665  | val_0_auc: 0.99451 |  0:10:30s\n","epoch 70 | loss: 0.0667  | val_0_auc: 0.99458 |  0:10:39s\n","epoch 71 | loss: 0.06585 | val_0_auc: 0.99446 |  0:10:48s\n","epoch 72 | loss: 0.06622 | val_0_auc: 0.99449 |  0:10:57s\n","epoch 73 | loss: 0.0661  | val_0_auc: 0.9946  |  0:11:06s\n","epoch 74 | loss: 0.06669 | val_0_auc: 0.99406 |  0:11:14s\n","epoch 75 | loss: 0.06751 | val_0_auc: 0.9946  |  0:11:23s\n","epoch 76 | loss: 0.06644 | val_0_auc: 0.99451 |  0:11:32s\n","epoch 77 | loss: 0.06679 | val_0_auc: 0.99455 |  0:11:41s\n","epoch 78 | loss: 0.06611 | val_0_auc: 0.99451 |  0:11:50s\n","epoch 79 | loss: 0.06602 | val_0_auc: 0.99468 |  0:11:59s\n","epoch 80 | loss: 0.06558 | val_0_auc: 0.99449 |  0:12:08s\n","epoch 81 | loss: 0.0663  | val_0_auc: 0.99456 |  0:12:17s\n","epoch 82 | loss: 0.06644 | val_0_auc: 0.9944  |  0:12:26s\n","epoch 83 | loss: 0.06601 | val_0_auc: 0.99458 |  0:12:35s\n","epoch 84 | loss: 0.0652  | val_0_auc: 0.99449 |  0:12:45s\n","epoch 85 | loss: 0.06543 | val_0_auc: 0.99473 |  0:12:53s\n","epoch 86 | loss: 0.06556 | val_0_auc: 0.99468 |  0:13:02s\n","epoch 87 | loss: 0.06498 | val_0_auc: 0.99484 |  0:13:11s\n","epoch 88 | loss: 0.06532 | val_0_auc: 0.99437 |  0:13:20s\n","epoch 89 | loss: 0.06506 | val_0_auc: 0.99493 |  0:13:29s\n","epoch 90 | loss: 0.06458 | val_0_auc: 0.9947  |  0:13:38s\n","epoch 91 | loss: 0.06554 | val_0_auc: 0.99436 |  0:13:47s\n","epoch 92 | loss: 0.06558 | val_0_auc: 0.99453 |  0:13:56s\n","epoch 93 | loss: 0.06465 | val_0_auc: 0.99427 |  0:14:05s\n","epoch 94 | loss: 0.06584 | val_0_auc: 0.99345 |  0:14:14s\n","epoch 95 | loss: 0.06598 | val_0_auc: 0.99455 |  0:14:24s\n","epoch 96 | loss: 0.06478 | val_0_auc: 0.99477 |  0:14:33s\n","epoch 97 | loss: 0.06449 | val_0_auc: 0.99471 |  0:14:42s\n","epoch 98 | loss: 0.06393 | val_0_auc: 0.9946  |  0:14:51s\n","epoch 99 | loss: 0.06455 | val_0_auc: 0.99476 |  0:15:00s\n","epoch 100| loss: 0.0638  | val_0_auc: 0.99473 |  0:15:08s\n","epoch 101| loss: 0.06378 | val_0_auc: 0.99477 |  0:15:18s\n","epoch 102| loss: 0.06405 | val_0_auc: 0.99439 |  0:15:27s\n","epoch 103| loss: 0.06365 | val_0_auc: 0.99453 |  0:15:36s\n","epoch 104| loss: 0.06357 | val_0_auc: 0.99464 |  0:15:45s\n","epoch 105| loss: 0.06319 | val_0_auc: 0.99451 |  0:15:53s\n","epoch 106| loss: 0.06275 | val_0_auc: 0.99447 |  0:16:02s\n","epoch 107| loss: 0.06297 | val_0_auc: 0.99375 |  0:16:11s\n","epoch 108| loss: 0.06251 | val_0_auc: 0.99449 |  0:16:20s\n","epoch 109| loss: 0.06278 | val_0_auc: 0.9944  |  0:16:29s\n","epoch 110| loss: 0.06412 | val_0_auc: 0.99423 |  0:16:38s\n","epoch 111| loss: 0.06941 | val_0_auc: 0.99456 |  0:16:47s\n","epoch 112| loss: 0.06465 | val_0_auc: 0.99407 |  0:16:56s\n","epoch 113| loss: 0.06654 | val_0_auc: 0.99426 |  0:17:06s\n","epoch 114| loss: 0.06466 | val_0_auc: 0.99445 |  0:17:14s\n","epoch 115| loss: 0.06348 | val_0_auc: 0.99432 |  0:17:24s\n","epoch 116| loss: 0.06341 | val_0_auc: 0.99481 |  0:17:33s\n","epoch 117| loss: 0.06254 | val_0_auc: 0.99458 |  0:17:41s\n","epoch 118| loss: 0.06278 | val_0_auc: 0.99457 |  0:17:51s\n","epoch 119| loss: 0.06218 | val_0_auc: 0.99456 |  0:17:59s\n","\n","Early stopping occurred at epoch 119 with best_epoch = 89 and best_val_0_auc = 0.99493\n"," 33%|███▎      | 5/15 [1:38:38<2:41:29, 968.93s/trial, best loss: -0.9783712546997946]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 40%|████      | 6/15 [1:38:52<2:31:43, 1011.50s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.28513 | val_0_auc: 0.9488  |  0:00:08s\n","epoch 1  | loss: 0.12584 | val_0_auc: 0.99034 |  0:00:18s\n","epoch 2  | loss: 0.11087 | val_0_auc: 0.99102 |  0:00:26s\n","epoch 3  | loss: 0.10149 | val_0_auc: 0.99123 |  0:00:36s\n","epoch 4  | loss: 0.09161 | val_0_auc: 0.99235 |  0:00:45s\n","epoch 5  | loss: 0.08726 | val_0_auc: 0.99299 |  0:00:54s\n","epoch 6  | loss: 0.08476 | val_0_auc: 0.99333 |  0:01:03s\n","epoch 7  | loss: 0.08353 | val_0_auc: 0.99363 |  0:01:12s\n","epoch 8  | loss: 0.08027 | val_0_auc: 0.99366 |  0:01:21s\n","epoch 9  | loss: 0.07887 | val_0_auc: 0.99372 |  0:01:30s\n","epoch 10 | loss: 0.07806 | val_0_auc: 0.9938  |  0:01:39s\n","epoch 11 | loss: 0.07708 | val_0_auc: 0.99397 |  0:01:48s\n","epoch 12 | loss: 0.07748 | val_0_auc: 0.9936  |  0:01:57s\n","epoch 13 | loss: 0.07538 | val_0_auc: 0.99407 |  0:02:06s\n","epoch 14 | loss: 0.07628 | val_0_auc: 0.99359 |  0:02:15s\n","epoch 15 | loss: 0.0759  | val_0_auc: 0.99397 |  0:02:24s\n","epoch 16 | loss: 0.07548 | val_0_auc: 0.99411 |  0:02:33s\n","epoch 17 | loss: 0.07523 | val_0_auc: 0.99374 |  0:02:41s\n","epoch 18 | loss: 0.0761  | val_0_auc: 0.99403 |  0:02:51s\n","epoch 19 | loss: 0.07577 | val_0_auc: 0.99399 |  0:03:00s\n","epoch 20 | loss: 0.07428 | val_0_auc: 0.99389 |  0:03:09s\n","epoch 21 | loss: 0.07442 | val_0_auc: 0.99401 |  0:03:18s\n","epoch 22 | loss: 0.07363 | val_0_auc: 0.99365 |  0:03:27s\n","epoch 23 | loss: 0.0725  | val_0_auc: 0.99388 |  0:03:36s\n","epoch 24 | loss: 0.07321 | val_0_auc: 0.99386 |  0:03:44s\n","epoch 25 | loss: 0.07227 | val_0_auc: 0.9942  |  0:03:54s\n","epoch 26 | loss: 0.07226 | val_0_auc: 0.99393 |  0:04:02s\n","epoch 27 | loss: 0.07159 | val_0_auc: 0.99429 |  0:04:12s\n","epoch 28 | loss: 0.07236 | val_0_auc: 0.99416 |  0:04:21s\n","epoch 29 | loss: 0.07119 | val_0_auc: 0.99437 |  0:04:29s\n","epoch 30 | loss: 0.07077 | val_0_auc: 0.99424 |  0:04:39s\n","epoch 31 | loss: 0.07081 | val_0_auc: 0.99418 |  0:04:47s\n","epoch 32 | loss: 0.07029 | val_0_auc: 0.99439 |  0:04:56s\n","epoch 33 | loss: 0.07011 | val_0_auc: 0.99444 |  0:05:05s\n","epoch 34 | loss: 0.06978 | val_0_auc: 0.9943  |  0:05:14s\n","epoch 35 | loss: 0.06926 | val_0_auc: 0.99452 |  0:05:24s\n","epoch 36 | loss: 0.07005 | val_0_auc: 0.99458 |  0:05:33s\n","epoch 37 | loss: 0.0693  | val_0_auc: 0.99441 |  0:05:42s\n","epoch 38 | loss: 0.07011 | val_0_auc: 0.99456 |  0:05:51s\n","epoch 39 | loss: 0.06897 | val_0_auc: 0.99478 |  0:06:00s\n","epoch 40 | loss: 0.06836 | val_0_auc: 0.9947  |  0:06:09s\n","epoch 41 | loss: 0.06892 | val_0_auc: 0.99446 |  0:06:18s\n","epoch 42 | loss: 0.06804 | val_0_auc: 0.99452 |  0:06:27s\n","epoch 43 | loss: 0.06888 | val_0_auc: 0.99445 |  0:06:36s\n","epoch 44 | loss: 0.06888 | val_0_auc: 0.99324 |  0:06:45s\n","epoch 45 | loss: 0.0681  | val_0_auc: 0.99454 |  0:06:54s\n","epoch 46 | loss: 0.06733 | val_0_auc: 0.99418 |  0:07:03s\n","epoch 47 | loss: 0.0677  | val_0_auc: 0.99473 |  0:07:12s\n","epoch 48 | loss: 0.06988 | val_0_auc: 0.99452 |  0:07:21s\n","epoch 49 | loss: 0.06972 | val_0_auc: 0.99413 |  0:07:30s\n","epoch 50 | loss: 0.0689  | val_0_auc: 0.99458 |  0:07:39s\n","epoch 51 | loss: 0.06775 | val_0_auc: 0.99468 |  0:07:48s\n","epoch 52 | loss: 0.06691 | val_0_auc: 0.99474 |  0:07:57s\n","epoch 53 | loss: 0.06707 | val_0_auc: 0.99454 |  0:08:06s\n","epoch 54 | loss: 0.06611 | val_0_auc: 0.99466 |  0:08:15s\n","epoch 55 | loss: 0.06638 | val_0_auc: 0.99465 |  0:08:24s\n","epoch 56 | loss: 0.06592 | val_0_auc: 0.99469 |  0:08:33s\n","epoch 57 | loss: 0.06579 | val_0_auc: 0.99457 |  0:08:42s\n","epoch 58 | loss: 0.06597 | val_0_auc: 0.9946  |  0:08:51s\n","epoch 59 | loss: 0.06568 | val_0_auc: 0.99451 |  0:09:00s\n","epoch 60 | loss: 0.06533 | val_0_auc: 0.99464 |  0:09:09s\n","epoch 61 | loss: 0.06499 | val_0_auc: 0.99451 |  0:09:18s\n","epoch 62 | loss: 0.06515 | val_0_auc: 0.99421 |  0:09:27s\n","epoch 63 | loss: 0.06506 | val_0_auc: 0.994   |  0:09:36s\n","epoch 64 | loss: 0.06515 | val_0_auc: 0.99431 |  0:09:45s\n","epoch 65 | loss: 0.06459 | val_0_auc: 0.99453 |  0:09:54s\n","epoch 66 | loss: 0.06466 | val_0_auc: 0.99455 |  0:10:03s\n","epoch 67 | loss: 0.06433 | val_0_auc: 0.99442 |  0:10:12s\n","epoch 68 | loss: 0.06433 | val_0_auc: 0.99452 |  0:10:21s\n","epoch 69 | loss: 0.06441 | val_0_auc: 0.99469 |  0:10:30s\n","\n","Early stopping occurred at epoch 69 with best_epoch = 39 and best_val_0_auc = 0.99478\n"," 40%|████      | 6/15 [1:49:23<2:31:43, 1011.50s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 47%|████▋     | 7/15 [1:49:37<1:58:53, 891.69s/trial, best loss: -0.9785816646655564] "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.36218 | val_0_auc: 0.93237 |  0:00:08s\n","epoch 1  | loss: 0.16261 | val_0_auc: 0.98508 |  0:00:17s\n","epoch 2  | loss: 0.12692 | val_0_auc: 0.98814 |  0:00:26s\n","epoch 3  | loss: 0.11617 | val_0_auc: 0.98967 |  0:00:36s\n","epoch 4  | loss: 0.10852 | val_0_auc: 0.98978 |  0:00:45s\n","epoch 5  | loss: 0.10306 | val_0_auc: 0.99105 |  0:00:54s\n","epoch 6  | loss: 0.09716 | val_0_auc: 0.99057 |  0:01:03s\n","epoch 7  | loss: 0.09353 | val_0_auc: 0.99127 |  0:01:12s\n","epoch 8  | loss: 0.09046 | val_0_auc: 0.9918  |  0:01:21s\n","epoch 9  | loss: 0.0889  | val_0_auc: 0.99215 |  0:01:30s\n","epoch 10 | loss: 0.08916 | val_0_auc: 0.99176 |  0:01:39s\n","epoch 11 | loss: 0.08755 | val_0_auc: 0.99214 |  0:01:48s\n","epoch 12 | loss: 0.08564 | val_0_auc: 0.99228 |  0:01:57s\n","epoch 13 | loss: 0.08492 | val_0_auc: 0.99229 |  0:02:06s\n","epoch 14 | loss: 0.08418 | val_0_auc: 0.99201 |  0:02:15s\n","epoch 15 | loss: 0.08295 | val_0_auc: 0.99315 |  0:02:24s\n","epoch 16 | loss: 0.08548 | val_0_auc: 0.99325 |  0:02:33s\n","epoch 17 | loss: 0.08281 | val_0_auc: 0.99278 |  0:02:42s\n","epoch 18 | loss: 0.08168 | val_0_auc: 0.99346 |  0:02:51s\n","epoch 19 | loss: 0.08004 | val_0_auc: 0.99363 |  0:03:00s\n","epoch 20 | loss: 0.07945 | val_0_auc: 0.99376 |  0:03:09s\n","epoch 21 | loss: 0.07805 | val_0_auc: 0.99374 |  0:03:18s\n","epoch 22 | loss: 0.07772 | val_0_auc: 0.99325 |  0:03:27s\n","epoch 23 | loss: 0.078   | val_0_auc: 0.99364 |  0:03:36s\n","epoch 24 | loss: 0.07816 | val_0_auc: 0.99405 |  0:03:45s\n","epoch 25 | loss: 0.07682 | val_0_auc: 0.99309 |  0:03:54s\n","epoch 26 | loss: 0.07631 | val_0_auc: 0.9939  |  0:04:03s\n","epoch 27 | loss: 0.07689 | val_0_auc: 0.99371 |  0:04:12s\n","epoch 28 | loss: 0.07748 | val_0_auc: 0.99399 |  0:04:21s\n","epoch 29 | loss: 0.07506 | val_0_auc: 0.99364 |  0:04:30s\n","epoch 30 | loss: 0.07449 | val_0_auc: 0.99412 |  0:04:39s\n","epoch 31 | loss: 0.0833  | val_0_auc: 0.99036 |  0:04:48s\n","epoch 32 | loss: 0.08043 | val_0_auc: 0.99387 |  0:04:57s\n","epoch 33 | loss: 0.0763  | val_0_auc: 0.99398 |  0:05:06s\n","epoch 34 | loss: 0.07546 | val_0_auc: 0.99407 |  0:05:15s\n","epoch 35 | loss: 0.07436 | val_0_auc: 0.9944  |  0:05:24s\n","epoch 36 | loss: 0.07387 | val_0_auc: 0.99427 |  0:05:33s\n","epoch 37 | loss: 0.07393 | val_0_auc: 0.99458 |  0:05:42s\n","epoch 38 | loss: 0.07414 | val_0_auc: 0.99434 |  0:05:51s\n","epoch 39 | loss: 0.07332 | val_0_auc: 0.99415 |  0:06:00s\n","epoch 40 | loss: 0.07286 | val_0_auc: 0.99442 |  0:06:09s\n","epoch 41 | loss: 0.07402 | val_0_auc: 0.99404 |  0:06:18s\n","epoch 42 | loss: 0.07294 | val_0_auc: 0.99457 |  0:06:27s\n","epoch 43 | loss: 0.07218 | val_0_auc: 0.99463 |  0:06:36s\n","epoch 44 | loss: 0.07342 | val_0_auc: 0.99427 |  0:06:45s\n","epoch 45 | loss: 0.07362 | val_0_auc: 0.99441 |  0:06:54s\n","epoch 46 | loss: 0.07245 | val_0_auc: 0.99428 |  0:07:03s\n","epoch 47 | loss: 0.07222 | val_0_auc: 0.9945  |  0:07:12s\n","epoch 48 | loss: 0.07179 | val_0_auc: 0.99432 |  0:07:21s\n","epoch 49 | loss: 0.07217 | val_0_auc: 0.99457 |  0:07:30s\n","epoch 50 | loss: 0.07169 | val_0_auc: 0.9941  |  0:07:39s\n","epoch 51 | loss: 0.07144 | val_0_auc: 0.99449 |  0:07:47s\n","epoch 52 | loss: 0.07166 | val_0_auc: 0.99427 |  0:07:57s\n","epoch 53 | loss: 0.0719  | val_0_auc: 0.99458 |  0:08:05s\n","epoch 54 | loss: 0.07124 | val_0_auc: 0.99465 |  0:08:14s\n","epoch 55 | loss: 0.07193 | val_0_auc: 0.99449 |  0:08:23s\n","epoch 56 | loss: 0.07158 | val_0_auc: 0.99458 |  0:08:32s\n","epoch 57 | loss: 0.07067 | val_0_auc: 0.99433 |  0:08:41s\n","epoch 58 | loss: 0.07123 | val_0_auc: 0.99422 |  0:08:50s\n","epoch 59 | loss: 0.07072 | val_0_auc: 0.99408 |  0:08:59s\n","epoch 60 | loss: 0.07097 | val_0_auc: 0.9943  |  0:09:08s\n","epoch 61 | loss: 0.07119 | val_0_auc: 0.99463 |  0:09:17s\n","epoch 62 | loss: 0.07066 | val_0_auc: 0.99454 |  0:09:26s\n","epoch 63 | loss: 0.07076 | val_0_auc: 0.99473 |  0:09:35s\n","epoch 64 | loss: 0.07105 | val_0_auc: 0.99427 |  0:09:44s\n","epoch 65 | loss: 0.07075 | val_0_auc: 0.99466 |  0:09:53s\n","epoch 66 | loss: 0.07031 | val_0_auc: 0.99434 |  0:10:02s\n","epoch 67 | loss: 0.07026 | val_0_auc: 0.99432 |  0:10:11s\n","epoch 68 | loss: 0.07004 | val_0_auc: 0.99468 |  0:10:20s\n","epoch 69 | loss: 0.07008 | val_0_auc: 0.99469 |  0:10:29s\n","epoch 70 | loss: 0.06978 | val_0_auc: 0.99449 |  0:10:38s\n","epoch 71 | loss: 0.07032 | val_0_auc: 0.99424 |  0:10:47s\n","epoch 72 | loss: 0.06943 | val_0_auc: 0.9946  |  0:10:56s\n","epoch 73 | loss: 0.0819  | val_0_auc: 0.99375 |  0:11:04s\n","epoch 74 | loss: 0.07396 | val_0_auc: 0.99421 |  0:11:13s\n","epoch 75 | loss: 0.07333 | val_0_auc: 0.99438 |  0:11:22s\n","epoch 76 | loss: 0.07097 | val_0_auc: 0.99426 |  0:11:31s\n","epoch 77 | loss: 0.07071 | val_0_auc: 0.99451 |  0:11:40s\n","epoch 78 | loss: 0.07015 | val_0_auc: 0.99457 |  0:11:49s\n","epoch 79 | loss: 0.07001 | val_0_auc: 0.99464 |  0:11:58s\n","epoch 80 | loss: 0.06939 | val_0_auc: 0.9946  |  0:12:07s\n","epoch 81 | loss: 0.07022 | val_0_auc: 0.99476 |  0:12:16s\n","epoch 82 | loss: 0.06921 | val_0_auc: 0.99469 |  0:12:25s\n","epoch 83 | loss: 0.06903 | val_0_auc: 0.9947  |  0:12:34s\n","epoch 84 | loss: 0.06926 | val_0_auc: 0.99469 |  0:12:43s\n","epoch 85 | loss: 0.06883 | val_0_auc: 0.99475 |  0:12:52s\n","epoch 86 | loss: 0.0689  | val_0_auc: 0.9946  |  0:13:01s\n","epoch 87 | loss: 0.06842 | val_0_auc: 0.99468 |  0:13:10s\n","epoch 88 | loss: 0.06896 | val_0_auc: 0.99458 |  0:13:19s\n","epoch 89 | loss: 0.06856 | val_0_auc: 0.99475 |  0:13:28s\n","epoch 90 | loss: 0.06859 | val_0_auc: 0.99478 |  0:13:37s\n","epoch 91 | loss: 0.06828 | val_0_auc: 0.99459 |  0:13:46s\n","epoch 92 | loss: 0.06913 | val_0_auc: 0.99481 |  0:13:55s\n","epoch 93 | loss: 0.06899 | val_0_auc: 0.99475 |  0:14:03s\n","epoch 94 | loss: 0.06849 | val_0_auc: 0.99429 |  0:14:12s\n","epoch 95 | loss: 0.06892 | val_0_auc: 0.99412 |  0:14:21s\n","epoch 96 | loss: 0.06851 | val_0_auc: 0.99492 |  0:14:30s\n","epoch 97 | loss: 0.06815 | val_0_auc: 0.99457 |  0:14:39s\n","epoch 98 | loss: 0.06774 | val_0_auc: 0.99466 |  0:14:48s\n","epoch 99 | loss: 0.07052 | val_0_auc: 0.99477 |  0:14:56s\n","epoch 100| loss: 0.06864 | val_0_auc: 0.99475 |  0:15:05s\n","epoch 101| loss: 0.0682  | val_0_auc: 0.9945  |  0:15:14s\n","epoch 102| loss: 0.06792 | val_0_auc: 0.99486 |  0:15:23s\n","epoch 103| loss: 0.06809 | val_0_auc: 0.99463 |  0:15:32s\n","epoch 104| loss: 0.06842 | val_0_auc: 0.99435 |  0:15:41s\n","epoch 105| loss: 0.06802 | val_0_auc: 0.99479 |  0:15:50s\n","epoch 106| loss: 0.0675  | val_0_auc: 0.99458 |  0:15:59s\n","epoch 107| loss: 0.068   | val_0_auc: 0.99455 |  0:16:08s\n","epoch 108| loss: 0.06783 | val_0_auc: 0.99412 |  0:16:17s\n","epoch 109| loss: 0.06746 | val_0_auc: 0.99476 |  0:16:26s\n","epoch 110| loss: 0.06766 | val_0_auc: 0.99456 |  0:16:35s\n","epoch 111| loss: 0.06842 | val_0_auc: 0.99458 |  0:16:44s\n","epoch 112| loss: 0.06859 | val_0_auc: 0.99447 |  0:16:53s\n","epoch 113| loss: 0.06796 | val_0_auc: 0.99487 |  0:17:02s\n","epoch 114| loss: 0.06884 | val_0_auc: 0.99429 |  0:17:10s\n","epoch 115| loss: 0.06759 | val_0_auc: 0.99488 |  0:17:19s\n","epoch 116| loss: 0.06708 | val_0_auc: 0.99487 |  0:17:29s\n","epoch 117| loss: 0.06702 | val_0_auc: 0.99495 |  0:17:38s\n","epoch 118| loss: 0.06734 | val_0_auc: 0.99488 |  0:17:47s\n","epoch 119| loss: 0.06712 | val_0_auc: 0.99485 |  0:17:56s\n","epoch 120| loss: 0.06739 | val_0_auc: 0.99463 |  0:18:05s\n","epoch 121| loss: 0.06729 | val_0_auc: 0.99495 |  0:18:14s\n","epoch 122| loss: 0.06699 | val_0_auc: 0.99499 |  0:18:22s\n","epoch 123| loss: 0.06648 | val_0_auc: 0.99423 |  0:18:32s\n","epoch 124| loss: 0.06653 | val_0_auc: 0.99494 |  0:18:41s\n","epoch 125| loss: 0.06671 | val_0_auc: 0.99471 |  0:18:49s\n","epoch 126| loss: 0.06667 | val_0_auc: 0.99479 |  0:18:58s\n","epoch 127| loss: 0.0666  | val_0_auc: 0.99505 |  0:19:07s\n","epoch 128| loss: 0.07329 | val_0_auc: 0.99436 |  0:19:16s\n","epoch 129| loss: 0.0669  | val_0_auc: 0.9947  |  0:19:25s\n","epoch 130| loss: 0.06692 | val_0_auc: 0.99469 |  0:19:34s\n","epoch 131| loss: 0.06667 | val_0_auc: 0.99485 |  0:19:43s\n","epoch 132| loss: 0.06674 | val_0_auc: 0.99491 |  0:19:52s\n","epoch 133| loss: 0.06662 | val_0_auc: 0.99305 |  0:20:01s\n","epoch 134| loss: 0.06651 | val_0_auc: 0.9948  |  0:20:10s\n","epoch 135| loss: 0.06621 | val_0_auc: 0.99216 |  0:20:19s\n","epoch 136| loss: 0.0663  | val_0_auc: 0.9912  |  0:20:28s\n","epoch 137| loss: 0.06623 | val_0_auc: 0.9946  |  0:20:37s\n","epoch 138| loss: 0.06607 | val_0_auc: 0.99488 |  0:20:46s\n","epoch 139| loss: 0.06621 | val_0_auc: 0.99285 |  0:20:55s\n","epoch 140| loss: 0.06665 | val_0_auc: 0.99488 |  0:21:04s\n","epoch 141| loss: 0.06653 | val_0_auc: 0.99369 |  0:21:13s\n","epoch 142| loss: 0.0662  | val_0_auc: 0.99467 |  0:21:22s\n","epoch 143| loss: 0.06616 | val_0_auc: 0.99461 |  0:21:31s\n","epoch 144| loss: 0.06627 | val_0_auc: 0.99469 |  0:21:40s\n","epoch 145| loss: 0.06572 | val_0_auc: 0.9949  |  0:21:49s\n","epoch 146| loss: 0.06574 | val_0_auc: 0.99457 |  0:21:58s\n","epoch 147| loss: 0.0666  | val_0_auc: 0.99455 |  0:22:07s\n","epoch 148| loss: 0.06582 | val_0_auc: 0.99484 |  0:22:16s\n","epoch 149| loss: 0.06605 | val_0_auc: 0.99443 |  0:22:25s\n","epoch 150| loss: 0.0662  | val_0_auc: 0.99431 |  0:22:34s\n","epoch 151| loss: 0.06576 | val_0_auc: 0.99358 |  0:22:42s\n","epoch 152| loss: 0.06529 | val_0_auc: 0.99478 |  0:22:51s\n","epoch 153| loss: 0.06558 | val_0_auc: 0.99498 |  0:23:00s\n","epoch 154| loss: 0.06562 | val_0_auc: 0.99438 |  0:23:09s\n","epoch 155| loss: 0.06606 | val_0_auc: 0.99459 |  0:23:18s\n","epoch 156| loss: 0.06609 | val_0_auc: 0.99452 |  0:23:27s\n","epoch 157| loss: 0.06579 | val_0_auc: 0.99492 |  0:23:36s\n","\n","Early stopping occurred at epoch 157 with best_epoch = 127 and best_val_0_auc = 0.99505\n"," 47%|████▋     | 7/15 [2:13:14<1:58:53, 891.69s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 53%|█████▎    | 8/15 [2:13:28<2:04:03, 1063.34s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.30379 | val_0_auc: 0.97343 |  0:00:09s\n","epoch 1  | loss: 0.12909 | val_0_auc: 0.98818 |  0:00:18s\n","epoch 2  | loss: 0.1091  | val_0_auc: 0.99023 |  0:00:28s\n","epoch 3  | loss: 0.09914 | val_0_auc: 0.99127 |  0:00:37s\n","epoch 4  | loss: 0.09567 | val_0_auc: 0.99158 |  0:00:46s\n","epoch 5  | loss: 0.09271 | val_0_auc: 0.99162 |  0:00:55s\n","epoch 6  | loss: 0.09074 | val_0_auc: 0.9922  |  0:01:04s\n","epoch 7  | loss: 0.09039 | val_0_auc: 0.99202 |  0:01:13s\n","epoch 8  | loss: 0.08802 | val_0_auc: 0.99189 |  0:01:22s\n","epoch 9  | loss: 0.09012 | val_0_auc: 0.99129 |  0:01:31s\n","epoch 10 | loss: 0.08844 | val_0_auc: 0.99241 |  0:01:40s\n","epoch 11 | loss: 0.08483 | val_0_auc: 0.99268 |  0:01:49s\n","epoch 12 | loss: 0.08294 | val_0_auc: 0.99249 |  0:01:58s\n","epoch 13 | loss: 0.08037 | val_0_auc: 0.99304 |  0:02:07s\n","epoch 14 | loss: 0.0809  | val_0_auc: 0.99297 |  0:02:17s\n","epoch 15 | loss: 0.07978 | val_0_auc: 0.9932  |  0:02:26s\n","epoch 16 | loss: 0.07868 | val_0_auc: 0.99327 |  0:02:35s\n","epoch 17 | loss: 0.07828 | val_0_auc: 0.99323 |  0:02:44s\n","epoch 18 | loss: 0.07785 | val_0_auc: 0.99274 |  0:02:53s\n","epoch 19 | loss: 0.0766  | val_0_auc: 0.99366 |  0:03:02s\n","epoch 20 | loss: 0.07706 | val_0_auc: 0.99372 |  0:03:10s\n","epoch 21 | loss: 0.07566 | val_0_auc: 0.99376 |  0:03:20s\n","epoch 22 | loss: 0.07608 | val_0_auc: 0.99374 |  0:03:29s\n","epoch 23 | loss: 0.07502 | val_0_auc: 0.99371 |  0:03:38s\n","epoch 24 | loss: 0.0746  | val_0_auc: 0.99333 |  0:03:47s\n","epoch 25 | loss: 0.07466 | val_0_auc: 0.99344 |  0:03:55s\n","epoch 26 | loss: 0.07654 | val_0_auc: 0.99359 |  0:04:05s\n","epoch 27 | loss: 0.07395 | val_0_auc: 0.99382 |  0:04:13s\n","epoch 28 | loss: 0.07572 | val_0_auc: 0.99412 |  0:04:22s\n","epoch 29 | loss: 0.07446 | val_0_auc: 0.99355 |  0:04:31s\n","epoch 30 | loss: 0.07606 | val_0_auc: 0.99405 |  0:04:40s\n","epoch 31 | loss: 0.07282 | val_0_auc: 0.99425 |  0:04:49s\n","epoch 32 | loss: 0.07253 | val_0_auc: 0.9939  |  0:04:58s\n","epoch 33 | loss: 0.07171 | val_0_auc: 0.99409 |  0:05:07s\n","epoch 34 | loss: 0.07257 | val_0_auc: 0.99369 |  0:05:16s\n","epoch 35 | loss: 0.07118 | val_0_auc: 0.99414 |  0:05:25s\n","epoch 36 | loss: 0.07176 | val_0_auc: 0.99444 |  0:05:35s\n","epoch 37 | loss: 0.07092 | val_0_auc: 0.99432 |  0:05:44s\n","epoch 38 | loss: 0.07126 | val_0_auc: 0.99392 |  0:05:53s\n","epoch 39 | loss: 0.07014 | val_0_auc: 0.99441 |  0:06:03s\n","epoch 40 | loss: 0.06991 | val_0_auc: 0.99463 |  0:06:12s\n","epoch 41 | loss: 0.06988 | val_0_auc: 0.99443 |  0:06:21s\n","epoch 42 | loss: 0.06968 | val_0_auc: 0.9944  |  0:06:31s\n","epoch 43 | loss: 0.07057 | val_0_auc: 0.99437 |  0:06:40s\n","epoch 44 | loss: 0.06949 | val_0_auc: 0.99461 |  0:06:49s\n","epoch 45 | loss: 0.06934 | val_0_auc: 0.99426 |  0:06:58s\n","epoch 46 | loss: 0.06907 | val_0_auc: 0.99436 |  0:07:07s\n","epoch 47 | loss: 0.07016 | val_0_auc: 0.99463 |  0:07:17s\n","epoch 48 | loss: 0.07137 | val_0_auc: 0.99428 |  0:07:26s\n","epoch 49 | loss: 0.07012 | val_0_auc: 0.99403 |  0:07:35s\n","epoch 50 | loss: 0.07006 | val_0_auc: 0.99425 |  0:07:45s\n","epoch 51 | loss: 0.06858 | val_0_auc: 0.99453 |  0:07:54s\n","epoch 52 | loss: 0.06873 | val_0_auc: 0.99437 |  0:08:03s\n","epoch 53 | loss: 0.06905 | val_0_auc: 0.99427 |  0:08:12s\n","epoch 54 | loss: 0.06906 | val_0_auc: 0.99447 |  0:08:21s\n","epoch 55 | loss: 0.06864 | val_0_auc: 0.99416 |  0:08:30s\n","epoch 56 | loss: 0.06841 | val_0_auc: 0.99445 |  0:08:39s\n","epoch 57 | loss: 0.06832 | val_0_auc: 0.99407 |  0:08:49s\n","epoch 58 | loss: 0.06834 | val_0_auc: 0.99418 |  0:08:58s\n","epoch 59 | loss: 0.06831 | val_0_auc: 0.9944  |  0:09:07s\n","epoch 60 | loss: 0.06839 | val_0_auc: 0.9938  |  0:09:16s\n","epoch 61 | loss: 0.06946 | val_0_auc: 0.99447 |  0:09:25s\n","epoch 62 | loss: 0.06927 | val_0_auc: 0.99393 |  0:09:34s\n","epoch 63 | loss: 0.0685  | val_0_auc: 0.99397 |  0:09:43s\n","epoch 64 | loss: 0.06841 | val_0_auc: 0.99451 |  0:09:52s\n","epoch 65 | loss: 0.06796 | val_0_auc: 0.99439 |  0:10:01s\n","epoch 66 | loss: 0.06773 | val_0_auc: 0.9935  |  0:10:11s\n","epoch 67 | loss: 0.06831 | val_0_auc: 0.9942  |  0:10:20s\n","epoch 68 | loss: 0.06702 | val_0_auc: 0.99408 |  0:10:29s\n","epoch 69 | loss: 0.06708 | val_0_auc: 0.99373 |  0:10:38s\n","epoch 70 | loss: 0.06769 | val_0_auc: 0.99461 |  0:10:47s\n","epoch 71 | loss: 0.06732 | val_0_auc: 0.99461 |  0:10:56s\n","epoch 72 | loss: 0.06689 | val_0_auc: 0.99447 |  0:11:05s\n","epoch 73 | loss: 0.06735 | val_0_auc: 0.99409 |  0:11:14s\n","epoch 74 | loss: 0.0673  | val_0_auc: 0.99461 |  0:11:23s\n","epoch 75 | loss: 0.06898 | val_0_auc: 0.99433 |  0:11:32s\n","epoch 76 | loss: 0.06819 | val_0_auc: 0.99447 |  0:11:41s\n","epoch 77 | loss: 0.06762 | val_0_auc: 0.99444 |  0:11:51s\n","\n","Early stopping occurred at epoch 77 with best_epoch = 47 and best_val_0_auc = 0.99463\n"," 53%|█████▎    | 8/15 [2:25:19<2:04:03, 1063.34s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 60%|██████    | 9/15 [2:25:33<1:35:45, 957.64s/trial, best loss: -0.9785816646655564] "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.31089 | val_0_auc: 0.96027 |  0:00:08s\n","epoch 1  | loss: 0.13865 | val_0_auc: 0.98917 |  0:00:17s\n","epoch 2  | loss: 0.11253 | val_0_auc: 0.99089 |  0:00:26s\n","epoch 3  | loss: 0.10187 | val_0_auc: 0.99174 |  0:00:35s\n","epoch 4  | loss: 0.09626 | val_0_auc: 0.99167 |  0:00:44s\n","epoch 5  | loss: 0.09229 | val_0_auc: 0.9917  |  0:00:53s\n","epoch 6  | loss: 0.08848 | val_0_auc: 0.99248 |  0:01:02s\n","epoch 7  | loss: 0.0875  | val_0_auc: 0.99259 |  0:01:11s\n","epoch 8  | loss: 0.08761 | val_0_auc: 0.99221 |  0:01:20s\n","epoch 9  | loss: 0.0847  | val_0_auc: 0.99248 |  0:01:29s\n","epoch 10 | loss: 0.08237 | val_0_auc: 0.99254 |  0:01:38s\n","epoch 11 | loss: 0.08141 | val_0_auc: 0.99288 |  0:01:47s\n","epoch 12 | loss: 0.08108 | val_0_auc: 0.99307 |  0:01:56s\n","epoch 13 | loss: 0.07962 | val_0_auc: 0.99307 |  0:02:06s\n","epoch 14 | loss: 0.07867 | val_0_auc: 0.99292 |  0:02:15s\n","epoch 15 | loss: 0.07765 | val_0_auc: 0.99347 |  0:02:24s\n","epoch 16 | loss: 0.07929 | val_0_auc: 0.99322 |  0:02:33s\n","epoch 17 | loss: 0.07809 | val_0_auc: 0.99322 |  0:02:42s\n","epoch 18 | loss: 0.07695 | val_0_auc: 0.99369 |  0:02:51s\n","epoch 19 | loss: 0.0758  | val_0_auc: 0.99382 |  0:03:00s\n","epoch 20 | loss: 0.07523 | val_0_auc: 0.99378 |  0:03:09s\n","epoch 21 | loss: 0.07423 | val_0_auc: 0.99344 |  0:03:18s\n","epoch 22 | loss: 0.07472 | val_0_auc: 0.99363 |  0:03:27s\n","epoch 23 | loss: 0.07456 | val_0_auc: 0.9935  |  0:03:36s\n","epoch 24 | loss: 0.07677 | val_0_auc: 0.99241 |  0:03:44s\n","epoch 25 | loss: 0.07646 | val_0_auc: 0.99269 |  0:03:53s\n","epoch 26 | loss: 0.07555 | val_0_auc: 0.9942  |  0:04:02s\n","epoch 27 | loss: 0.07411 | val_0_auc: 0.9943  |  0:04:11s\n","epoch 28 | loss: 0.0731  | val_0_auc: 0.99413 |  0:04:20s\n","epoch 29 | loss: 0.07337 | val_0_auc: 0.99433 |  0:04:29s\n","epoch 30 | loss: 0.07161 | val_0_auc: 0.99445 |  0:04:39s\n","epoch 31 | loss: 0.07135 | val_0_auc: 0.99388 |  0:04:48s\n","epoch 32 | loss: 0.07093 | val_0_auc: 0.99415 |  0:04:57s\n","epoch 33 | loss: 0.07593 | val_0_auc: 0.99351 |  0:05:06s\n","epoch 34 | loss: 0.07301 | val_0_auc: 0.99414 |  0:05:14s\n","epoch 35 | loss: 0.0734  | val_0_auc: 0.99415 |  0:05:23s\n","epoch 36 | loss: 0.0721  | val_0_auc: 0.9939  |  0:05:32s\n","epoch 37 | loss: 0.07164 | val_0_auc: 0.99423 |  0:05:41s\n","epoch 38 | loss: 0.07038 | val_0_auc: 0.99414 |  0:05:50s\n","epoch 39 | loss: 0.07129 | val_0_auc: 0.99451 |  0:05:59s\n","epoch 40 | loss: 0.07107 | val_0_auc: 0.99413 |  0:06:09s\n","epoch 41 | loss: 0.0708  | val_0_auc: 0.99458 |  0:06:18s\n","epoch 42 | loss: 0.06955 | val_0_auc: 0.99449 |  0:06:27s\n","epoch 43 | loss: 0.06924 | val_0_auc: 0.99472 |  0:06:36s\n","epoch 44 | loss: 0.06877 | val_0_auc: 0.99479 |  0:06:45s\n","epoch 45 | loss: 0.06971 | val_0_auc: 0.99467 |  0:06:54s\n","epoch 46 | loss: 0.06861 | val_0_auc: 0.99452 |  0:07:03s\n","epoch 47 | loss: 0.06894 | val_0_auc: 0.99458 |  0:07:12s\n","epoch 48 | loss: 0.06797 | val_0_auc: 0.99477 |  0:07:21s\n","epoch 49 | loss: 0.06805 | val_0_auc: 0.99475 |  0:07:30s\n","epoch 50 | loss: 0.06832 | val_0_auc: 0.99447 |  0:07:39s\n","epoch 51 | loss: 0.06824 | val_0_auc: 0.99478 |  0:07:48s\n","epoch 52 | loss: 0.06789 | val_0_auc: 0.99476 |  0:07:57s\n","epoch 53 | loss: 0.06823 | val_0_auc: 0.99491 |  0:08:06s\n","epoch 54 | loss: 0.0675  | val_0_auc: 0.99493 |  0:08:15s\n","epoch 55 | loss: 0.06702 | val_0_auc: 0.99498 |  0:08:24s\n","epoch 56 | loss: 0.06673 | val_0_auc: 0.99499 |  0:08:33s\n","epoch 57 | loss: 0.06629 | val_0_auc: 0.995   |  0:08:42s\n","epoch 58 | loss: 0.0664  | val_0_auc: 0.99478 |  0:08:51s\n","epoch 59 | loss: 0.06628 | val_0_auc: 0.99491 |  0:09:00s\n","epoch 60 | loss: 0.0665  | val_0_auc: 0.99487 |  0:09:09s\n","epoch 61 | loss: 0.06522 | val_0_auc: 0.99482 |  0:09:18s\n","epoch 62 | loss: 0.06527 | val_0_auc: 0.9949  |  0:09:27s\n","epoch 63 | loss: 0.0653  | val_0_auc: 0.99485 |  0:09:36s\n","epoch 64 | loss: 0.06607 | val_0_auc: 0.99483 |  0:09:45s\n","epoch 65 | loss: 0.06608 | val_0_auc: 0.99482 |  0:09:54s\n","epoch 66 | loss: 0.06509 | val_0_auc: 0.99507 |  0:10:03s\n","epoch 67 | loss: 0.06519 | val_0_auc: 0.99487 |  0:10:12s\n","epoch 68 | loss: 0.06561 | val_0_auc: 0.99496 |  0:10:21s\n","epoch 69 | loss: 0.0648  | val_0_auc: 0.99497 |  0:10:30s\n","epoch 70 | loss: 0.06426 | val_0_auc: 0.99482 |  0:10:39s\n","epoch 71 | loss: 0.06486 | val_0_auc: 0.99492 |  0:10:48s\n","epoch 72 | loss: 0.0641  | val_0_auc: 0.99456 |  0:10:57s\n","epoch 73 | loss: 0.06377 | val_0_auc: 0.9947  |  0:11:06s\n","epoch 74 | loss: 0.06386 | val_0_auc: 0.99463 |  0:11:15s\n","epoch 75 | loss: 0.06314 | val_0_auc: 0.99486 |  0:11:24s\n","epoch 76 | loss: 0.06445 | val_0_auc: 0.99464 |  0:11:33s\n","epoch 77 | loss: 0.06422 | val_0_auc: 0.99443 |  0:11:42s\n","epoch 78 | loss: 0.06388 | val_0_auc: 0.99478 |  0:11:51s\n","epoch 79 | loss: 0.06481 | val_0_auc: 0.99469 |  0:12:00s\n","epoch 80 | loss: 0.06385 | val_0_auc: 0.99457 |  0:12:09s\n","epoch 81 | loss: 0.06292 | val_0_auc: 0.99476 |  0:12:18s\n","epoch 82 | loss: 0.06293 | val_0_auc: 0.9947  |  0:12:27s\n","epoch 83 | loss: 0.06319 | val_0_auc: 0.99493 |  0:12:36s\n","epoch 84 | loss: 0.06281 | val_0_auc: 0.99473 |  0:12:45s\n","epoch 85 | loss: 0.06344 | val_0_auc: 0.99404 |  0:12:54s\n","epoch 86 | loss: 0.06419 | val_0_auc: 0.99454 |  0:13:03s\n","epoch 87 | loss: 0.06497 | val_0_auc: 0.9947  |  0:13:12s\n","epoch 88 | loss: 0.06278 | val_0_auc: 0.99468 |  0:13:21s\n","epoch 89 | loss: 0.06317 | val_0_auc: 0.99488 |  0:13:30s\n","epoch 90 | loss: 0.06264 | val_0_auc: 0.9946  |  0:13:39s\n","epoch 91 | loss: 0.06263 | val_0_auc: 0.99481 |  0:13:48s\n","epoch 92 | loss: 0.06245 | val_0_auc: 0.9949  |  0:13:57s\n","epoch 93 | loss: 0.06233 | val_0_auc: 0.99498 |  0:14:06s\n","epoch 94 | loss: 0.06215 | val_0_auc: 0.99472 |  0:14:16s\n","epoch 95 | loss: 0.06296 | val_0_auc: 0.99499 |  0:14:25s\n","epoch 96 | loss: 0.0618  | val_0_auc: 0.99481 |  0:14:34s\n","\n","Early stopping occurred at epoch 96 with best_epoch = 66 and best_val_0_auc = 0.99507\n"," 60%|██████    | 9/15 [2:40:08<1:35:45, 957.64s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 67%|██████▋   | 10/15 [2:40:22<1:18:01, 936.29s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.29035 | val_0_auc: 0.97824 |  0:00:09s\n","epoch 1  | loss: 0.11475 | val_0_auc: 0.9914  |  0:00:17s\n","epoch 2  | loss: 0.09951 | val_0_auc: 0.99121 |  0:00:27s\n","epoch 3  | loss: 0.09367 | val_0_auc: 0.99276 |  0:00:35s\n","epoch 4  | loss: 0.08591 | val_0_auc: 0.99315 |  0:00:44s\n","epoch 5  | loss: 0.08414 | val_0_auc: 0.99304 |  0:00:54s\n","epoch 6  | loss: 0.08092 | val_0_auc: 0.99304 |  0:01:02s\n","epoch 7  | loss: 0.08231 | val_0_auc: 0.99254 |  0:01:11s\n","epoch 8  | loss: 0.08621 | val_0_auc: 0.99287 |  0:01:20s\n","epoch 9  | loss: 0.08034 | val_0_auc: 0.99352 |  0:01:29s\n","epoch 10 | loss: 0.07997 | val_0_auc: 0.9936  |  0:01:38s\n","epoch 11 | loss: 0.08028 | val_0_auc: 0.99364 |  0:01:48s\n","epoch 12 | loss: 0.07752 | val_0_auc: 0.99346 |  0:01:56s\n","epoch 13 | loss: 0.07565 | val_0_auc: 0.99386 |  0:02:05s\n","epoch 14 | loss: 0.07541 | val_0_auc: 0.99349 |  0:02:15s\n","epoch 15 | loss: 0.07621 | val_0_auc: 0.99335 |  0:02:23s\n","epoch 16 | loss: 0.0753  | val_0_auc: 0.9939  |  0:02:32s\n","epoch 17 | loss: 0.0736  | val_0_auc: 0.99429 |  0:02:41s\n","epoch 18 | loss: 0.07317 | val_0_auc: 0.9939  |  0:02:50s\n","epoch 19 | loss: 0.07388 | val_0_auc: 0.99366 |  0:03:00s\n","epoch 20 | loss: 0.07533 | val_0_auc: 0.99347 |  0:03:08s\n","epoch 21 | loss: 0.07311 | val_0_auc: 0.99358 |  0:03:17s\n","epoch 22 | loss: 0.07279 | val_0_auc: 0.99401 |  0:03:27s\n","epoch 23 | loss: 0.07144 | val_0_auc: 0.99295 |  0:03:35s\n","epoch 24 | loss: 0.07205 | val_0_auc: 0.99421 |  0:03:44s\n","epoch 25 | loss: 0.07289 | val_0_auc: 0.99372 |  0:03:53s\n","epoch 26 | loss: 0.07286 | val_0_auc: 0.99398 |  0:04:02s\n","epoch 27 | loss: 0.07595 | val_0_auc: 0.9939  |  0:04:11s\n","epoch 28 | loss: 0.07197 | val_0_auc: 0.994   |  0:04:20s\n","epoch 29 | loss: 0.07124 | val_0_auc: 0.99424 |  0:04:29s\n","epoch 30 | loss: 0.07046 | val_0_auc: 0.99419 |  0:04:38s\n","epoch 31 | loss: 0.07258 | val_0_auc: 0.99342 |  0:04:47s\n","epoch 32 | loss: 0.07674 | val_0_auc: 0.99357 |  0:04:56s\n","epoch 33 | loss: 0.0735  | val_0_auc: 0.99386 |  0:05:05s\n","epoch 34 | loss: 0.0718  | val_0_auc: 0.99421 |  0:05:14s\n","epoch 35 | loss: 0.07037 | val_0_auc: 0.99407 |  0:05:23s\n","epoch 36 | loss: 0.07077 | val_0_auc: 0.99399 |  0:05:32s\n","epoch 37 | loss: 0.07049 | val_0_auc: 0.99438 |  0:05:41s\n","epoch 38 | loss: 0.06879 | val_0_auc: 0.99452 |  0:05:50s\n","epoch 39 | loss: 0.06901 | val_0_auc: 0.9943  |  0:05:59s\n","epoch 40 | loss: 0.06862 | val_0_auc: 0.99462 |  0:06:08s\n","epoch 41 | loss: 0.06888 | val_0_auc: 0.99434 |  0:06:17s\n","epoch 42 | loss: 0.06882 | val_0_auc: 0.99426 |  0:06:26s\n","epoch 43 | loss: 0.07053 | val_0_auc: 0.99385 |  0:06:35s\n","epoch 44 | loss: 0.07    | val_0_auc: 0.99409 |  0:06:44s\n","epoch 45 | loss: 0.06944 | val_0_auc: 0.9947  |  0:06:53s\n","epoch 46 | loss: 0.06867 | val_0_auc: 0.99432 |  0:07:02s\n","epoch 47 | loss: 0.06868 | val_0_auc: 0.99461 |  0:07:11s\n","epoch 48 | loss: 0.06832 | val_0_auc: 0.99461 |  0:07:20s\n","epoch 49 | loss: 0.06898 | val_0_auc: 0.99362 |  0:07:29s\n","epoch 50 | loss: 0.07212 | val_0_auc: 0.99457 |  0:07:38s\n","epoch 51 | loss: 0.06857 | val_0_auc: 0.99456 |  0:07:46s\n","epoch 52 | loss: 0.0677  | val_0_auc: 0.99467 |  0:07:55s\n","epoch 53 | loss: 0.06665 | val_0_auc: 0.99454 |  0:08:05s\n","epoch 54 | loss: 0.07044 | val_0_auc: 0.99454 |  0:08:13s\n","epoch 55 | loss: 0.06827 | val_0_auc: 0.99474 |  0:08:22s\n","epoch 56 | loss: 0.06716 | val_0_auc: 0.99475 |  0:08:31s\n","epoch 57 | loss: 0.06677 | val_0_auc: 0.99482 |  0:08:40s\n","epoch 58 | loss: 0.06578 | val_0_auc: 0.9944  |  0:08:49s\n","epoch 59 | loss: 0.06595 | val_0_auc: 0.99457 |  0:08:58s\n","epoch 60 | loss: 0.06559 | val_0_auc: 0.99448 |  0:09:07s\n","epoch 61 | loss: 0.06513 | val_0_auc: 0.99465 |  0:09:16s\n","epoch 62 | loss: 0.06523 | val_0_auc: 0.99455 |  0:09:25s\n","epoch 63 | loss: 0.06487 | val_0_auc: 0.99432 |  0:09:34s\n","epoch 64 | loss: 0.0649  | val_0_auc: 0.99459 |  0:09:43s\n","epoch 65 | loss: 0.06453 | val_0_auc: 0.99429 |  0:09:52s\n","epoch 66 | loss: 0.06439 | val_0_auc: 0.99456 |  0:10:01s\n","epoch 67 | loss: 0.06443 | val_0_auc: 0.99463 |  0:10:10s\n","epoch 68 | loss: 0.06401 | val_0_auc: 0.99449 |  0:10:19s\n","epoch 69 | loss: 0.06426 | val_0_auc: 0.99506 |  0:10:28s\n","epoch 70 | loss: 0.06394 | val_0_auc: 0.99445 |  0:10:37s\n","epoch 71 | loss: 0.06334 | val_0_auc: 0.99491 |  0:10:46s\n","epoch 72 | loss: 0.06444 | val_0_auc: 0.99422 |  0:10:55s\n","epoch 73 | loss: 0.06351 | val_0_auc: 0.99475 |  0:11:03s\n","epoch 74 | loss: 0.06344 | val_0_auc: 0.99413 |  0:11:13s\n","epoch 75 | loss: 0.06312 | val_0_auc: 0.99502 |  0:11:22s\n","epoch 76 | loss: 0.06265 | val_0_auc: 0.99444 |  0:11:31s\n","epoch 77 | loss: 0.06248 | val_0_auc: 0.99441 |  0:11:40s\n","epoch 78 | loss: 0.06293 | val_0_auc: 0.99463 |  0:11:49s\n","epoch 79 | loss: 0.06254 | val_0_auc: 0.9946  |  0:11:58s\n","epoch 80 | loss: 0.0619  | val_0_auc: 0.99435 |  0:12:07s\n","epoch 81 | loss: 0.06208 | val_0_auc: 0.99433 |  0:12:16s\n","epoch 82 | loss: 0.06214 | val_0_auc: 0.99448 |  0:12:25s\n","epoch 83 | loss: 0.0613  | val_0_auc: 0.99403 |  0:12:33s\n","epoch 84 | loss: 0.06163 | val_0_auc: 0.99422 |  0:12:43s\n","epoch 85 | loss: 0.0615  | val_0_auc: 0.99461 |  0:12:51s\n","epoch 86 | loss: 0.06068 | val_0_auc: 0.99447 |  0:13:00s\n","epoch 87 | loss: 0.06097 | val_0_auc: 0.99417 |  0:13:09s\n","epoch 88 | loss: 0.06099 | val_0_auc: 0.99411 |  0:13:18s\n","epoch 89 | loss: 0.06038 | val_0_auc: 0.99406 |  0:13:27s\n","epoch 90 | loss: 0.06051 | val_0_auc: 0.9942  |  0:13:35s\n","epoch 91 | loss: 0.06027 | val_0_auc: 0.99448 |  0:13:44s\n","epoch 92 | loss: 0.06009 | val_0_auc: 0.99378 |  0:13:53s\n","epoch 93 | loss: 0.06022 | val_0_auc: 0.9942  |  0:14:02s\n","epoch 94 | loss: 0.06041 | val_0_auc: 0.9941  |  0:14:11s\n","epoch 95 | loss: 0.06034 | val_0_auc: 0.99422 |  0:14:20s\n","epoch 96 | loss: 0.05948 | val_0_auc: 0.99398 |  0:14:29s\n","epoch 97 | loss: 0.05977 | val_0_auc: 0.99424 |  0:14:38s\n","epoch 98 | loss: 0.05971 | val_0_auc: 0.99399 |  0:14:47s\n","epoch 99 | loss: 0.05926 | val_0_auc: 0.99354 |  0:14:56s\n","\n","Early stopping occurred at epoch 99 with best_epoch = 69 and best_val_0_auc = 0.99506\n"," 67%|██████▋   | 10/15 [2:55:18<1:18:01, 936.29s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 73%|███████▎  | 11/15 [2:55:32<1:01:53, 928.46s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.36292 | val_0_auc: 0.96387 |  0:00:09s\n","epoch 1  | loss: 0.15792 | val_0_auc: 0.98745 |  0:00:18s\n","epoch 2  | loss: 0.12334 | val_0_auc: 0.9889  |  0:00:27s\n","epoch 3  | loss: 0.11015 | val_0_auc: 0.99038 |  0:00:36s\n","epoch 4  | loss: 0.10166 | val_0_auc: 0.98944 |  0:00:45s\n","epoch 5  | loss: 0.09806 | val_0_auc: 0.99113 |  0:00:53s\n","epoch 6  | loss: 0.09308 | val_0_auc: 0.99177 |  0:01:03s\n","epoch 7  | loss: 0.08963 | val_0_auc: 0.99233 |  0:01:11s\n","epoch 8  | loss: 0.08984 | val_0_auc: 0.99265 |  0:01:20s\n","epoch 9  | loss: 0.08515 | val_0_auc: 0.99256 |  0:01:29s\n","epoch 10 | loss: 0.08513 | val_0_auc: 0.99324 |  0:01:38s\n","epoch 11 | loss: 0.08212 | val_0_auc: 0.99311 |  0:01:47s\n","epoch 12 | loss: 0.0813  | val_0_auc: 0.99344 |  0:01:56s\n","epoch 13 | loss: 0.08058 | val_0_auc: 0.99375 |  0:02:05s\n","epoch 14 | loss: 0.0845  | val_0_auc: 0.99316 |  0:02:14s\n","epoch 15 | loss: 0.07904 | val_0_auc: 0.99334 |  0:02:23s\n","epoch 16 | loss: 0.07792 | val_0_auc: 0.99283 |  0:02:32s\n","epoch 17 | loss: 0.07666 | val_0_auc: 0.99335 |  0:02:41s\n","epoch 18 | loss: 0.0768  | val_0_auc: 0.99329 |  0:02:50s\n","epoch 19 | loss: 0.07692 | val_0_auc: 0.99381 |  0:02:59s\n","epoch 20 | loss: 0.07687 | val_0_auc: 0.99354 |  0:03:08s\n","epoch 21 | loss: 0.07606 | val_0_auc: 0.99368 |  0:03:17s\n","epoch 22 | loss: 0.07768 | val_0_auc: 0.99326 |  0:03:26s\n","epoch 23 | loss: 0.07568 | val_0_auc: 0.99365 |  0:03:35s\n","epoch 24 | loss: 0.07411 | val_0_auc: 0.99387 |  0:03:44s\n","epoch 25 | loss: 0.07467 | val_0_auc: 0.99382 |  0:03:53s\n","epoch 26 | loss: 0.07326 | val_0_auc: 0.99401 |  0:04:02s\n","epoch 27 | loss: 0.07204 | val_0_auc: 0.99388 |  0:04:11s\n","epoch 28 | loss: 0.07296 | val_0_auc: 0.99403 |  0:04:20s\n","epoch 29 | loss: 0.07262 | val_0_auc: 0.99352 |  0:04:29s\n","epoch 30 | loss: 0.07151 | val_0_auc: 0.99412 |  0:04:38s\n","epoch 31 | loss: 0.07177 | val_0_auc: 0.99397 |  0:04:47s\n","epoch 32 | loss: 0.07181 | val_0_auc: 0.9942  |  0:04:56s\n","epoch 33 | loss: 0.07207 | val_0_auc: 0.99415 |  0:05:05s\n","epoch 34 | loss: 0.07168 | val_0_auc: 0.99419 |  0:05:14s\n","epoch 35 | loss: 0.07102 | val_0_auc: 0.99404 |  0:05:23s\n","epoch 36 | loss: 0.07339 | val_0_auc: 0.99425 |  0:05:32s\n","epoch 37 | loss: 0.0712  | val_0_auc: 0.99419 |  0:05:40s\n","epoch 38 | loss: 0.0704  | val_0_auc: 0.99413 |  0:05:49s\n","epoch 39 | loss: 0.06995 | val_0_auc: 0.99417 |  0:05:58s\n","epoch 40 | loss: 0.06974 | val_0_auc: 0.99446 |  0:06:07s\n","epoch 41 | loss: 0.07054 | val_0_auc: 0.99373 |  0:06:16s\n","epoch 42 | loss: 0.07088 | val_0_auc: 0.99456 |  0:06:25s\n","epoch 43 | loss: 0.06966 | val_0_auc: 0.99447 |  0:06:34s\n","epoch 44 | loss: 0.06842 | val_0_auc: 0.9946  |  0:06:43s\n","epoch 45 | loss: 0.06868 | val_0_auc: 0.99462 |  0:06:52s\n","epoch 46 | loss: 0.06944 | val_0_auc: 0.99421 |  0:07:01s\n","epoch 47 | loss: 0.06969 | val_0_auc: 0.99452 |  0:07:10s\n","epoch 48 | loss: 0.06909 | val_0_auc: 0.99464 |  0:07:19s\n","epoch 49 | loss: 0.06826 | val_0_auc: 0.99471 |  0:07:28s\n","epoch 50 | loss: 0.06844 | val_0_auc: 0.99402 |  0:07:37s\n","epoch 51 | loss: 0.06841 | val_0_auc: 0.99467 |  0:07:46s\n","epoch 52 | loss: 0.06772 | val_0_auc: 0.99422 |  0:07:55s\n","epoch 53 | loss: 0.06778 | val_0_auc: 0.99317 |  0:08:04s\n","epoch 54 | loss: 0.06833 | val_0_auc: 0.99463 |  0:08:13s\n","epoch 55 | loss: 0.06739 | val_0_auc: 0.99461 |  0:08:22s\n","epoch 56 | loss: 0.06818 | val_0_auc: 0.99466 |  0:08:31s\n","epoch 57 | loss: 0.06759 | val_0_auc: 0.99473 |  0:08:40s\n","epoch 58 | loss: 0.06743 | val_0_auc: 0.9945  |  0:08:49s\n","epoch 59 | loss: 0.06714 | val_0_auc: 0.99492 |  0:08:58s\n","epoch 60 | loss: 0.0672  | val_0_auc: 0.99467 |  0:09:07s\n","epoch 61 | loss: 0.06697 | val_0_auc: 0.99391 |  0:09:15s\n","epoch 62 | loss: 0.06673 | val_0_auc: 0.99479 |  0:09:24s\n","epoch 63 | loss: 0.06674 | val_0_auc: 0.99466 |  0:09:33s\n","epoch 64 | loss: 0.0663  | val_0_auc: 0.99447 |  0:09:42s\n","epoch 65 | loss: 0.06834 | val_0_auc: 0.99462 |  0:09:51s\n","epoch 66 | loss: 0.06684 | val_0_auc: 0.99393 |  0:10:00s\n","epoch 67 | loss: 0.06661 | val_0_auc: 0.99388 |  0:10:09s\n","epoch 68 | loss: 0.06661 | val_0_auc: 0.99456 |  0:10:18s\n","epoch 69 | loss: 0.06651 | val_0_auc: 0.99474 |  0:10:27s\n","epoch 70 | loss: 0.06605 | val_0_auc: 0.99452 |  0:10:36s\n","epoch 71 | loss: 0.06575 | val_0_auc: 0.99482 |  0:10:45s\n","epoch 72 | loss: 0.06515 | val_0_auc: 0.99472 |  0:10:54s\n","epoch 73 | loss: 0.06553 | val_0_auc: 0.99447 |  0:11:03s\n","epoch 74 | loss: 0.06591 | val_0_auc: 0.99468 |  0:11:12s\n","epoch 75 | loss: 0.06524 | val_0_auc: 0.99498 |  0:11:21s\n","epoch 76 | loss: 0.06559 | val_0_auc: 0.99474 |  0:11:30s\n","epoch 77 | loss: 0.06478 | val_0_auc: 0.99477 |  0:11:39s\n","epoch 78 | loss: 0.06477 | val_0_auc: 0.9946  |  0:11:47s\n","epoch 79 | loss: 0.06478 | val_0_auc: 0.99481 |  0:11:57s\n","epoch 80 | loss: 0.06493 | val_0_auc: 0.99437 |  0:12:06s\n","epoch 81 | loss: 0.06456 | val_0_auc: 0.99469 |  0:12:14s\n","epoch 82 | loss: 0.06468 | val_0_auc: 0.99484 |  0:12:24s\n","epoch 83 | loss: 0.06494 | val_0_auc: 0.99468 |  0:12:33s\n","epoch 84 | loss: 0.0644  | val_0_auc: 0.99501 |  0:12:42s\n","epoch 85 | loss: 0.06478 | val_0_auc: 0.99475 |  0:12:51s\n","epoch 86 | loss: 0.06481 | val_0_auc: 0.99483 |  0:13:00s\n","epoch 87 | loss: 0.06425 | val_0_auc: 0.99487 |  0:13:08s\n","epoch 88 | loss: 0.06439 | val_0_auc: 0.9948  |  0:13:17s\n","epoch 89 | loss: 0.06376 | val_0_auc: 0.99464 |  0:13:26s\n","epoch 90 | loss: 0.06341 | val_0_auc: 0.99482 |  0:13:35s\n","epoch 91 | loss: 0.06406 | val_0_auc: 0.99481 |  0:13:44s\n","epoch 92 | loss: 0.06308 | val_0_auc: 0.99481 |  0:13:53s\n","epoch 93 | loss: 0.06308 | val_0_auc: 0.99485 |  0:14:02s\n","epoch 94 | loss: 0.06343 | val_0_auc: 0.99478 |  0:14:11s\n","epoch 95 | loss: 0.06315 | val_0_auc: 0.99497 |  0:14:20s\n","epoch 96 | loss: 0.06364 | val_0_auc: 0.99484 |  0:14:29s\n","epoch 97 | loss: 0.06368 | val_0_auc: 0.99497 |  0:14:37s\n","epoch 98 | loss: 0.06311 | val_0_auc: 0.99464 |  0:14:46s\n","epoch 99 | loss: 0.06333 | val_0_auc: 0.99473 |  0:14:55s\n","epoch 100| loss: 0.06329 | val_0_auc: 0.9949  |  0:15:04s\n","epoch 101| loss: 0.06284 | val_0_auc: 0.99459 |  0:15:13s\n","epoch 102| loss: 0.06439 | val_0_auc: 0.9943  |  0:15:22s\n","epoch 103| loss: 0.06332 | val_0_auc: 0.99496 |  0:15:31s\n","epoch 104| loss: 0.0633  | val_0_auc: 0.99483 |  0:15:40s\n","epoch 105| loss: 0.06323 | val_0_auc: 0.99482 |  0:15:49s\n","epoch 106| loss: 0.06253 | val_0_auc: 0.99456 |  0:15:58s\n","epoch 107| loss: 0.0626  | val_0_auc: 0.99458 |  0:16:07s\n","epoch 108| loss: 0.06189 | val_0_auc: 0.99485 |  0:16:16s\n","epoch 109| loss: 0.06161 | val_0_auc: 0.99493 |  0:16:25s\n","epoch 110| loss: 0.06432 | val_0_auc: 0.99443 |  0:16:34s\n","epoch 111| loss: 0.06263 | val_0_auc: 0.99476 |  0:16:43s\n","epoch 112| loss: 0.06217 | val_0_auc: 0.99459 |  0:16:52s\n","epoch 113| loss: 0.0619  | val_0_auc: 0.99489 |  0:17:01s\n","epoch 114| loss: 0.06169 | val_0_auc: 0.99463 |  0:17:10s\n","\n","Early stopping occurred at epoch 114 with best_epoch = 84 and best_val_0_auc = 0.99501\n"," 73%|███████▎  | 11/15 [3:12:43<1:01:53, 928.46s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 80%|████████  | 12/15 [3:12:57<48:11, 963.68s/trial, best loss: -0.9785816646655564]  "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.28513 | val_0_auc: 0.9488  |  0:00:09s\n","epoch 1  | loss: 0.12584 | val_0_auc: 0.99034 |  0:00:17s\n","epoch 2  | loss: 0.11087 | val_0_auc: 0.99102 |  0:00:26s\n","epoch 3  | loss: 0.10149 | val_0_auc: 0.99123 |  0:00:35s\n","epoch 4  | loss: 0.09161 | val_0_auc: 0.99235 |  0:00:44s\n","epoch 5  | loss: 0.08726 | val_0_auc: 0.99299 |  0:00:53s\n","epoch 6  | loss: 0.08476 | val_0_auc: 0.99333 |  0:01:02s\n","epoch 7  | loss: 0.08353 | val_0_auc: 0.99363 |  0:01:11s\n","epoch 8  | loss: 0.08027 | val_0_auc: 0.99366 |  0:01:20s\n","epoch 9  | loss: 0.07887 | val_0_auc: 0.99372 |  0:01:29s\n","epoch 10 | loss: 0.07806 | val_0_auc: 0.9938  |  0:01:38s\n","epoch 11 | loss: 0.07708 | val_0_auc: 0.99397 |  0:01:47s\n","epoch 12 | loss: 0.07748 | val_0_auc: 0.9936  |  0:01:56s\n","epoch 13 | loss: 0.07538 | val_0_auc: 0.99407 |  0:02:05s\n","epoch 14 | loss: 0.07628 | val_0_auc: 0.99359 |  0:02:14s\n","epoch 15 | loss: 0.0759  | val_0_auc: 0.99397 |  0:02:23s\n","epoch 16 | loss: 0.07548 | val_0_auc: 0.99411 |  0:02:32s\n","epoch 17 | loss: 0.07523 | val_0_auc: 0.99374 |  0:02:41s\n","epoch 18 | loss: 0.0761  | val_0_auc: 0.99403 |  0:02:50s\n","epoch 19 | loss: 0.07577 | val_0_auc: 0.99399 |  0:02:59s\n","epoch 20 | loss: 0.07428 | val_0_auc: 0.99389 |  0:03:08s\n","epoch 21 | loss: 0.07442 | val_0_auc: 0.99401 |  0:03:17s\n","epoch 22 | loss: 0.07363 | val_0_auc: 0.99365 |  0:03:26s\n","epoch 23 | loss: 0.0725  | val_0_auc: 0.99388 |  0:03:35s\n","epoch 24 | loss: 0.07321 | val_0_auc: 0.99386 |  0:03:43s\n","epoch 25 | loss: 0.07227 | val_0_auc: 0.9942  |  0:03:52s\n","epoch 26 | loss: 0.07226 | val_0_auc: 0.99393 |  0:04:01s\n","epoch 27 | loss: 0.07159 | val_0_auc: 0.99429 |  0:04:10s\n","epoch 28 | loss: 0.07236 | val_0_auc: 0.99416 |  0:04:19s\n","epoch 29 | loss: 0.07119 | val_0_auc: 0.99437 |  0:04:28s\n","epoch 30 | loss: 0.07077 | val_0_auc: 0.99424 |  0:04:37s\n","epoch 31 | loss: 0.07081 | val_0_auc: 0.99418 |  0:04:46s\n","epoch 32 | loss: 0.07029 | val_0_auc: 0.99439 |  0:04:55s\n","epoch 33 | loss: 0.07011 | val_0_auc: 0.99444 |  0:05:04s\n","epoch 34 | loss: 0.06978 | val_0_auc: 0.9943  |  0:05:13s\n","epoch 35 | loss: 0.06926 | val_0_auc: 0.99452 |  0:05:22s\n","epoch 36 | loss: 0.07005 | val_0_auc: 0.99458 |  0:05:31s\n","epoch 37 | loss: 0.0693  | val_0_auc: 0.99441 |  0:05:40s\n","epoch 38 | loss: 0.07011 | val_0_auc: 0.99456 |  0:05:49s\n","epoch 39 | loss: 0.06897 | val_0_auc: 0.99478 |  0:05:58s\n","epoch 40 | loss: 0.06836 | val_0_auc: 0.9947  |  0:06:07s\n","epoch 41 | loss: 0.06892 | val_0_auc: 0.99446 |  0:06:16s\n","epoch 42 | loss: 0.06804 | val_0_auc: 0.99452 |  0:06:25s\n","epoch 43 | loss: 0.06888 | val_0_auc: 0.99445 |  0:06:34s\n","epoch 44 | loss: 0.06888 | val_0_auc: 0.99324 |  0:06:43s\n","epoch 45 | loss: 0.0681  | val_0_auc: 0.99454 |  0:06:52s\n","epoch 46 | loss: 0.06733 | val_0_auc: 0.99418 |  0:07:01s\n","epoch 47 | loss: 0.0677  | val_0_auc: 0.99473 |  0:07:10s\n","epoch 48 | loss: 0.06988 | val_0_auc: 0.99452 |  0:07:19s\n","epoch 49 | loss: 0.06972 | val_0_auc: 0.99413 |  0:07:27s\n","epoch 50 | loss: 0.0689  | val_0_auc: 0.99458 |  0:07:37s\n","epoch 51 | loss: 0.06775 | val_0_auc: 0.99468 |  0:07:45s\n","epoch 52 | loss: 0.06691 | val_0_auc: 0.99474 |  0:07:54s\n","epoch 53 | loss: 0.06707 | val_0_auc: 0.99454 |  0:08:03s\n","epoch 54 | loss: 0.06611 | val_0_auc: 0.99466 |  0:08:12s\n","epoch 55 | loss: 0.06638 | val_0_auc: 0.99465 |  0:08:21s\n","epoch 56 | loss: 0.06592 | val_0_auc: 0.99469 |  0:08:30s\n","epoch 57 | loss: 0.06579 | val_0_auc: 0.99457 |  0:08:39s\n","epoch 58 | loss: 0.06597 | val_0_auc: 0.9946  |  0:08:48s\n","epoch 59 | loss: 0.06568 | val_0_auc: 0.99451 |  0:08:57s\n","epoch 60 | loss: 0.06533 | val_0_auc: 0.99464 |  0:09:06s\n","epoch 61 | loss: 0.06499 | val_0_auc: 0.99451 |  0:09:15s\n","epoch 62 | loss: 0.06515 | val_0_auc: 0.99421 |  0:09:24s\n","epoch 63 | loss: 0.06506 | val_0_auc: 0.994   |  0:09:33s\n","epoch 64 | loss: 0.06515 | val_0_auc: 0.99431 |  0:09:42s\n","epoch 65 | loss: 0.06459 | val_0_auc: 0.99453 |  0:09:51s\n","epoch 66 | loss: 0.06466 | val_0_auc: 0.99455 |  0:10:00s\n","epoch 67 | loss: 0.06433 | val_0_auc: 0.99442 |  0:10:09s\n","epoch 68 | loss: 0.06433 | val_0_auc: 0.99452 |  0:10:18s\n","epoch 69 | loss: 0.06441 | val_0_auc: 0.99469 |  0:10:27s\n","\n","Early stopping occurred at epoch 69 with best_epoch = 39 and best_val_0_auc = 0.99478\n"," 80%|████████  | 12/15 [3:23:24<48:11, 963.68s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 87%|████████▋ | 13/15 [3:23:38<28:52, 866.13s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.35493 | val_0_auc: 0.96531 |  0:00:08s\n","epoch 1  | loss: 0.13607 | val_0_auc: 0.98834 |  0:00:17s\n","epoch 2  | loss: 0.10964 | val_0_auc: 0.99078 |  0:00:26s\n","epoch 3  | loss: 0.09664 | val_0_auc: 0.99117 |  0:00:35s\n","epoch 4  | loss: 0.09351 | val_0_auc: 0.99    |  0:00:44s\n","epoch 5  | loss: 0.09061 | val_0_auc: 0.99239 |  0:00:53s\n","epoch 6  | loss: 0.08641 | val_0_auc: 0.99228 |  0:01:02s\n","epoch 7  | loss: 0.08536 | val_0_auc: 0.99321 |  0:01:11s\n","epoch 8  | loss: 0.08138 | val_0_auc: 0.99323 |  0:01:20s\n","epoch 9  | loss: 0.07947 | val_0_auc: 0.99351 |  0:01:29s\n","epoch 10 | loss: 0.0797  | val_0_auc: 0.99346 |  0:01:38s\n","epoch 11 | loss: 0.07874 | val_0_auc: 0.99382 |  0:01:47s\n","epoch 12 | loss: 0.07767 | val_0_auc: 0.99353 |  0:01:56s\n","epoch 13 | loss: 0.07651 | val_0_auc: 0.99422 |  0:02:05s\n","epoch 14 | loss: 0.07505 | val_0_auc: 0.99396 |  0:02:14s\n","epoch 15 | loss: 0.07441 | val_0_auc: 0.99389 |  0:02:23s\n","epoch 16 | loss: 0.07527 | val_0_auc: 0.99394 |  0:02:32s\n","epoch 17 | loss: 0.07351 | val_0_auc: 0.99353 |  0:02:41s\n","epoch 18 | loss: 0.0725  | val_0_auc: 0.99375 |  0:02:50s\n","epoch 19 | loss: 0.07272 | val_0_auc: 0.99395 |  0:02:59s\n","epoch 20 | loss: 0.07198 | val_0_auc: 0.99404 |  0:03:08s\n","epoch 21 | loss: 0.0733  | val_0_auc: 0.99401 |  0:03:17s\n","epoch 22 | loss: 0.07276 | val_0_auc: 0.99424 |  0:03:26s\n","epoch 23 | loss: 0.07129 | val_0_auc: 0.99407 |  0:03:35s\n","epoch 24 | loss: 0.07169 | val_0_auc: 0.994   |  0:03:44s\n","epoch 25 | loss: 0.0718  | val_0_auc: 0.99361 |  0:03:54s\n","epoch 26 | loss: 0.07145 | val_0_auc: 0.99429 |  0:04:03s\n","epoch 27 | loss: 0.07036 | val_0_auc: 0.99431 |  0:04:12s\n","epoch 28 | loss: 0.07057 | val_0_auc: 0.9944  |  0:04:22s\n","epoch 29 | loss: 0.07036 | val_0_auc: 0.99456 |  0:04:31s\n","epoch 30 | loss: 0.06957 | val_0_auc: 0.99425 |  0:04:40s\n","epoch 31 | loss: 0.06968 | val_0_auc: 0.99377 |  0:04:49s\n","epoch 32 | loss: 0.07023 | val_0_auc: 0.99451 |  0:04:58s\n","epoch 33 | loss: 0.06946 | val_0_auc: 0.99461 |  0:05:07s\n","epoch 34 | loss: 0.06921 | val_0_auc: 0.99466 |  0:05:16s\n","epoch 35 | loss: 0.06837 | val_0_auc: 0.99459 |  0:05:25s\n","epoch 36 | loss: 0.06883 | val_0_auc: 0.99476 |  0:05:34s\n","epoch 37 | loss: 0.06883 | val_0_auc: 0.99463 |  0:05:43s\n","epoch 38 | loss: 0.06833 | val_0_auc: 0.99484 |  0:05:52s\n","epoch 39 | loss: 0.06828 | val_0_auc: 0.99426 |  0:06:01s\n","epoch 40 | loss: 0.06863 | val_0_auc: 0.99458 |  0:06:10s\n","epoch 41 | loss: 0.06769 | val_0_auc: 0.9947  |  0:06:19s\n","epoch 42 | loss: 0.06759 | val_0_auc: 0.99462 |  0:06:28s\n","epoch 43 | loss: 0.06775 | val_0_auc: 0.99454 |  0:06:37s\n","epoch 44 | loss: 0.06852 | val_0_auc: 0.99473 |  0:06:46s\n","epoch 45 | loss: 0.06739 | val_0_auc: 0.99483 |  0:06:55s\n","epoch 46 | loss: 0.06639 | val_0_auc: 0.99458 |  0:07:04s\n","epoch 47 | loss: 0.06647 | val_0_auc: 0.99461 |  0:07:13s\n","epoch 48 | loss: 0.06613 | val_0_auc: 0.99469 |  0:07:22s\n","epoch 49 | loss: 0.0662  | val_0_auc: 0.99457 |  0:07:31s\n","epoch 50 | loss: 0.06641 | val_0_auc: 0.99471 |  0:07:40s\n","epoch 51 | loss: 0.06662 | val_0_auc: 0.99477 |  0:07:49s\n","epoch 52 | loss: 0.066   | val_0_auc: 0.99468 |  0:07:58s\n","epoch 53 | loss: 0.066   | val_0_auc: 0.99457 |  0:08:07s\n","epoch 54 | loss: 0.06599 | val_0_auc: 0.99454 |  0:08:15s\n","epoch 55 | loss: 0.06589 | val_0_auc: 0.99441 |  0:08:24s\n","epoch 56 | loss: 0.06508 | val_0_auc: 0.99477 |  0:08:33s\n","epoch 57 | loss: 0.06581 | val_0_auc: 0.99483 |  0:08:42s\n","epoch 58 | loss: 0.06483 | val_0_auc: 0.99484 |  0:08:51s\n","epoch 59 | loss: 0.06476 | val_0_auc: 0.99455 |  0:09:00s\n","epoch 60 | loss: 0.06501 | val_0_auc: 0.99476 |  0:09:09s\n","epoch 61 | loss: 0.06549 | val_0_auc: 0.99485 |  0:09:18s\n","epoch 62 | loss: 0.06483 | val_0_auc: 0.99458 |  0:09:27s\n","epoch 63 | loss: 0.06527 | val_0_auc: 0.99469 |  0:09:36s\n","epoch 64 | loss: 0.06483 | val_0_auc: 0.99468 |  0:09:45s\n","epoch 65 | loss: 0.0642  | val_0_auc: 0.99439 |  0:09:54s\n","epoch 66 | loss: 0.06426 | val_0_auc: 0.99474 |  0:10:03s\n","epoch 67 | loss: 0.06429 | val_0_auc: 0.99436 |  0:10:12s\n","epoch 68 | loss: 0.0645  | val_0_auc: 0.99481 |  0:10:21s\n","epoch 69 | loss: 0.06406 | val_0_auc: 0.99476 |  0:10:30s\n","epoch 70 | loss: 0.06371 | val_0_auc: 0.99456 |  0:10:39s\n","epoch 71 | loss: 0.06383 | val_0_auc: 0.99447 |  0:10:48s\n","epoch 72 | loss: 0.06442 | val_0_auc: 0.99473 |  0:10:57s\n","epoch 73 | loss: 0.06409 | val_0_auc: 0.99444 |  0:11:06s\n","epoch 74 | loss: 0.0633  | val_0_auc: 0.99445 |  0:11:15s\n","epoch 75 | loss: 0.06335 | val_0_auc: 0.99461 |  0:11:24s\n","epoch 76 | loss: 0.06415 | val_0_auc: 0.99458 |  0:11:33s\n","epoch 77 | loss: 0.06354 | val_0_auc: 0.99454 |  0:11:42s\n","epoch 78 | loss: 0.06325 | val_0_auc: 0.99456 |  0:11:51s\n","epoch 79 | loss: 0.0631  | val_0_auc: 0.99477 |  0:11:59s\n","epoch 80 | loss: 0.06323 | val_0_auc: 0.99449 |  0:12:08s\n","epoch 81 | loss: 0.0623  | val_0_auc: 0.99454 |  0:12:17s\n","epoch 82 | loss: 0.0625  | val_0_auc: 0.99447 |  0:12:26s\n","epoch 83 | loss: 0.06238 | val_0_auc: 0.99454 |  0:12:35s\n","epoch 84 | loss: 0.0627  | val_0_auc: 0.99489 |  0:12:44s\n","epoch 85 | loss: 0.06223 | val_0_auc: 0.99476 |  0:12:53s\n","epoch 86 | loss: 0.06225 | val_0_auc: 0.99434 |  0:13:02s\n","epoch 87 | loss: 0.06213 | val_0_auc: 0.99462 |  0:13:11s\n","epoch 88 | loss: 0.06204 | val_0_auc: 0.99456 |  0:13:20s\n","epoch 89 | loss: 0.06216 | val_0_auc: 0.99462 |  0:13:29s\n","epoch 90 | loss: 0.0621  | val_0_auc: 0.99454 |  0:13:38s\n","epoch 91 | loss: 0.06235 | val_0_auc: 0.99482 |  0:13:47s\n","epoch 92 | loss: 0.06202 | val_0_auc: 0.99421 |  0:13:56s\n","epoch 93 | loss: 0.06152 | val_0_auc: 0.99488 |  0:14:05s\n","epoch 94 | loss: 0.06153 | val_0_auc: 0.99462 |  0:14:14s\n","epoch 95 | loss: 0.06203 | val_0_auc: 0.99424 |  0:14:23s\n","epoch 96 | loss: 0.06109 | val_0_auc: 0.99453 |  0:14:32s\n","epoch 97 | loss: 0.06135 | val_0_auc: 0.99448 |  0:14:41s\n","epoch 98 | loss: 0.06146 | val_0_auc: 0.9948  |  0:14:50s\n","epoch 99 | loss: 0.06149 | val_0_auc: 0.99467 |  0:14:59s\n","epoch 100| loss: 0.06105 | val_0_auc: 0.99418 |  0:15:09s\n","epoch 101| loss: 0.061   | val_0_auc: 0.99476 |  0:15:18s\n","epoch 102| loss: 0.06223 | val_0_auc: 0.99428 |  0:15:27s\n","epoch 103| loss: 0.06103 | val_0_auc: 0.99411 |  0:15:36s\n","epoch 104| loss: 0.06064 | val_0_auc: 0.99483 |  0:15:45s\n","epoch 105| loss: 0.06041 | val_0_auc: 0.9947  |  0:15:54s\n","epoch 106| loss: 0.06049 | val_0_auc: 0.99484 |  0:16:03s\n","epoch 107| loss: 0.06022 | val_0_auc: 0.99466 |  0:16:12s\n","epoch 108| loss: 0.0602  | val_0_auc: 0.99452 |  0:16:21s\n","epoch 109| loss: 0.06016 | val_0_auc: 0.99451 |  0:16:30s\n","epoch 110| loss: 0.0602  | val_0_auc: 0.99436 |  0:16:39s\n","epoch 111| loss: 0.06089 | val_0_auc: 0.99427 |  0:16:48s\n","epoch 112| loss: 0.06032 | val_0_auc: 0.99453 |  0:16:57s\n","epoch 113| loss: 0.05997 | val_0_auc: 0.9942  |  0:17:06s\n","epoch 114| loss: 0.0598  | val_0_auc: 0.99442 |  0:17:15s\n","\n","Early stopping occurred at epoch 114 with best_epoch = 84 and best_val_0_auc = 0.99489\n"," 87%|████████▋ | 13/15 [3:40:54<28:52, 866.13s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r 93%|█████████▎| 14/15 [3:41:08<15:21, 921.51s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n","\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.28513 | val_0_auc: 0.9488  |  0:00:08s\n","epoch 1  | loss: 0.12584 | val_0_auc: 0.99034 |  0:00:17s\n","epoch 2  | loss: 0.11087 | val_0_auc: 0.99102 |  0:00:26s\n","epoch 3  | loss: 0.10149 | val_0_auc: 0.99123 |  0:00:35s\n","epoch 4  | loss: 0.09161 | val_0_auc: 0.99235 |  0:00:44s\n","epoch 5  | loss: 0.08726 | val_0_auc: 0.99299 |  0:00:53s\n","epoch 6  | loss: 0.08476 | val_0_auc: 0.99333 |  0:01:02s\n","epoch 7  | loss: 0.08353 | val_0_auc: 0.99363 |  0:01:11s\n","epoch 8  | loss: 0.08027 | val_0_auc: 0.99366 |  0:01:20s\n","epoch 9  | loss: 0.07887 | val_0_auc: 0.99372 |  0:01:29s\n","epoch 10 | loss: 0.07806 | val_0_auc: 0.9938  |  0:01:38s\n","epoch 11 | loss: 0.07708 | val_0_auc: 0.99397 |  0:01:47s\n","epoch 12 | loss: 0.07748 | val_0_auc: 0.9936  |  0:01:57s\n","epoch 13 | loss: 0.07538 | val_0_auc: 0.99407 |  0:02:05s\n","epoch 14 | loss: 0.07628 | val_0_auc: 0.99359 |  0:02:14s\n","epoch 15 | loss: 0.0759  | val_0_auc: 0.99397 |  0:02:23s\n","epoch 16 | loss: 0.07548 | val_0_auc: 0.99411 |  0:02:32s\n","epoch 17 | loss: 0.07523 | val_0_auc: 0.99374 |  0:02:41s\n","epoch 18 | loss: 0.0761  | val_0_auc: 0.99403 |  0:02:50s\n","epoch 19 | loss: 0.07577 | val_0_auc: 0.99399 |  0:02:59s\n","epoch 20 | loss: 0.07428 | val_0_auc: 0.99389 |  0:03:08s\n","epoch 21 | loss: 0.07442 | val_0_auc: 0.99401 |  0:03:17s\n","epoch 22 | loss: 0.07363 | val_0_auc: 0.99365 |  0:03:26s\n","epoch 23 | loss: 0.0725  | val_0_auc: 0.99388 |  0:03:35s\n","epoch 24 | loss: 0.07321 | val_0_auc: 0.99386 |  0:03:44s\n","epoch 25 | loss: 0.07227 | val_0_auc: 0.9942  |  0:03:53s\n","epoch 26 | loss: 0.07226 | val_0_auc: 0.99393 |  0:04:02s\n","epoch 27 | loss: 0.07159 | val_0_auc: 0.99429 |  0:04:11s\n","epoch 28 | loss: 0.07236 | val_0_auc: 0.99416 |  0:04:20s\n","epoch 29 | loss: 0.07119 | val_0_auc: 0.99437 |  0:04:29s\n","epoch 30 | loss: 0.07077 | val_0_auc: 0.99424 |  0:04:38s\n","epoch 31 | loss: 0.07081 | val_0_auc: 0.99418 |  0:04:47s\n","epoch 32 | loss: 0.07029 | val_0_auc: 0.99439 |  0:04:56s\n","epoch 33 | loss: 0.07011 | val_0_auc: 0.99444 |  0:05:05s\n","epoch 34 | loss: 0.06978 | val_0_auc: 0.9943  |  0:05:14s\n","epoch 35 | loss: 0.06926 | val_0_auc: 0.99452 |  0:05:23s\n","epoch 36 | loss: 0.07005 | val_0_auc: 0.99458 |  0:05:32s\n","epoch 37 | loss: 0.0693  | val_0_auc: 0.99441 |  0:05:41s\n","epoch 38 | loss: 0.07011 | val_0_auc: 0.99456 |  0:05:50s\n","epoch 39 | loss: 0.06897 | val_0_auc: 0.99478 |  0:05:59s\n","epoch 40 | loss: 0.06836 | val_0_auc: 0.9947  |  0:06:08s\n","epoch 41 | loss: 0.06892 | val_0_auc: 0.99446 |  0:06:17s\n","epoch 42 | loss: 0.06804 | val_0_auc: 0.99452 |  0:06:26s\n","epoch 43 | loss: 0.06888 | val_0_auc: 0.99445 |  0:06:35s\n","epoch 44 | loss: 0.06888 | val_0_auc: 0.99324 |  0:06:44s\n","epoch 45 | loss: 0.0681  | val_0_auc: 0.99454 |  0:06:53s\n","epoch 46 | loss: 0.06733 | val_0_auc: 0.99418 |  0:07:02s\n","epoch 47 | loss: 0.0677  | val_0_auc: 0.99473 |  0:07:11s\n","epoch 48 | loss: 0.06988 | val_0_auc: 0.99452 |  0:07:20s\n","epoch 49 | loss: 0.06972 | val_0_auc: 0.99413 |  0:07:29s\n","epoch 50 | loss: 0.0689  | val_0_auc: 0.99458 |  0:07:38s\n","epoch 51 | loss: 0.06775 | val_0_auc: 0.99468 |  0:07:47s\n","epoch 52 | loss: 0.06691 | val_0_auc: 0.99474 |  0:07:55s\n","epoch 53 | loss: 0.06707 | val_0_auc: 0.99454 |  0:08:04s\n","epoch 54 | loss: 0.06611 | val_0_auc: 0.99466 |  0:08:13s\n","epoch 55 | loss: 0.06638 | val_0_auc: 0.99465 |  0:08:22s\n","epoch 56 | loss: 0.06592 | val_0_auc: 0.99469 |  0:08:31s\n","epoch 57 | loss: 0.06579 | val_0_auc: 0.99457 |  0:08:41s\n","epoch 58 | loss: 0.06597 | val_0_auc: 0.9946  |  0:08:50s\n","epoch 59 | loss: 0.06568 | val_0_auc: 0.99451 |  0:08:59s\n","epoch 60 | loss: 0.06533 | val_0_auc: 0.99464 |  0:09:08s\n","epoch 61 | loss: 0.06499 | val_0_auc: 0.99451 |  0:09:16s\n","epoch 62 | loss: 0.06515 | val_0_auc: 0.99421 |  0:09:26s\n","epoch 63 | loss: 0.06506 | val_0_auc: 0.994   |  0:09:35s\n","epoch 64 | loss: 0.06515 | val_0_auc: 0.99431 |  0:09:44s\n","epoch 65 | loss: 0.06459 | val_0_auc: 0.99453 |  0:09:53s\n","epoch 66 | loss: 0.06466 | val_0_auc: 0.99455 |  0:10:01s\n","epoch 67 | loss: 0.06433 | val_0_auc: 0.99442 |  0:10:11s\n","epoch 68 | loss: 0.06433 | val_0_auc: 0.99452 |  0:10:19s\n","epoch 69 | loss: 0.06441 | val_0_auc: 0.99469 |  0:10:28s\n","\n","Early stopping occurred at epoch 69 with best_epoch = 39 and best_val_0_auc = 0.99478\n"," 93%|█████████▎| 14/15 [3:51:37<15:21, 921.51s/trial, best loss: -0.9785816646655564]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","\n"]},{"output_type":"stream","name":"stdout","text":["100%|██████████| 15/15 [3:51:51<00:00, 927.43s/trial, best loss: -0.9785816646655564]\n","Best:  {'n_a': 0, 'n_d': 3}\n"]}]},{"cell_type":"code","source":["print(best)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o26ZffLT0Smu","executionInfo":{"status":"ok","timestamp":1691496756051,"user_tz":-540,"elapsed":5,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}},"outputId":"6e13425d-9845-4ee5-f8ef-bacf984c4885"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["{'n_a': 0, 'n_d': 3}\n"]}]},{"cell_type":"code","source":["from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n","from pytorch_tabnet.tab_model import TabNetClassifier\n","from sklearn.metrics import roc_auc_score\n","import numpy as np\n","\n","# Convert indices to parameters\n","best_params = {\n","    'n_d': [8, 16, 24, 32][best['n_d']],\n","    'n_a': [8, 16, 24, 32][best['n_a']],\n","}\n","\n","# Train model with best parameters\n","clf = TabNetClassifier(\n","    n_d=best_params['n_d'],\n","    n_a=best_params['n_a'],\n",")\n","\n","clf.fit(\n","    X_train, y_train,\n","    eval_set=[(X_val, y_val)],\n","    patience=30,\n","    max_epochs=200,\n",")\n","\n","# Evaluate model\n","preds = clf.predict_proba(X_val)[:, 1]\n","auc = roc_auc_score(y_val, preds)\n","print(\"Best model ROC AUC: \", auc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhtsbely0Uuc","executionInfo":{"status":"ok","timestamp":1691497843300,"user_tz":-540,"elapsed":1087252,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}},"outputId":"a8bff814-cbc8-4c7b-aa34-687bfd0155c6"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.31276 | val_0_auc: 0.95096 |  0:00:09s\n","epoch 1  | loss: 0.13008 | val_0_auc: 0.98882 |  0:00:17s\n","epoch 2  | loss: 0.10735 | val_0_auc: 0.99048 |  0:00:26s\n","epoch 3  | loss: 0.09925 | val_0_auc: 0.9917  |  0:00:35s\n","epoch 4  | loss: 0.09521 | val_0_auc: 0.99171 |  0:00:44s\n","epoch 5  | loss: 0.09468 | val_0_auc: 0.99188 |  0:00:53s\n","epoch 6  | loss: 0.0884  | val_0_auc: 0.99249 |  0:01:02s\n","epoch 7  | loss: 0.08937 | val_0_auc: 0.9924  |  0:01:11s\n","epoch 8  | loss: 0.08548 | val_0_auc: 0.99286 |  0:01:20s\n","epoch 9  | loss: 0.08269 | val_0_auc: 0.99294 |  0:01:29s\n","epoch 10 | loss: 0.08141 | val_0_auc: 0.99316 |  0:01:38s\n","epoch 11 | loss: 0.07938 | val_0_auc: 0.9933  |  0:01:47s\n","epoch 12 | loss: 0.084   | val_0_auc: 0.99309 |  0:01:56s\n","epoch 13 | loss: 0.08559 | val_0_auc: 0.99201 |  0:02:05s\n","epoch 14 | loss: 0.08966 | val_0_auc: 0.99236 |  0:02:14s\n","epoch 15 | loss: 0.08235 | val_0_auc: 0.99333 |  0:02:23s\n","epoch 16 | loss: 0.07936 | val_0_auc: 0.99339 |  0:02:32s\n","epoch 17 | loss: 0.07811 | val_0_auc: 0.99361 |  0:02:41s\n","epoch 18 | loss: 0.07785 | val_0_auc: 0.99358 |  0:02:50s\n","epoch 19 | loss: 0.07819 | val_0_auc: 0.99354 |  0:02:59s\n","epoch 20 | loss: 0.07932 | val_0_auc: 0.99181 |  0:03:08s\n","epoch 21 | loss: 0.07998 | val_0_auc: 0.99128 |  0:03:17s\n","epoch 22 | loss: 0.08032 | val_0_auc: 0.99337 |  0:03:26s\n","epoch 23 | loss: 0.07786 | val_0_auc: 0.99373 |  0:03:35s\n","epoch 24 | loss: 0.07823 | val_0_auc: 0.99363 |  0:03:43s\n","epoch 25 | loss: 0.07539 | val_0_auc: 0.99369 |  0:03:53s\n","epoch 26 | loss: 0.07517 | val_0_auc: 0.99361 |  0:04:01s\n","epoch 27 | loss: 0.0739  | val_0_auc: 0.99392 |  0:04:10s\n","epoch 28 | loss: 0.07231 | val_0_auc: 0.99421 |  0:04:19s\n","epoch 29 | loss: 0.07557 | val_0_auc: 0.9941  |  0:04:28s\n","epoch 30 | loss: 0.07351 | val_0_auc: 0.99384 |  0:04:37s\n","epoch 31 | loss: 0.07369 | val_0_auc: 0.99356 |  0:04:46s\n","epoch 32 | loss: 0.07533 | val_0_auc: 0.99352 |  0:04:55s\n","epoch 33 | loss: 0.07476 | val_0_auc: 0.99395 |  0:05:04s\n","epoch 34 | loss: 0.07282 | val_0_auc: 0.99397 |  0:05:13s\n","epoch 35 | loss: 0.07201 | val_0_auc: 0.99417 |  0:05:21s\n","epoch 36 | loss: 0.07206 | val_0_auc: 0.99362 |  0:05:30s\n","epoch 37 | loss: 0.07151 | val_0_auc: 0.99392 |  0:05:39s\n","epoch 38 | loss: 0.07112 | val_0_auc: 0.99396 |  0:05:48s\n","epoch 39 | loss: 0.07192 | val_0_auc: 0.99392 |  0:05:57s\n","epoch 40 | loss: 0.07496 | val_0_auc: 0.99373 |  0:06:06s\n","epoch 41 | loss: 0.07357 | val_0_auc: 0.9941  |  0:06:15s\n","epoch 42 | loss: 0.07362 | val_0_auc: 0.99431 |  0:06:24s\n","epoch 43 | loss: 0.07127 | val_0_auc: 0.99444 |  0:06:33s\n","epoch 44 | loss: 0.06969 | val_0_auc: 0.99451 |  0:06:42s\n","epoch 45 | loss: 0.06925 | val_0_auc: 0.99452 |  0:06:51s\n","epoch 46 | loss: 0.0695  | val_0_auc: 0.99423 |  0:07:00s\n","epoch 47 | loss: 0.06931 | val_0_auc: 0.99448 |  0:07:09s\n","epoch 48 | loss: 0.07004 | val_0_auc: 0.99426 |  0:07:18s\n","epoch 49 | loss: 0.06945 | val_0_auc: 0.9943  |  0:07:27s\n","epoch 50 | loss: 0.06929 | val_0_auc: 0.99379 |  0:07:36s\n","epoch 51 | loss: 0.06952 | val_0_auc: 0.99415 |  0:07:45s\n","epoch 52 | loss: 0.06872 | val_0_auc: 0.99393 |  0:07:54s\n","epoch 53 | loss: 0.06846 | val_0_auc: 0.9939  |  0:08:03s\n","epoch 54 | loss: 0.06855 | val_0_auc: 0.9941  |  0:08:12s\n","epoch 55 | loss: 0.06813 | val_0_auc: 0.99469 |  0:08:21s\n","epoch 56 | loss: 0.06831 | val_0_auc: 0.9945  |  0:08:30s\n","epoch 57 | loss: 0.07102 | val_0_auc: 0.99436 |  0:08:39s\n","epoch 58 | loss: 0.06956 | val_0_auc: 0.9938  |  0:08:48s\n","epoch 59 | loss: 0.06913 | val_0_auc: 0.99399 |  0:08:57s\n","epoch 60 | loss: 0.06815 | val_0_auc: 0.99365 |  0:09:05s\n","epoch 61 | loss: 0.06841 | val_0_auc: 0.9944  |  0:09:14s\n","epoch 62 | loss: 0.06868 | val_0_auc: 0.99433 |  0:09:23s\n","epoch 63 | loss: 0.06735 | val_0_auc: 0.99449 |  0:09:32s\n","epoch 64 | loss: 0.06748 | val_0_auc: 0.99408 |  0:09:41s\n","epoch 65 | loss: 0.06755 | val_0_auc: 0.99458 |  0:09:50s\n","epoch 66 | loss: 0.06638 | val_0_auc: 0.99452 |  0:09:59s\n","epoch 67 | loss: 0.06746 | val_0_auc: 0.99433 |  0:10:08s\n","epoch 68 | loss: 0.06682 | val_0_auc: 0.99457 |  0:10:17s\n","epoch 69 | loss: 0.0665  | val_0_auc: 0.99451 |  0:10:26s\n","epoch 70 | loss: 0.0667  | val_0_auc: 0.99458 |  0:10:35s\n","epoch 71 | loss: 0.06585 | val_0_auc: 0.99446 |  0:10:44s\n","epoch 72 | loss: 0.06622 | val_0_auc: 0.99449 |  0:10:53s\n","epoch 73 | loss: 0.0661  | val_0_auc: 0.9946  |  0:11:02s\n","epoch 74 | loss: 0.06669 | val_0_auc: 0.99406 |  0:11:11s\n","epoch 75 | loss: 0.06751 | val_0_auc: 0.9946  |  0:11:20s\n","epoch 76 | loss: 0.06644 | val_0_auc: 0.99451 |  0:11:29s\n","epoch 77 | loss: 0.06679 | val_0_auc: 0.99455 |  0:11:38s\n","epoch 78 | loss: 0.06611 | val_0_auc: 0.99451 |  0:11:47s\n","epoch 79 | loss: 0.06602 | val_0_auc: 0.99468 |  0:11:56s\n","epoch 80 | loss: 0.06558 | val_0_auc: 0.99449 |  0:12:05s\n","epoch 81 | loss: 0.0663  | val_0_auc: 0.99456 |  0:12:14s\n","epoch 82 | loss: 0.06644 | val_0_auc: 0.9944  |  0:12:22s\n","epoch 83 | loss: 0.06601 | val_0_auc: 0.99458 |  0:12:32s\n","epoch 84 | loss: 0.0652  | val_0_auc: 0.99449 |  0:12:40s\n","epoch 85 | loss: 0.06543 | val_0_auc: 0.99473 |  0:12:49s\n","epoch 86 | loss: 0.06556 | val_0_auc: 0.99468 |  0:12:58s\n","epoch 87 | loss: 0.06498 | val_0_auc: 0.99484 |  0:13:07s\n","epoch 88 | loss: 0.06532 | val_0_auc: 0.99437 |  0:13:16s\n","epoch 89 | loss: 0.06506 | val_0_auc: 0.99493 |  0:13:25s\n","epoch 90 | loss: 0.06458 | val_0_auc: 0.9947  |  0:13:34s\n","epoch 91 | loss: 0.06554 | val_0_auc: 0.99436 |  0:13:43s\n","epoch 92 | loss: 0.06558 | val_0_auc: 0.99453 |  0:13:52s\n","epoch 93 | loss: 0.06465 | val_0_auc: 0.99427 |  0:14:01s\n","epoch 94 | loss: 0.06584 | val_0_auc: 0.99345 |  0:14:09s\n","epoch 95 | loss: 0.06598 | val_0_auc: 0.99455 |  0:14:18s\n","epoch 96 | loss: 0.06478 | val_0_auc: 0.99477 |  0:14:28s\n","epoch 97 | loss: 0.06449 | val_0_auc: 0.99471 |  0:14:37s\n","epoch 98 | loss: 0.06393 | val_0_auc: 0.9946  |  0:14:46s\n","epoch 99 | loss: 0.06455 | val_0_auc: 0.99476 |  0:14:55s\n","epoch 100| loss: 0.0638  | val_0_auc: 0.99473 |  0:15:04s\n","epoch 101| loss: 0.06378 | val_0_auc: 0.99477 |  0:15:13s\n","epoch 102| loss: 0.06405 | val_0_auc: 0.99439 |  0:15:22s\n","epoch 103| loss: 0.06365 | val_0_auc: 0.99453 |  0:15:31s\n","epoch 104| loss: 0.06357 | val_0_auc: 0.99464 |  0:15:39s\n","epoch 105| loss: 0.06319 | val_0_auc: 0.99451 |  0:15:48s\n","epoch 106| loss: 0.06275 | val_0_auc: 0.99447 |  0:15:57s\n","epoch 107| loss: 0.06297 | val_0_auc: 0.99375 |  0:16:06s\n","epoch 108| loss: 0.06251 | val_0_auc: 0.99449 |  0:16:15s\n","epoch 109| loss: 0.06278 | val_0_auc: 0.9944  |  0:16:24s\n","epoch 110| loss: 0.06412 | val_0_auc: 0.99423 |  0:16:33s\n","epoch 111| loss: 0.06941 | val_0_auc: 0.99456 |  0:16:42s\n","epoch 112| loss: 0.06465 | val_0_auc: 0.99407 |  0:16:51s\n","epoch 113| loss: 0.06654 | val_0_auc: 0.99426 |  0:16:59s\n","epoch 114| loss: 0.06466 | val_0_auc: 0.99445 |  0:17:08s\n","epoch 115| loss: 0.06348 | val_0_auc: 0.99432 |  0:17:17s\n","epoch 116| loss: 0.06341 | val_0_auc: 0.99481 |  0:17:25s\n","epoch 117| loss: 0.06254 | val_0_auc: 0.99458 |  0:17:34s\n","epoch 118| loss: 0.06278 | val_0_auc: 0.99457 |  0:17:43s\n","epoch 119| loss: 0.06218 | val_0_auc: 0.99456 |  0:17:52s\n","\n","Early stopping occurred at epoch 119 with best_epoch = 89 and best_val_0_auc = 0.99493\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"output_type":"stream","name":"stdout","text":["Best model ROC AUC:  0.9949315651900039\n"]}]},{"cell_type":"code","source":["test_df.iloc[:, 3:-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"HU1Gp31-vqYD","executionInfo":{"status":"ok","timestamp":1691497843301,"user_tz":-540,"elapsed":22,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}},"outputId":"75fcb62c-f8db-4a97-d141-26dd503c1c5e"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       총자산경상이익률  총자산순이익률  금융비용/매출액  금융비용/총부채  순금융비용/매출액  매출액경상이익률  매출액순이익률  \\\n","0          0.84     1.60   -0.0254 -0.122986  -0.025390      2.39     4.56   \n","1          1.45    -0.19   -0.0237 -0.091076  -0.023699      3.63    -0.47   \n","2         29.25    23.23    0.0006  0.001761   0.000592     32.86    26.09   \n","3         41.86    30.43   -0.0068 -0.024544  -0.006799     45.89    33.36   \n","4          0.03     0.60    0.0044  0.000687   0.006050      0.23     4.27   \n","...         ...      ...       ...       ...        ...       ...      ...   \n","38238     13.79    11.92   -0.0053 -0.017173  -0.005275     11.40     9.86   \n","38239     -1.90    -3.72    0.0716  0.026644   0.071566     -7.57   -14.79   \n","38240      5.27     3.85    0.0466  0.022394   0.046637     16.38    11.96   \n","38241      5.04     4.29   -0.0010 -0.003636  -0.001040      3.09     2.63   \n","38242     11.34    10.18   -0.0012 -0.003901  -0.001193      7.08     6.35   \n","\n","       자기자본경상이익률  자기자본순이익률   EBITDA/이자비용  ...  이익잉여금/총자산_변화량  이익잉여금/총부채_변화량  \\\n","0           0.91      1.73  7.907038e+07  ...       0.015472       0.184095   \n","1           1.61     -0.21  1.987598e+08  ...      -0.000921      -0.191910   \n","2          42.00     33.35  1.216048e+08  ...       0.146424       0.741189   \n","3          56.02     40.73  2.322498e+08  ...       0.133972       0.932836   \n","4           0.31     11.62  3.557666e+05  ...       0.020923       0.024074   \n","...          ...       ...           ...  ...            ...            ...   \n","38238      22.00     19.02  1.839779e+11  ...      -0.111046      -0.676866   \n","38239      -5.89    -11.50  1.190084e+00  ...      -0.063896       0.018403   \n","38240      15.96     11.65  5.976569e+00  ...       0.027306       0.026300   \n","38241       9.13      7.77  3.521770e+01  ...       0.038513       0.048395   \n","38242      22.35     20.06  6.250216e+01  ...       0.069819       0.105351   \n","\n","       이익잉여금/유동자산_변화량  현금비율_변화량  당좌비율_변화량  유동비율_변화량  유동부채회전율_변화량  \\\n","0            0.031436    -24.28   -141.42   -155.55     0.337766   \n","1           -0.003426    -14.32   -298.31   -297.30    -1.307682   \n","2           -0.123487     36.13     67.74     67.62     0.046018   \n","3            0.008106    -17.16     86.71     85.27     0.520850   \n","4            0.012722    488.70    757.69    811.07     5.032471   \n","...               ...       ...       ...       ...          ...   \n","38238    -7431.507309    -58.36    -42.92    -42.92    -0.883486   \n","38239       12.937910     12.01     11.49     11.33     0.334871   \n","38240        2.025173      2.73      3.97      4.01     0.354836   \n","38241        0.065115    -31.84    -21.77    -22.69    -0.319614   \n","38242        0.031779      2.66    -20.12    -21.71    -1.255076   \n","\n","            총자산_변화량       매출액_변화량      고정자산_변화량  \n","0      3.466000e+09  5.015000e+09  2.431900e+10  \n","1      8.952000e+09  1.272200e+10  4.497000e+09  \n","2      2.305240e+11  2.787620e+11  1.540300e+10  \n","3      4.720595e+11  4.606060e+11  6.020000e+10  \n","4      3.093865e+11  4.511500e+10 -3.254960e+11  \n","...             ...           ...           ...  \n","38238  5.942000e+09  6.175000e+09  9.220000e+08  \n","38239 -1.029500e+09  1.652000e+09 -1.153000e+09  \n","38240 -3.910000e+08  2.045000e+09 -5.620000e+08  \n","38241  5.591000e+09  5.698000e+09  3.170000e+08  \n","38242  1.188550e+10  1.742600e+10  1.719000e+09  \n","\n","[38243 rows x 56 columns]"],"text/html":["\n","\n","  <div id=\"df-98ceaea1-f112-4f8b-916d-9abb16ccd806\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>총자산경상이익률</th>\n","      <th>총자산순이익률</th>\n","      <th>금융비용/매출액</th>\n","      <th>금융비용/총부채</th>\n","      <th>순금융비용/매출액</th>\n","      <th>매출액경상이익률</th>\n","      <th>매출액순이익률</th>\n","      <th>자기자본경상이익률</th>\n","      <th>자기자본순이익률</th>\n","      <th>EBITDA/이자비용</th>\n","      <th>...</th>\n","      <th>이익잉여금/총자산_변화량</th>\n","      <th>이익잉여금/총부채_변화량</th>\n","      <th>이익잉여금/유동자산_변화량</th>\n","      <th>현금비율_변화량</th>\n","      <th>당좌비율_변화량</th>\n","      <th>유동비율_변화량</th>\n","      <th>유동부채회전율_변화량</th>\n","      <th>총자산_변화량</th>\n","      <th>매출액_변화량</th>\n","      <th>고정자산_변화량</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.84</td>\n","      <td>1.60</td>\n","      <td>-0.0254</td>\n","      <td>-0.122986</td>\n","      <td>-0.025390</td>\n","      <td>2.39</td>\n","      <td>4.56</td>\n","      <td>0.91</td>\n","      <td>1.73</td>\n","      <td>7.907038e+07</td>\n","      <td>...</td>\n","      <td>0.015472</td>\n","      <td>0.184095</td>\n","      <td>0.031436</td>\n","      <td>-24.28</td>\n","      <td>-141.42</td>\n","      <td>-155.55</td>\n","      <td>0.337766</td>\n","      <td>3.466000e+09</td>\n","      <td>5.015000e+09</td>\n","      <td>2.431900e+10</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.45</td>\n","      <td>-0.19</td>\n","      <td>-0.0237</td>\n","      <td>-0.091076</td>\n","      <td>-0.023699</td>\n","      <td>3.63</td>\n","      <td>-0.47</td>\n","      <td>1.61</td>\n","      <td>-0.21</td>\n","      <td>1.987598e+08</td>\n","      <td>...</td>\n","      <td>-0.000921</td>\n","      <td>-0.191910</td>\n","      <td>-0.003426</td>\n","      <td>-14.32</td>\n","      <td>-298.31</td>\n","      <td>-297.30</td>\n","      <td>-1.307682</td>\n","      <td>8.952000e+09</td>\n","      <td>1.272200e+10</td>\n","      <td>4.497000e+09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>29.25</td>\n","      <td>23.23</td>\n","      <td>0.0006</td>\n","      <td>0.001761</td>\n","      <td>0.000592</td>\n","      <td>32.86</td>\n","      <td>26.09</td>\n","      <td>42.00</td>\n","      <td>33.35</td>\n","      <td>1.216048e+08</td>\n","      <td>...</td>\n","      <td>0.146424</td>\n","      <td>0.741189</td>\n","      <td>-0.123487</td>\n","      <td>36.13</td>\n","      <td>67.74</td>\n","      <td>67.62</td>\n","      <td>0.046018</td>\n","      <td>2.305240e+11</td>\n","      <td>2.787620e+11</td>\n","      <td>1.540300e+10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>41.86</td>\n","      <td>30.43</td>\n","      <td>-0.0068</td>\n","      <td>-0.024544</td>\n","      <td>-0.006799</td>\n","      <td>45.89</td>\n","      <td>33.36</td>\n","      <td>56.02</td>\n","      <td>40.73</td>\n","      <td>2.322498e+08</td>\n","      <td>...</td>\n","      <td>0.133972</td>\n","      <td>0.932836</td>\n","      <td>0.008106</td>\n","      <td>-17.16</td>\n","      <td>86.71</td>\n","      <td>85.27</td>\n","      <td>0.520850</td>\n","      <td>4.720595e+11</td>\n","      <td>4.606060e+11</td>\n","      <td>6.020000e+10</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.03</td>\n","      <td>0.60</td>\n","      <td>0.0044</td>\n","      <td>0.000687</td>\n","      <td>0.006050</td>\n","      <td>0.23</td>\n","      <td>4.27</td>\n","      <td>0.31</td>\n","      <td>11.62</td>\n","      <td>3.557666e+05</td>\n","      <td>...</td>\n","      <td>0.020923</td>\n","      <td>0.024074</td>\n","      <td>0.012722</td>\n","      <td>488.70</td>\n","      <td>757.69</td>\n","      <td>811.07</td>\n","      <td>5.032471</td>\n","      <td>3.093865e+11</td>\n","      <td>4.511500e+10</td>\n","      <td>-3.254960e+11</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>38238</th>\n","      <td>13.79</td>\n","      <td>11.92</td>\n","      <td>-0.0053</td>\n","      <td>-0.017173</td>\n","      <td>-0.005275</td>\n","      <td>11.40</td>\n","      <td>9.86</td>\n","      <td>22.00</td>\n","      <td>19.02</td>\n","      <td>1.839779e+11</td>\n","      <td>...</td>\n","      <td>-0.111046</td>\n","      <td>-0.676866</td>\n","      <td>-7431.507309</td>\n","      <td>-58.36</td>\n","      <td>-42.92</td>\n","      <td>-42.92</td>\n","      <td>-0.883486</td>\n","      <td>5.942000e+09</td>\n","      <td>6.175000e+09</td>\n","      <td>9.220000e+08</td>\n","    </tr>\n","    <tr>\n","      <th>38239</th>\n","      <td>-1.90</td>\n","      <td>-3.72</td>\n","      <td>0.0716</td>\n","      <td>0.026644</td>\n","      <td>0.071566</td>\n","      <td>-7.57</td>\n","      <td>-14.79</td>\n","      <td>-5.89</td>\n","      <td>-11.50</td>\n","      <td>1.190084e+00</td>\n","      <td>...</td>\n","      <td>-0.063896</td>\n","      <td>0.018403</td>\n","      <td>12.937910</td>\n","      <td>12.01</td>\n","      <td>11.49</td>\n","      <td>11.33</td>\n","      <td>0.334871</td>\n","      <td>-1.029500e+09</td>\n","      <td>1.652000e+09</td>\n","      <td>-1.153000e+09</td>\n","    </tr>\n","    <tr>\n","      <th>38240</th>\n","      <td>5.27</td>\n","      <td>3.85</td>\n","      <td>0.0466</td>\n","      <td>0.022394</td>\n","      <td>0.046637</td>\n","      <td>16.38</td>\n","      <td>11.96</td>\n","      <td>15.96</td>\n","      <td>11.65</td>\n","      <td>5.976569e+00</td>\n","      <td>...</td>\n","      <td>0.027306</td>\n","      <td>0.026300</td>\n","      <td>2.025173</td>\n","      <td>2.73</td>\n","      <td>3.97</td>\n","      <td>4.01</td>\n","      <td>0.354836</td>\n","      <td>-3.910000e+08</td>\n","      <td>2.045000e+09</td>\n","      <td>-5.620000e+08</td>\n","    </tr>\n","    <tr>\n","      <th>38241</th>\n","      <td>5.04</td>\n","      <td>4.29</td>\n","      <td>-0.0010</td>\n","      <td>-0.003636</td>\n","      <td>-0.001040</td>\n","      <td>3.09</td>\n","      <td>2.63</td>\n","      <td>9.13</td>\n","      <td>7.77</td>\n","      <td>3.521770e+01</td>\n","      <td>...</td>\n","      <td>0.038513</td>\n","      <td>0.048395</td>\n","      <td>0.065115</td>\n","      <td>-31.84</td>\n","      <td>-21.77</td>\n","      <td>-22.69</td>\n","      <td>-0.319614</td>\n","      <td>5.591000e+09</td>\n","      <td>5.698000e+09</td>\n","      <td>3.170000e+08</td>\n","    </tr>\n","    <tr>\n","      <th>38242</th>\n","      <td>11.34</td>\n","      <td>10.18</td>\n","      <td>-0.0012</td>\n","      <td>-0.003901</td>\n","      <td>-0.001193</td>\n","      <td>7.08</td>\n","      <td>6.35</td>\n","      <td>22.35</td>\n","      <td>20.06</td>\n","      <td>6.250216e+01</td>\n","      <td>...</td>\n","      <td>0.069819</td>\n","      <td>0.105351</td>\n","      <td>0.031779</td>\n","      <td>2.66</td>\n","      <td>-20.12</td>\n","      <td>-21.71</td>\n","      <td>-1.255076</td>\n","      <td>1.188550e+10</td>\n","      <td>1.742600e+10</td>\n","      <td>1.719000e+09</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>38243 rows × 56 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98ceaea1-f112-4f8b-916d-9abb16ccd806')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-79970e5e-2fa2-452a-a7df-201e88f612c2\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79970e5e-2fa2-452a-a7df-201e88f612c2')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-79970e5e-2fa2-452a-a7df-201e88f612c2 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-98ceaea1-f112-4f8b-916d-9abb16ccd806 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-98ceaea1-f112-4f8b-916d-9abb16ccd806');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","from hyperopt import space_eval\n","\n","X_test = test_df.iloc[:, 3:-1]\n","y_test = test_df.iloc[:, -1]\n","X_test, y_test = X_test.values, y_test.values\n","\n","X_test_scaled = scaler.transform(X_test)\n","\n","# 테스트 데이터에 대한 예측 생성\n","preds_test = clf.predict_proba(X_test_scaled)[:, 1]\n","\n","# ROC AUC 점수 계산\n","roc_auc = roc_auc_score(y_test, preds_test)\n","print(\"ROC AUC score on test data: \", roc_auc)"],"metadata":{"id":"txULMF5C4Dgl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691497844255,"user_tz":-540,"elapsed":959,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}},"outputId":"28e7d9f5-eb5f-44de-a5e9-225a6d6c30bd"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["ROC AUC score on test data:  0.8561638770731713\n"]}]},{"cell_type":"code","source":["# import torch\n","\n","# torch.save(clf, '최종_이상치처리후_best_model.pth')\n","\n","# # Load the entire model\n","# clf = torch.load('best_tabnet_model.pth')\n"],"metadata":{"id":"wPyML__vTTNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, auc\n","\n","# thresholds = np.arange(0, 1.1, 0.1)  # 0부터 1까지 0.1 단위로 임계값 설정\n","\n","# for threshold in thresholds:\n","#     # 예측 확률을 기준으로 예측 레이블 생성\n","#     preds_label = (preds_test >= threshold).astype(int)\n","\n","#     print(f\"Threshold: {threshold}\")\n","#     print(\"Confusion Matrix:\")\n","#     print(confusion_matrix(y_test, preds_label))\n","#     print(\"Accuracy Score: \", accuracy_score(y_test, preds_label))\n","#     print(\"Precision Score: \", precision_score(y_test, preds_label))\n","#     print(\"Recall Score: \", recall_score(y_test, preds_label))\n","#     print(\"ROC AUC Score: \", roc_auc_score(y_test, preds_test))  # 주의: ROC AUC는 예측 확률을 사용\n","#     print(\"F1 Score: \", f1_score(y_test, preds_label))\n","#     print(\"-\" * 50)\n","\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, auc, fbeta_score\n","\n","precision, recall, thresholds = precision_recall_curve(y_test, preds_test)\n","\n","beta=2\n","f2_scores = ((1 + beta**2) * (precision * recall)) / ((beta**2 * precision) + recall)\n","f2_scores = f2_scores[~np.isnan(f2_scores)]\n","optimal_idx = np.argmax(f2_scores)\n","optimal_threshold = thresholds[optimal_idx]\n","\n","# thresholds = np.clip(np.linspace(optimal_threshold - 0.1, optimal_threshold + 0.1, 40), 0, 1)\n","# for threshold in thresholds:\n","\n","# predictions = (probas[:, 1] >= optimal_threshold).astype('int')\n","\n","\n","\n","# f1_scores = 2*(precision*recall)/(precision+recall)\n","# f1_scores = f1_scores[~np.isnan(f1_scores)]\n","\n","optimal_idx = np.argmax(f2_scores)\n","optimal_threshold = thresholds[optimal_idx]\n","\n","\n","\n","# thresholds = np.clip(np.linspace(optimal_threshold - 0.1, optimal_threshold + 0.1, 40), 0, 1)\n","# for threshold in thresholds:\n","\n","predictions = (preds_test >= optimal_threshold).astype('int')\n","\n","# 테스트 데이터에 대한 예측 성능 지표를 계산\n","test_cm = confusion_matrix(y_test, predictions)\n","test_acc = accuracy_score(y_test, predictions)\n","test_pre = precision_score(y_test, predictions)\n","test_rcll = recall_score(y_test, predictions)\n","test_roc_auc = roc_auc_score(y_test, preds_test)\n","test_f1 = f1_score(y_test, predictions)\n","test_f2 = fbeta_score(y_test, predictions, beta=2)\n","\n","# 테스트 데이터에 대한 예측 성능 출력\n","print('Threshold :', round(optimal_threshold, 2))\n","print('Confusion Matrix :', test_cm)\n","print('Accuracy Score :', round(test_acc,4))\n","print('Precision Score :', round(test_pre,4))\n","print('Recall Score :', round(test_rcll,4))\n","print('ROC AUC Score :', round(test_roc_auc,4))\n","print('F1 Score :', round(test_f1,4))\n","print('f2 스코어 :', round(test_f2,4))\n","print('\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtU5NKqILn6Q","executionInfo":{"status":"ok","timestamp":1691497844256,"user_tz":-540,"elapsed":10,"user":{"displayName":"특이점온다","userId":"16958813353461462268"}},"outputId":"6ce4b07c-9341-4a1f-a5c2-eb3250c98900"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Threshold : 0.04\n","Confusion Matrix : [[30203  6252]\n"," [  475  1313]]\n","Accuracy Score : 0.8241\n","Precision Score : 0.1736\n","Recall Score : 0.7343\n","ROC AUC Score : 0.8562\n","F1 Score : 0.2808\n","f2 스코어 : 0.4461\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-29-32472b63feb7>:25: RuntimeWarning: invalid value encountered in true_divide\n","  f2_scores = ((1 + beta**2) * (precision * recall)) / ((beta**2 * precision) + recall)\n"]}]}]}