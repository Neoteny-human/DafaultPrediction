{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>회사명</th>\n",
       "      <th>거래소코드</th>\n",
       "      <th>회계년도</th>\n",
       "      <th>총자산경상이익률</th>\n",
       "      <th>총자산순이익률</th>\n",
       "      <th>금융비용/매출액</th>\n",
       "      <th>금융비용/총부채</th>\n",
       "      <th>순금융비용/매출액</th>\n",
       "      <th>매출액경상이익률</th>\n",
       "      <th>매출액순이익률</th>\n",
       "      <th>...</th>\n",
       "      <th>이익잉여금/총부채</th>\n",
       "      <th>이익잉여금/유동자산</th>\n",
       "      <th>현금비율</th>\n",
       "      <th>당좌비율</th>\n",
       "      <th>유동비율</th>\n",
       "      <th>유동부채회전율</th>\n",
       "      <th>총자산</th>\n",
       "      <th>매출액</th>\n",
       "      <th>고정자산</th>\n",
       "      <th>부실여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(주)CMG제약_58820</td>\n",
       "      <td>58820</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-0.0254</td>\n",
       "      <td>-0.122986</td>\n",
       "      <td>-0.025390</td>\n",
       "      <td>2.39</td>\n",
       "      <td>4.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612677</td>\n",
       "      <td>0.070058</td>\n",
       "      <td>34.66</td>\n",
       "      <td>817.88</td>\n",
       "      <td>913.99</td>\n",
       "      <td>5.060456</td>\n",
       "      <td>1.972655e+11</td>\n",
       "      <td>6.947500e+10</td>\n",
       "      <td>7.369300e+10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(주)CMG제약_58820</td>\n",
       "      <td>58820</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.0237</td>\n",
       "      <td>-0.091076</td>\n",
       "      <td>-0.023699</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420767</td>\n",
       "      <td>0.066632</td>\n",
       "      <td>20.34</td>\n",
       "      <td>519.57</td>\n",
       "      <td>616.69</td>\n",
       "      <td>3.752774</td>\n",
       "      <td>2.062175e+11</td>\n",
       "      <td>8.219700e+10</td>\n",
       "      <td>7.819000e+10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(주)DB하이텍_990</td>\n",
       "      <td>990</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>29.25</td>\n",
       "      <td>23.23</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>32.86</td>\n",
       "      <td>26.09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.009866</td>\n",
       "      <td>1.020618</td>\n",
       "      <td>54.58</td>\n",
       "      <td>223.40</td>\n",
       "      <td>243.09</td>\n",
       "      <td>3.622273</td>\n",
       "      <td>1.364260e+12</td>\n",
       "      <td>1.214682e+12</td>\n",
       "      <td>7.347760e+11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(주)DB하이텍_990</td>\n",
       "      <td>990</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>41.86</td>\n",
       "      <td>30.43</td>\n",
       "      <td>-0.0068</td>\n",
       "      <td>-0.024544</td>\n",
       "      <td>-0.006799</td>\n",
       "      <td>45.89</td>\n",
       "      <td>33.36</td>\n",
       "      <td>...</td>\n",
       "      <td>2.942702</td>\n",
       "      <td>1.028724</td>\n",
       "      <td>37.42</td>\n",
       "      <td>310.11</td>\n",
       "      <td>328.36</td>\n",
       "      <td>4.143122</td>\n",
       "      <td>1.836319e+12</td>\n",
       "      <td>1.675288e+12</td>\n",
       "      <td>7.949760e+11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(주)ES큐브_50120</td>\n",
       "      <td>50120</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011783</td>\n",
       "      <td>-0.044887</td>\n",
       "      <td>490.81</td>\n",
       "      <td>817.39</td>\n",
       "      <td>872.57</td>\n",
       "      <td>5.186451</td>\n",
       "      <td>8.988110e+11</td>\n",
       "      <td>1.257870e+11</td>\n",
       "      <td>6.718500e+10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38238</th>\n",
       "      <td>히스보험중개주식회사_81703</td>\n",
       "      <td>81703</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>13.79</td>\n",
       "      <td>11.92</td>\n",
       "      <td>-0.0053</td>\n",
       "      <td>-0.017173</td>\n",
       "      <td>-0.005275</td>\n",
       "      <td>11.40</td>\n",
       "      <td>9.86</td>\n",
       "      <td>...</td>\n",
       "      <td>1.903422</td>\n",
       "      <td>-7430.500000</td>\n",
       "      <td>104.78</td>\n",
       "      <td>188.17</td>\n",
       "      <td>188.17</td>\n",
       "      <td>2.661670</td>\n",
       "      <td>3.009900e+10</td>\n",
       "      <td>3.640100e+10</td>\n",
       "      <td>7.842000e+09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38239</th>\n",
       "      <td>힐리언스_49187</td>\n",
       "      <td>49187</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-3.72</td>\n",
       "      <td>0.0716</td>\n",
       "      <td>0.026644</td>\n",
       "      <td>0.071566</td>\n",
       "      <td>-7.57</td>\n",
       "      <td>-14.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.428756</td>\n",
       "      <td>-17.214491</td>\n",
       "      <td>14.59</td>\n",
       "      <td>19.97</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.908741</td>\n",
       "      <td>3.097500e+10</td>\n",
       "      <td>7.797000e+09</td>\n",
       "      <td>2.905000e+10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38240</th>\n",
       "      <td>힐리언스_49187</td>\n",
       "      <td>49187</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>5.27</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.046637</td>\n",
       "      <td>16.38</td>\n",
       "      <td>11.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402456</td>\n",
       "      <td>-15.189318</td>\n",
       "      <td>17.32</td>\n",
       "      <td>23.94</td>\n",
       "      <td>24.28</td>\n",
       "      <td>1.263577</td>\n",
       "      <td>3.058400e+10</td>\n",
       "      <td>9.842000e+09</td>\n",
       "      <td>2.848800e+10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38241</th>\n",
       "      <td>힐티코리아주식회사_10534</td>\n",
       "      <td>10534</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>5.04</td>\n",
       "      <td>4.29</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.003636</td>\n",
       "      <td>-0.001040</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402792</td>\n",
       "      <td>0.240642</td>\n",
       "      <td>37.18</td>\n",
       "      <td>116.00</td>\n",
       "      <td>204.04</td>\n",
       "      <td>4.432747</td>\n",
       "      <td>5.722750e+10</td>\n",
       "      <td>9.326500e+10</td>\n",
       "      <td>1.503200e+10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38242</th>\n",
       "      <td>힐티코리아주식회사_10534</td>\n",
       "      <td>10534</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>11.34</td>\n",
       "      <td>10.18</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>-0.003901</td>\n",
       "      <td>-0.001193</td>\n",
       "      <td>7.08</td>\n",
       "      <td>6.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508143</td>\n",
       "      <td>0.272421</td>\n",
       "      <td>39.84</td>\n",
       "      <td>95.88</td>\n",
       "      <td>182.33</td>\n",
       "      <td>3.177671</td>\n",
       "      <td>6.911300e+10</td>\n",
       "      <td>1.106910e+11</td>\n",
       "      <td>1.675100e+10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38243 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    회사명  거래소코드        회계년도  총자산경상이익률  총자산순이익률  금융비용/매출액  \\\n",
       "0        (주)CMG제약_58820  58820  2021-12-01      0.84     1.60   -0.0254   \n",
       "1        (주)CMG제약_58820  58820  2022-12-01      1.45    -0.19   -0.0237   \n",
       "2          (주)DB하이텍_990    990  2021-12-01     29.25    23.23    0.0006   \n",
       "3          (주)DB하이텍_990    990  2022-12-01     41.86    30.43   -0.0068   \n",
       "4         (주)ES큐브_50120  50120  2021-12-01      0.03     0.60    0.0044   \n",
       "...                 ...    ...         ...       ...      ...       ...   \n",
       "38238  히스보험중개주식회사_81703  81703  2022-12-01     13.79    11.92   -0.0053   \n",
       "38239        힐리언스_49187  49187  2021-12-01     -1.90    -3.72    0.0716   \n",
       "38240        힐리언스_49187  49187  2022-12-01      5.27     3.85    0.0466   \n",
       "38241   힐티코리아주식회사_10534  10534  2021-12-01      5.04     4.29   -0.0010   \n",
       "38242   힐티코리아주식회사_10534  10534  2022-12-01     11.34    10.18   -0.0012   \n",
       "\n",
       "       금융비용/총부채  순금융비용/매출액  매출액경상이익률  매출액순이익률  ...  이익잉여금/총부채   이익잉여금/유동자산  \\\n",
       "0     -0.122986  -0.025390      2.39     4.56  ...   0.612677     0.070058   \n",
       "1     -0.091076  -0.023699      3.63    -0.47  ...   0.420767     0.066632   \n",
       "2      0.001761   0.000592     32.86    26.09  ...   2.009866     1.020618   \n",
       "3     -0.024544  -0.006799     45.89    33.36  ...   2.942702     1.028724   \n",
       "4      0.000687   0.006050      0.23     4.27  ...  -0.011783    -0.044887   \n",
       "...         ...        ...       ...      ...  ...        ...          ...   \n",
       "38238 -0.017173  -0.005275     11.40     9.86  ...   1.903422 -7430.500000   \n",
       "38239  0.026644   0.071566     -7.57   -14.79  ...  -1.428756   -17.214491   \n",
       "38240  0.022394   0.046637     16.38    11.96  ...  -1.402456   -15.189318   \n",
       "38241 -0.003636  -0.001040      3.09     2.63  ...   0.402792     0.240642   \n",
       "38242 -0.003901  -0.001193      7.08     6.35  ...   0.508143     0.272421   \n",
       "\n",
       "         현금비율    당좌비율    유동비율   유동부채회전율           총자산           매출액  \\\n",
       "0       34.66  817.88  913.99  5.060456  1.972655e+11  6.947500e+10   \n",
       "1       20.34  519.57  616.69  3.752774  2.062175e+11  8.219700e+10   \n",
       "2       54.58  223.40  243.09  3.622273  1.364260e+12  1.214682e+12   \n",
       "3       37.42  310.11  328.36  4.143122  1.836319e+12  1.675288e+12   \n",
       "4      490.81  817.39  872.57  5.186451  8.988110e+11  1.257870e+11   \n",
       "...       ...     ...     ...       ...           ...           ...   \n",
       "38238  104.78  188.17  188.17  2.661670  3.009900e+10  3.640100e+10   \n",
       "38239   14.59   19.97   20.27  0.908741  3.097500e+10  7.797000e+09   \n",
       "38240   17.32   23.94   24.28  1.263577  3.058400e+10  9.842000e+09   \n",
       "38241   37.18  116.00  204.04  4.432747  5.722750e+10  9.326500e+10   \n",
       "38242   39.84   95.88  182.33  3.177671  6.911300e+10  1.106910e+11   \n",
       "\n",
       "               고정자산  부실여부  \n",
       "0      7.369300e+10   0.0  \n",
       "1      7.819000e+10   0.0  \n",
       "2      7.347760e+11   0.0  \n",
       "3      7.949760e+11   0.0  \n",
       "4      6.718500e+10   0.0  \n",
       "...             ...   ...  \n",
       "38238  7.842000e+09   0.0  \n",
       "38239  2.905000e+10   0.0  \n",
       "38240  2.848800e+10   0.0  \n",
       "38241  1.503200e+10   0.0  \n",
       "38242  1.675100e+10   0.0  \n",
       "\n",
       "[38243 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/최종_이상치처리후_train.csv',encoding='utf-8')\n",
    "df2 = pd.read_csv('./data/최종_test.csv',encoding='utf-8')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ktwjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "c:\\Users\\ktwjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# # 데이터 분할\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[39m# TabNet 모델 생성 및 학습\u001b[39;00m\n\u001b[0;32m     21\u001b[0m clf \u001b[39m=\u001b[39m TabNetClassifier()\n\u001b[1;32m---> 22\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, virtual_batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n\u001b[0;32m     24\u001b[0m \u001b[39m# 모델 평가\u001b[39;00m\n\u001b[0;32m     25\u001b[0m predictions \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\ktwjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m     \u001b[39m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[1;32m--> 258\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[1;32mc:\\Users\\ktwjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m    487\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m--> 489\u001b[0m     batch_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X, y)\n\u001b[0;32m    491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[0;32m    493\u001b[0m epoch_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "File \u001b[1;32mc:\\Users\\ktwjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:534\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    531\u001b[0m loss \u001b[39m=\u001b[39m loss \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_sparse \u001b[39m*\u001b[39m M_loss\n\u001b[0;32m    533\u001b[0m \u001b[39m# Perform backward pass and optimization\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_value:\n\u001b[0;32m    536\u001b[0m     clip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_value)\n",
      "File \u001b[1;32mc:\\Users\\ktwjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ktwjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ktwjj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\function.py:264\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m    265\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    268\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_train = df.drop(['부실여부','회사명','회계년도'], axis=1)\n",
    "y_train = df['부실여부']\n",
    "X_test = df2.drop(['부실여부','회사명','회계년도'], axis=1)\n",
    "y_test = df2['부실여부']\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "# # 데이터 분할\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TabNet 모델 생성 및 학습\n",
    "clf = TabNetClassifier()\n",
    "clf.fit(X_train, y_train, max_epochs=100, batch_size=32, virtual_batch_size=128)\n",
    "\n",
    "# 모델 평가\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Accuracy: \", (predictions == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hyperopt import hp, tpe, fmin, Trials\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 데이터를 로드합니다. 'data.csv'를 자신의 데이터셋 파일 경로로 바꿔주세요.\n",
    "# df = pd.read_csv('data.csv')\n",
    "\n",
    "# # 데이터를 특징과 타겟으로 분리합니다. 'target'를 예측하고자 하는 열 이름으로 바꿔주세요.\n",
    "# X = df.drop('target', axis=1)\n",
    "# y = df['target']\n",
    "\n",
    "# # 문자열 레이블을 숫자로 변환합니다.\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(y)\n",
    "\n",
    "# # 하이퍼파라미터 탐색 공간을 정의합니다.\n",
    "# space = {\n",
    "#     'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "#     'gamma': hp.uniform('gamma', 1.0, 2.0),\n",
    "#     'n_d': hp.choice('n_d', [8, 16, 24, 32]),\n",
    "#     'n_a': hp.choice('n_a', [8, 16, 24, 32]),\n",
    "#     'batch_size': hp.choice('batch_size', [32, 64, 128, 256])\n",
    "# }\n",
    "\n",
    "# # 목표 함수를 정의합니다.\n",
    "# def objective(params):\n",
    "#     clf = TabNetClassifier(\n",
    "#         n_d=int(params['n_d']),\n",
    "#         n_a=int(params['n_a']),\n",
    "#         learning_rate=params['learning_rate'],\n",
    "#         gamma=params['gamma'],\n",
    "#         batch_size=int(params['batch_size']),\n",
    "#         verbose=0\n",
    "#     )\n",
    "    \n",
    "#     score = cross_val_score(clf, X, y, cv=StratifiedKFold(), scoring='roc_auc').mean()\n",
    "#     return {'loss': -score, 'status': 'ok'}\n",
    "\n",
    "# # 최적화를 실행합니다.\n",
    "# trials = Trials()\n",
    "# best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "# print(f\"Best parameters: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'gamma': hp.uniform('gamma', 1.0, 2.0),\n",
    "    'n_d': hp.choice('n_d', [8, 16, 24, 32]),\n",
    "    'n_a': hp.choice('n_a', [8, 16, 24, 32]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128, 256])\n",
    "}\n",
    "\n",
    "# Assume we have X_train, Y_train, X_val, Y_val as our dataset\n",
    "def objective(params):\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=int(params['n_d']),\n",
    "        n_a=int(params['n_a']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        gamma=params['gamma'],\n",
    "        batch_size=int(params['batch_size']),\n",
    "        verbose=0\n",
    "    )\n",
    "    clf.fit(\n",
    "        X_train, Y_train,\n",
    "        eval_set=[(X_val, Y_val)],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=0,\n",
    "    )\n",
    "    pred = clf.predict(X_val)\n",
    "    accuracy = (pred == Y_val).mean()  # Or use any other metric you like\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,  # Set this to a higher number for a longer search\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(f\"Best parameters: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameter space\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'gamma': hp.uniform('gamma', 1.0, 2.0),\n",
    "    'n_d': hp.choice('n_d', [8, 16, 24, 32]),\n",
    "    'n_a': hp.choice('n_a', [8, 16, 24, 32]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128, 256])\n",
    "}\n",
    "\n",
    "# 데이터를 불러오고 전처리해야합니다. 이 코드는 플레이스 홀더입니다.\n",
    "# X_train, y_train, X_valid, y_valid = load_and_preprocess_data()\n",
    "\n",
    "def objective(params):\n",
    "    # Ensure parameters that should be integers are integers\n",
    "    for parameter_name in ['n_d', 'n_a', 'batch_size']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=params['n_d'],\n",
    "        n_a=params['n_a'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        gamma=params['gamma'],\n",
    "        batch_size=params['batch_size'],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    preds = clf.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Run the algorithm\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Best: \", best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
